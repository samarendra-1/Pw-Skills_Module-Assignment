{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPlRD8iUcZzinkRbYoNolnV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1.1\n","\n","1.\n","\n","Static Variables (Technically not present in Python);\n","\n","Memory Allocation: Static variables have a fixed memory location allocated at compile time (which Python doesn't do).\n","\n","Scope: Limited to the scope they're declared in (e.g., within a function).\n","Lifetime: Persists throughout the program's execution.\n","\n","Initialization: Initialized only once, typically at the beginning of the program.\n","\n","Data Sharing: Shared among all calls to the function they're defined in.\n","\n","\n","Dynamic Variables (Common in Python);\n","\n","Memory Allocation: Memory is allocated at runtime when the variable is assigned a value.\n","\n","Scope: Can vary depending on where they're declared (e.g., local to a function, global within a module).\n","\n","Lifetime: Exists as long as the object or block they're associated with is in scope.\n","\n","Initialization: Can be initialized multiple times during runtime.\n","\n","Data Sharing: Isolated to their specific scope, unless explicitly shared through mechanisms like arguments or return values."],"metadata":{"id":"_y5RlgQ5jHAi"}},{"cell_type":"markdown","source":["2.\n","pop(key, default=None)\n","\n","Purpose: Removes an item (key-value pair) from the dictionary and returns its value.\n","\n","Arguments:\n","\n","key (required): The key of the item you want to remove.\n","default (optional): A value to return if the key is not found in the dictionary (defaults to None)\n","\n","Examples:\n"],"metadata":{"id":"8Peb_ZXnjZHp"}},{"cell_type":"code","source":["my_dict = {'name': 'Alice', 'age': 30, 'city': 'New York'}\n","\n","removed_value = my_dict.pop('age')  # Returns 30 (value associated with 'age')\n","print(my_dict)  # Output: {'name': 'Alice', 'city': 'New York'}\n","\n","missing_value = my_dict.pop('job', 'Unemployed')  # Returns 'Unemployed' as 'job' doesn't exist\n","print(my_dict)  # Output: {'name': 'Alice', 'city': 'New York'}\n","\n","try:\n","    # This will raise a KeyError because 'country' is not in the dictionary\n","    my_dict.pop('country')\n","except KeyError:\n","    print(\"Key 'country' not found\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rbai_hj2jksr","executionInfo":{"status":"ok","timestamp":1720277971831,"user_tz":-330,"elapsed":7,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"1394aaa8-bba0-40aa-fbaa-d00dce93eb3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': 'Alice', 'city': 'New York'}\n","{'name': 'Alice', 'city': 'New York'}\n","Key 'country' not found\n"]}]},{"cell_type":"markdown","source":[" popitem()\n","\n","Purpose: Removes and returns an arbitrary key-value pair (a tuple containing both key and value) from the dictionary.\n","\n","Arguments: None (it takes no arguments).\n","\n","Order: In Python versions 3.7 and later, dictionaries maintain insertion order. So, popitem() removes elements in a Last-In-First-Out (LIFO) manner, similar to a stack.\n","\n","Example:"],"metadata":{"id":"VLIDISwQjs1L"}},{"cell_type":"code","source":["my_dict = {'name': 'Alice', 'age': 30, 'city': 'New York'}\n","\n","removed_item = my_dict.popitem()  # Order not guaranteed, but might return ('city', 'New York')\n","print(my_dict)  # Dictionary content might change (depending on removal order)\n","\n","# In Python versions before 3.7, the order was not guaranteed, so the output could vary.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yvofxVbajn08","executionInfo":{"status":"ok","timestamp":1720278012237,"user_tz":-330,"elapsed":514,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"a3539b96-54bf-4ba1-cc31-5b23fdbc5490"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': 'Alice', 'age': 30}\n"]}]},{"cell_type":"markdown","source":["clear()\n","\n","Purpose: Removes all items (key-value pairs) from the dictionary.\n","\n","Arguments: None (it takes no arguments).\n","\n","Example:"],"metadata":{"id":"xrWuo-jvj8mH"}},{"cell_type":"code","source":["my_dict = {'name': 'Alice', 'age': 30, 'city': 'New York'}\n","\n","my_dict.clear()\n","print(my_dict)  # Output: {} (empty dictionary)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qHEzxZsj0kU","executionInfo":{"status":"ok","timestamp":1720278065162,"user_tz":-330,"elapsed":501,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"2a25f01b-7900-4364-8c74-7389196e7d73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{}\n"]}]},{"cell_type":"markdown","source":["3.\n","Dictionary Keys: Since dictionaries require hashable keys (objects that can be uniquely identified), and frozensets are immutable, they can be used as reliable dictionary keys. This ensures the key itself cannot be accidentally altered.\n","\n","Set Operations: FrozenSets can participate in set operations like union, intersection, difference, and symmetric difference, just like regular sets.\n","\n","Membership Testing: You can efficiently check if an element exists in a frozenset using the in operator.\n","\n","Memory Efficiency: In cases where you know the set elements won't change, frozensets can be slightly more memory-efficient than regular sets.\n","\n","Creating a FrozenSet:\n","\n","You can create a frozenset using the frozenset() function. It accepts an iterable (like a list, tuple, or another set) as input and returns a frozenset containing the unique elements."],"metadata":{"id":"ugjp2GXxkFS_"}},{"cell_type":"code","source":["my_list = [1, 2, 2, 3, 4]\n","my_frozenset = frozenset(my_list)  # {1, 2, 3, 4} (duplicates are removed)\n","\n","my_tuple = (5, 'apple', 5, 'banana')\n","my_frozenset2 = frozenset(my_tuple)  # {'apple', 5, 'banana'}\n"],"metadata":{"id":"5iqpmpX0kBsc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since frozensets are immutable, you cannot add, remove, or modify elements after creation. Attempting these operations will result in an AttributeError"],"metadata":{"id":"hFuK-wT-kX0M"}},{"cell_type":"code","source":["my_frozenset = frozenset({1, 2, 3})\n","\n","try:\n","  my_frozenset.add(4)  # Raises an AttributeError\n","except AttributeError as e:\n","  print(\"Error:\", e)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y-4HmCLbkTpt","executionInfo":{"status":"ok","timestamp":1720278177308,"user_tz":-330,"elapsed":482,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"42f180c9-eadf-42d7-8693-a6a67b5d7bcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: 'frozenset' object has no attribute 'add'\n"]}]},{"cell_type":"markdown","source":["4.\n","\n","In Python, data types can be classified into two categories based on their ability to be modified after creation: mutable and immutable.\n","\n","Mutable Data Types:\n","\n","Can be changed: The content or value of a mutable data type can be altered after it's assigned.\n","\n","Common operations: You can add, remove, or modify elements within a mutable data type.\n","\n","Examples:\n","\n","Lists ([]): Ordered collections of items that can be of different data types. You can add, remove, or change elements within a list"],"metadata":{"id":"ViSpMjUlljja"}},{"cell_type":"code","source":["my_list = [1, 2, 3]\n","my_list.append(4)  # Adds 4 to the end\n","my_list[1] = 5     # Changes the second element to 5\n","print(my_list)  # Output: [1, 5, 3, 4]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cI58rG0lpau","executionInfo":{"status":"ok","timestamp":1720278492704,"user_tz":-330,"elapsed":494,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"9c2d11ce-c0e2-49bd-e96d-51dc11aea06a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 5, 3, 4]\n"]}]},{"cell_type":"markdown","source":["Dictionaries ({}): Unordered collections of key-value pairs. You can add, remove, or modify key-value pairs within a dictionary."],"metadata":{"id":"ik1m0wnFluns"}},{"cell_type":"code","source":["my_dict = {'name': 'Alice', 'age': 30}\n","my_dict['city'] = 'New York'  # Adds a new key-value pair\n","my_dict['age'] = 31           # Modifies an existing value\n","print(my_dict)  # Output: {'name': 'Alice', 'age': 31, 'city': 'New York'}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9clAqBYklqEu","executionInfo":{"status":"ok","timestamp":1720278527202,"user_tz":-330,"elapsed":6,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"7b5e5bc7-7c63-4d48-9331-a469ccc734ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': 'Alice', 'age': 31, 'city': 'New York'}\n"]}]},{"cell_type":"markdown","source":["Sets (set()): Unordered collections of unique elements. You can add or remove elements from a set, but you cannot change the order or modify existing elements"],"metadata":{"id":"_tZmdmH2l3d8"}},{"cell_type":"code","source":["my_set = {1, 2, 3}\n","my_set.add(4)  # Adds 4 to the set\n","my_set.remove(2)  # Removes 2 from the set\n","print(my_set)  # Output: {1, 3, 4} (Order may vary)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQITylOPlylE","executionInfo":{"status":"ok","timestamp":1720278558467,"user_tz":-330,"elapsed":514,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"661c53dd-1da4-4935-8325-938d802cc0b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{1, 3, 4}\n"]}]},{"cell_type":"markdown","source":["Immutable Data Types:\n","\n","Cannot be changed: The value of an immutable data type remains constant after it's created. Any attempt to modify it will result in the creation of a new object.\n","\n","Common operations: You can access elements or perform operations on the data, but not directly modify the content.\n","\n","Examples:\n","\n","Strings ('' or \"\"): Sequences of characters. You cannot change individual characters within a string"],"metadata":{"id":"5ceuaJPjl9ph"}},{"cell_type":"code","source":["my_string = \"Hello, world!\"\n","try:\n","    my_string[0] = 'A'  # This will raise a TypeError\n","except TypeError:\n","    print(\"Strings are immutable\")\n","print(my_string)  # Output: \"Hello, world!\" (Original string remains unchanged)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WvwHqRjHl5zU","executionInfo":{"status":"ok","timestamp":1720278585598,"user_tz":-330,"elapsed":517,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"e78e7822-8438-4557-9c1a-8703e336b6a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Strings are immutable\n","Hello, world!\n"]}]},{"cell_type":"markdown","source":["Numbers (e.g., int, float): Represent numeric values. You cannot modify an existing number, but you can perform operations on it"],"metadata":{"id":"GD7ijGg-mECp"}},{"cell_type":"code","source":["my_number = 10\n","my_number += 5  # Assigns the result of adding 5 to a new variable\n","print(my_number)  # Output: 10 (Original number remains unchanged)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9pmeW5OmAwM","executionInfo":{"status":"ok","timestamp":1720278609371,"user_tz":-330,"elapsed":595,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"d3ae11ef-8b2c-492b-f221-61a4fa93c2c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["15\n"]}]},{"cell_type":"markdown","source":["Tuples (()): Ordered collections of elements that can be of different data types. You cannot modify elements or the structure of a tuple."],"metadata":{"id":"J4YkhVCjmJbj"}},{"cell_type":"code","source":["my_tuple = (1, \"apple\", 3.14)\n","try:\n","    my_tuple[0] = 2  # This will raise a TypeError\n","except TypeError:\n","    print(\"Tuples are immutable\")\n","print(my_tuple)  # Output: (1, \"apple\", 3.14) (Original tuple remains unchanged)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35weULiumGik","executionInfo":{"status":"ok","timestamp":1720278631414,"user_tz":-330,"elapsed":536,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"141671c7-6f20-4c2b-c70e-c15e743b0386"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tuples are immutable\n","(1, 'apple', 3.14)\n"]}]},{"cell_type":"markdown","source":["5..\n","Purpose:\n","\n","The __init__ method is used to initialize the attributes (variables) of an object. These attributes will hold the specific data for each instance of the class.\n","\n","You can also perform any necessary setup tasks within __init__ to configure the object's initial state.\n","\n","How it Works:\n","\n","Defining the Class: You define a class with its attributes and methods.\n","Creating an Object: When you create an object (instance) of the class using the Classname() syntax, Python automatically calls the __init__ method behind the scenes.\n","\n","Initializing Attributes: Inside the __init__ method, you can define arguments (parameters) to receive values and assign them to the object's attributes using self. The self parameter refers to the current object itself."],"metadata":{"id":"LYg-ZYtYmXe4"}},{"cell_type":"code","source":["class Car:\n","  def __init__(self, make, model, year):  # The __init__ method with parameters\n","    self.make = make  # Assigning values to object attributes using self\n","    self.model = model\n","    self.year = year\n","\n","  def display_info(self):  # A regular method to access object attributes\n","    print(f\"Make: {self.make}, Model: {self.model}, Year: {self.year}\")\n","\n","# Creating objects (instances) and initializing attributes\n","my_car = Car(\"Honda\", \"Civic\", 2023)\n","my_truck = Car(\"Ford\", \"F-150\", 2022)\n","\n","my_car.display_info()  # Output: Make: Honda, Model: Civic, Year: 2023\n","my_truck.display_info()  # Output: Make: Ford, Model: F-150, Year: 2022\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8dp2Fa0mLw0","executionInfo":{"status":"ok","timestamp":1720278726803,"user_tz":-330,"elapsed":765,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"185cad3b-6ddc-41f9-c727-e5174bfadf3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Make: Honda, Model: Civic, Year: 2023\n","Make: Ford, Model: F-150, Year: 2022\n"]}]},{"cell_type":"markdown","source":["6..\n","\n","\n","In Python, a docstring (documentation string) is a multiline string used to document Python code. It's written within triple quotes (\"\"\"Docstring goes here\"\"\") and is used to explain what a function, class, module, or method does.\n","\n","Example:"],"metadata":{"id":"gKocdZPemvj_"}},{"cell_type":"code","source":["def add_numbers(x, y):\n","  \"\"\"\n","  This function takes two numbers and returns their sum.\n","\n","  Args:\n","      x: The first number.\n","      y: The second number.\n","\n","  Returns:\n","      The sum of x and y.\n","  \"\"\"\n","  return x + y\n"],"metadata":{"id":"dBdacdw1mjJu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7..\n","\n","In Python, unit tests are small, isolated pieces of code that test individual units or components of your software in isolation. The goal is to verify that each part of your code works as expected, independently of other parts.\n","\n","Here's how it works:\n","\n","Choose a Unit: Select a small, specific part of your code to test, like a function or a method.\n","\n","Write Test Cases: Create test functions that call the unit you're testing with different inputs and check if the output matches your expectations.\n","\n","Use Assertions: Employ assertion statements (like assert) to verify the correctness of the results. If an assertion fails, the test indicates a potential issue.\n","\n","Organize Tests: Group related tests into test suites or test cases for better organization.\n","\n","Run Tests: Execute the tests using a testing framework like unittest or pytest"],"metadata":{"id":"Asx0I_1Gm27H"}},{"cell_type":"code","source":["import unittest\n","\n","def add(x, y):\n","  return x + y\n","\n","class TestAdd(unittest.TestCase):\n","  def test_positive_numbers(self):\n","    self.assertEqual(add(2, 3), 5)\n","\n","  def test_zero(self):\n","    self.assertEqual(add(5, 0), 5)\n","\n","if __name__ == '__main__':\n","  unittest.main(exit=False) # Suppress the SystemExit message"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DBqmyODSnGwT","executionInfo":{"status":"ok","timestamp":1720278913003,"user_tz":-330,"elapsed":586,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"15d3956f-2b75-47aa-ecca-114a72f23913"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["E\n","======================================================================\n","ERROR: /root/ (unittest.loader._FailedTest)\n","----------------------------------------------------------------------\n","AttributeError: module '__main__' has no attribute '/root/'\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.004s\n","\n","FAILED (errors=1)\n"]}]},{"cell_type":"markdown","source":["8..\n","\n","break\n","\n","Purpose: Immediately terminates the current loop (innermost loop if nested) and resumes execution at the next statement after the loop.\n","\n","Use case: When you want to exit a loop prematurely based on a certain condition being met.\n","\n"],"metadata":{"id":"PJi2rhqGnaec"}},{"cell_type":"code","source":["for number in range(10):\n","    if number == 5:\n","        break  # Exit the loop when number reaches 5\n","    print(number)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bNGg3EjsnQcs","executionInfo":{"status":"ok","timestamp":1720278983543,"user_tz":-330,"elapsed":835,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"dcbb0143-4281-4956-e2e7-c6433fbac342"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n"]}]},{"cell_type":"markdown","source":["continue\n","\n","Purpose: Skips the remaining statements in the current iteration of the loop and moves on to the next iteration.\n","\n","Use case: When you want to bypass specific iterations based on a condition, but continue the loop overall.\n","\n","Example:"],"metadata":{"id":"Ebtbgv-Snks6"}},{"cell_type":"code","source":["for number in range(10):\n","    if number % 2 == 0:  # Check if number is even\n","        continue  # Skip even numbers\n","    print(number)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHSWTEaBnhz6","executionInfo":{"status":"ok","timestamp":1720279003580,"user_tz":-330,"elapsed":544,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"baf77387-a064-433b-e989-da80d062979a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","3\n","5\n","7\n","9\n"]}]},{"cell_type":"markdown","source":["pass\n","\n","Purpose: A null operation – it does nothing. It's a placeholder statement.\n","\n","Use case:\n","\n","When you need syntactically correct code but don't want any action to be taken (e.g., during development when you're outlining the structure of your code).\n","To create empty classes, functions, or conditional blocks without causing errors"],"metadata":{"id":"E2k_uSodno6L"}},{"cell_type":"code","source":["for number in range(10):\n","    if number % 2 == 0:\n","        pass  # Do nothing for even numbers\n","    else:\n","        print(number)  # Print odd numbers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCdgYUBHnmx1","executionInfo":{"status":"ok","timestamp":1720279030752,"user_tz":-330,"elapsed":504,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"91e37a83-a388-460c-c7e6-1f2f5ad89c0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","3\n","5\n","7\n","9\n"]}]},{"cell_type":"markdown","source":["9..\n","\n","In Python, self is a convention used as the first parameter of instance methods within a class. It refers to the instance of the class itself. Think of it as a way for an object to refer to its own attributes and methods.\n","\n","Key Points\n","\n","Not a Keyword: self is not a strict keyword in Python (you could technically use a different name), but it's a universally adopted convention. Using any other name would make your code less readable and confusing to other Python developers.\n","\n","Instance Reference: When you create an object from a class, self within the class's methods automatically points to that specific object.\n","\n","Accessing Attributes and Methods: You use self to:\n","\n","Access instance variables (attributes) that belong to the object.\n","Call other instance methods defined within the class"],"metadata":{"id":"on_s8fjZnwAR"}},{"cell_type":"code","source":["class Dog:\n","    def __init__(self, name, breed):\n","        self.name = name  # Assign the 'name' argument to the instance's 'name' attribute\n","        self.breed = breed\n","\n","    def bark(self):\n","        print(\"Woof!\")\n","\n","    def describe(self):\n","        print(f\"My name is {self.name} and I'm a {self.breed}\")\n","\n","my_dog = Dog(\"Buddy\", \"Golden Retriever\")\n","my_dog.bark()  # Output: Woof!\n","my_dog.describe()  # Output: My name is Buddy and I'm a Golden Retriever"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wr0jDDofntDc","executionInfo":{"status":"ok","timestamp":1720279109888,"user_tz":-330,"elapsed":479,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"eba16760-462e-4126-bb01-0630818953be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Woof!\n","My name is Buddy and I'm a Golden Retriever\n"]}]},{"cell_type":"markdown","source":["Explanation\n","\n","In the __init__ method (the constructor), self.name and self.breed create attributes for the Dog object and store the provided values.\n","In the bark and describe methods, self is used to access the object's name attribute and to call the bark method.\n","\n","Why Use self?\n","\n","Without self, there would be no way for a method to differentiate between its own attributes and variables from the outside world. self provides the necessary context for an object to work with its own data and behavior."],"metadata":{"id":"ytAbMI77oFgY"}},{"cell_type":"markdown","source":["10..\n","\n","Global Attributes\n","\n","Definition: Variables declared outside any function or class, accessible from anywhere within the module.\n","\n","Convention: No specific naming convention, but it's generally recommended to use all-uppercase names for global constants (e.g., PI = 3.14159).\n","\n","Accessibility: Can be read and modified from anywhere in the module.\n","\n","Example:"],"metadata":{"id":"50InFGYAoKBL"}},{"cell_type":"code","source":["GLOBAL_VAR = 10\n","\n","def my_function():\n","    print(GLOBAL_VAR)  # Accessing the global variable\n","\n","my_function()  # Output: 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Diz-Ut_4oAiX","executionInfo":{"status":"ok","timestamp":1720279206338,"user_tz":-330,"elapsed":544,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"4f691f74-59af-4c5c-c87f-8a25badc6c4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}]},{"cell_type":"markdown","source":["Protected Attributes\n","\n","Definition: Attributes within a class that are intended for internal use within the class or its subclasses.\n","\n","Convention: Prefix the attribute name with a single underscore (e.g., _protected_attribute). This signals to other developers that the attribute is not meant to be directly accessed from outside the class.\n","\n","Accessibility: Technically, protected attributes can still be accessed from outside the class, but doing so is discouraged.\n","\n","Example:"],"metadata":{"id":"s_mSFpdQoap-"}},{"cell_type":"code","source":["class MyClass:\n","    def __init__(self):\n","        self._protected_var = 5\n","\n","    def get_protected_var(self):\n","        return self._protected_var\n","\n","obj = MyClass()\n","print(obj.get_protected_var())  # Accessing through a method: Output: 5\n","print(obj._protected_var)      # Direct access (discouraged): Output: 5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RP-5k__0oYSm","executionInfo":{"status":"ok","timestamp":1720279225455,"user_tz":-330,"elapsed":512,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"e9fcba6e-f811-4be2-ebb3-d11f8194fcdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","5\n"]}]},{"cell_type":"markdown","source":["\n","Private Attributes\n","\n","Definition: Attributes within a class that are intended to be strictly internal to the class and not accessible from outside.\n","\n","Convention: Prefix the attribute name with two underscores (e.g., __private_attribute). Python performs name mangling on these attributes, making them less straightforward to access directly from outside the class.\n","\n","Accessibility: While technically still accessible through name mangling (_ClassName__private_attribute), direct access from outside the class is strongly discouraged.\n","\n","Example:"],"metadata":{"id":"s7z3hdUgoemb"}},{"cell_type":"code","source":["class MyClass:\n","    def __init__(self):\n","        self.__private_var = 10\n","\n","obj = MyClass()\n","# print(obj.__private_var)  # This would raise an AttributeError\n","print(obj._MyClass__private_var)  # Accessing through name mangling (discouraged): Output: 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cIu__nbNocxE","executionInfo":{"status":"ok","timestamp":1720279251558,"user_tz":-330,"elapsed":545,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"fca42a20-db03-44a7-a49b-9839bfca5093"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}]},{"cell_type":"markdown","source":["11..\n","\n","Modules\n","\n","Definition: A module is simply a file containing Python code. It can define functions, classes, and variables that you can use in other Python programs.\n","\n","Purpose: Modules promote code reusability. You write a module once and then import it into other scripts or projects whenever you need its functionality.\n","\n","Example: Imagine you have a file named my_math.py with functions for mathematical calculations"],"metadata":{"id":"iIV8Jp3LolGH"}},{"cell_type":"code","source":["# my_math.py\n","\n","def add(x, y):\n","    return x + y\n","\n","def multiply(x, y):\n","    return x * y"],"metadata":{"id":"tJwiwguzojN_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Packages\n","\n","Definition: A package is a way to organize related modules into a hierarchical structure, forming a directory containing multiple module files and an __init__.py file.\n","\n","Purpose: Packages help you structure large projects, preventing naming conflicts and making your codebase easier to navigate.\n","\n","__init__.py: This special file signals to Python that the directory should be treated as a package. It can be empty or contain initialization code for the package.\n","\n","Example: Suppose you have a project with modules for handling different data formats (CSV, JSON, etc.). You could organize them into a package called data_processing:\n"],"metadata":{"id":"pjg89lGApi0V"}},{"cell_type":"code","source":["data_processing/\n","    __init__.py\n","    csv_utils.py\n","    json_utils.py"],"metadata":{"id":"3mC7uNw6pndj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import data_processing.csv_utils\n","\n","data = data_processing.csv_utils.read_csv('my_data.csv')"],"metadata":{"id":"a0zkOl9Apqhi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Benefits of Using Modules and Packages\n","\n","Code Organization: Break down complex projects into smaller, more manageable units.\n","\n","Code Reusability: Write code once and reuse it in multiple projects.\n","\n","Namespace Management: Prevent naming conflicts when using code from different sources.\n","\n","Collaboration: Enable multiple developers to work on different parts of a project without interfering with each other's code."],"metadata":{"id":"aErO276wpslm"}},{"cell_type":"markdown","source":["12..\n","\n","\n","Lists\n","\n","Definition: Ordered collections of items that can be of different data types (numbers, strings, even other lists).\n","\n","Mutability: Lists are mutable, meaning you can change their contents (add, remove, or modify items) after they are created.\n","\n","Syntax: Defined using square brackets [].\n","\n","Example:"],"metadata":{"id":"XBZTF8DPpyN1"}},{"cell_type":"code","source":["my_list = [1, \"apple\", 3.14, True]\n","my_list.append(\"banana\")  # Add an item\n","my_list[0] = 2  # Change the first item\n","print(my_list)  # Output: [2, \"apple\", 3.14, True, \"banana\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FkMQ6XUSqB5S","executionInfo":{"status":"ok","timestamp":1720279642697,"user_tz":-330,"elapsed":516,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"bf6bad6e-815a-4a37-a990-6e11407f49ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 'apple', 3.14, True, 'banana']\n"]}]},{"cell_type":"markdown","source":["Tuples\n","\n","Definition: Similar to lists, ordered collections of items of potentially mixed data types.\n","\n","Immutability: Tuples are immutable, meaning you cannot change their contents once they are created.\n","\n","Syntax: Defined using parentheses ().\n","Example"],"metadata":{"id":"k0Ru-AIjqFjI"}},{"cell_type":"code","source":["my_tuple = (1, \"orange\", 2.71, False)\n","# my_tuple.append(\"grape\")  # This would raise an error (tuples are immutable)\n","print(my_tuple)  # Output: (1, \"orange\", 2.71, False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFVvTX5aqCj1","executionInfo":{"status":"ok","timestamp":1720279670046,"user_tz":-330,"elapsed":920,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"428e0a38-7ab1-46ff-f2e0-197c2b164f89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 'orange', 2.71, False)\n"]}]},{"cell_type":"markdown","source":["Key Difference: Mutability\n","\n","The main distinction between lists and tuples is their mutability:\n","\n","Lists: Flexible for situations where you need to modify the sequence of items.\n","\n","Tuples: Suitable for representing fixed collections of data where you want to ensure that the contents remain unchanged.\n","\n","\n","When to Use Which\n","\n","Lists: Use lists when you need a collection that can be modified, like a to-do list or a shopping cart.\n","\n","Tuples: Use tuples when you need a fixed collection, such as coordinates, database records, or function return values where you want to guarantee the data integrity"],"metadata":{"id":"31IsrOv3qMoA"}},{"cell_type":"markdown","source":["13..\n","\n","Interpreted Language:\n","\n","Execution: Code is read and executed line by line by an interpreter at runtime. No separate compilation step is required.\n","\n","Flexibility: Changes to code can be tested immediately without recompiling the entire program.\n","\n","Portability: Often, the same code can run on different platforms with the appropriate interpreter.\n","\n","Examples: Python, JavaScript, Ruby, PHP.\n","\n","\n","Dynamically Typed Language:\n","\n","Type Checking: Variable types are checked at runtime, not during compilation.\n","\n","Flexibility: You don't need to explicitly declare variable types; the interpreter infers them.\n","\n","Ease of Use: Can lead to faster development, as you don't need to worry about strict type declarations.\n","\n","Potential Runtime Errors: Type mismatches might not be detected until the code is executed.\n","\n","Examples: Python, JavaScript, Ruby, PHP."],"metadata":{"id":"0ERZ6ZCPqbhL"}},{"cell_type":"markdown","source":["14..\n","\n","List Comprehensions:\n","\n","Syntax: [expression for item in iterable if condition]\n","\n","Purpose: Creates a new list by applying an expression to each item in an iterable (like a list, tuple, or range), optionally filtering items based on a condition.\n","\n","Example:"],"metadata":{"id":"r38_ybXJrr1g"}},{"cell_type":"code","source":["numbers = [1, 2, 3, 4, 5]\n","squares = [x**2 for x in numbers]  # [1, 4, 9, 16, 25]\n","even_squares = [x**2 for x in numbers if x % 2 == 0]  # [4, 16]"],"metadata":{"id":"q0fzhI7vqJE8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dictionary Comprehensions:\n","\n","Syntax: {key_expression: value_expression for item in iterable if condition}\n","\n","Purpose: Creates a new dictionary by generating key-value pairs from an iterable, optionally filtering items based on a condition.\n","\n","Example:"],"metadata":{"id":"D_CwPuOIr8s3"}},{"cell_type":"code","source":["names = [\"Alice\", \"Bob\", \"Charlie\"]\n","name_lengths = {name: len(name) for name in names}\n","# {'Alice': 5, 'Bob': 3, 'Charlie': 7}"],"metadata":{"id":"DfHplwZEr6hw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["15..\n","\n","Decorators in Python\n","\n","Decorators are a powerful and versatile tool in Python that allow you to modify the behavior of functions without permanently altering their original code. They achieve this by wrapping a function in another function, which adds additional functionality before or after the wrapped function's execution.\n","\n","Example: Adding Logging Functionality\n","\n","Here's an example of a decorator that logs the execution of a function:"],"metadata":{"id":"B5lyA2F4sDVy"}},{"cell_type":"code","source":["def my_decorator(func):\n","  def wrapper(*args, **kwargs):\n","    print(f\"Running {func.__name__} with args {args} and kwargs {kwargs}\")\n","    result = func(*args, **kwargs)\n","    print(f\"Function {func.__name__} returned {result}\")\n","    return result\n","  return wrapper\n","\n","@my_decorator\n","def say_hello(name):\n","  return f\"Hello, {name}!\"\n","\n","greeting = say_hello(\"Alice\")\n","print(greeting)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1XgWAN8sAj_","executionInfo":{"status":"ok","timestamp":1720280278982,"user_tz":-330,"elapsed":508,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"f56f19ee-fd7d-4c19-8605-57b41a797b80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running say_hello with args ('Alice',) and kwargs {}\n","Function say_hello returned Hello, Alice!\n","Hello, Alice!\n"]}]},{"cell_type":"markdown","source":["Use Cases for Decorators\n","\n","Decorators have a wide range of applications in Python, including:\n","\n","Authorization and Authentication: Checking user permissions before allowing access to specific functions.\n","\n","Logging and Debugging: Tracking function calls, arguments, and return values for troubleshooting.\n","\n","Caching: Storing results of expensive computations to avoid redundant calculations.\n","\n","Error Handling: Implementing common error handling patterns across functions.\n","\n","Performance Optimization: Measuring function execution time and identifying bottlenecks.\n","\n","Transaction Management: Ensuring data consistency in database operations.\n","\n","Aspect-Oriented Programming (AOP): Separating cross-cutting concerns like logging or error handling from core function logic."],"metadata":{"id":"QtUc-Gcgsi0l"}},{"cell_type":"markdown","source":["16..\n","\n","Automatic Management with Reference Counting and Garbage Collection\n","\n","Allocation: When you create objects (variables, lists, dictionaries, etc.) in Python, memory is allocated from a private heap managed by the CPython memory manager.\n","\n","Reference Counting: Each object has a reference count that keeps track of how many references (variables) point to it.\n","\n","Garbage Collection: The garbage collector is a background process that automatically reclaims memory occupied by objects no longer in use. It works by:\n","\n","Marking: Identifying reachable objects that are still accessible from your program. This involves tracing references from variables and data structures.\n","\n","Sweeping: Reclaiming memory occupied by unmarked objects (those with a reference count of zero).\n","\n","Key Points to Remember:\n","\n","You don't directly control memory allocation or deallocation in Python.\n","Reference counting and garbage collection work together to ensure efficient memory usage.\n","While automatic, understanding these concepts can help you write memory-efficient Python code.\n","\n","Additional Considerations:\n","\n","Circular References: If two objects hold references to each other and no other part of your program points to them, garbage collection might not reclaim them. Be mindful of circular references in your code.\n","\n","Forced Garbage Collection: There's no built-in way to force garbage collection in Python, but techniques like assigning None to variables or using the gc module can help nudge it along (use with caution)."],"metadata":{"id":"imR33U35spOJ"}},{"cell_type":"markdown","source":["17..\n","\n","Lambda Functions: Anonymous Expressions for Simple Tasks\n","\n","\n","Syntax: lambda arguments: expression\n","arguments: Comma-separated list of arguments the function can accept (can be zero or more).\n","\n","expression: A single expression that's evaluated and returned when the function is called.\n","\n","No Name: Unlike regular functions defined with def, lambda functions are unnamed (anonymous).\n","\n","\n","Why Use Lambda Functions?\n","\n","\n","Lambdas are ideal for short, throwaway functions needed in specific contexts. Here are some common use\n","\n"," cases:\n","\n","Sorting: You can use lambdas to define custom sorting criteria within functions like sorted(). For example, sorting a list by the length of strings:\n"],"metadata":{"id":"7hifRZQhtLwv"}},{"cell_type":"code","source":["fruits = [\"apple\", \"banana\", \"cherry\", \"passionfruit\"]\n","sorted_by_length = sorted(fruits, key=lambda fruit: len(fruit))\n","print(sorted_by_length)  # Output: ['cherry', 'apple', 'banana', 'passionfruit']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39Gmxqg4sd-C","executionInfo":{"status":"ok","timestamp":1720280537171,"user_tz":-330,"elapsed":758,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"0265bead-e6e4-4545-b81e-5917be6ac164"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['apple', 'banana', 'cherry', 'passionfruit']\n"]}]},{"cell_type":"markdown","source":["Filtering: Lambdas can be used with functions like filter() to create concise filtering logic. For example, filtering even numbers from a list:"],"metadata":{"id":"OSvii0wethtu"}},{"cell_type":"code","source":["numbers = [1, 2, 3, 4, 5]\n","even_numbers = list(filter(lambda num: num % 2 == 0, numbers))\n","print(even_numbers)  # Output: [2, 4]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDeahiudtdIO","executionInfo":{"status":"ok","timestamp":1720280566163,"user_tz":-330,"elapsed":574,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"b9b41ebe-d99b-4125-a984-88a80f42c6ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 4]\n"]}]},{"cell_type":"markdown","source":["Callbacks: Lambdas are handy for defining callbacks that are passed as arguments to other functions. For example, a function that takes a comparison function as a callback:"],"metadata":{"id":"Un-RUWaItomT"}},{"cell_type":"code","source":["def compare(item1, item2, compare_func):\n","  return compare_func(item1, item2)\n","\n","result = compare(5, 8, lambda a, b: a > b)\n","print(result)  # Output: False\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8pN6I9MHtkQU","executionInfo":{"status":"ok","timestamp":1720280594191,"user_tz":-330,"elapsed":5,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"0b4cf349-d270-484b-d531-72571b8332d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["Advantages of Lambda Functions:\n","\n","Conciseness: They offer a compact way to define small functions, improving code readability in certain situations.\n","\n","Readability: When used appropriately (for simple expressions), they can enhance readability within the context where they're used."],"metadata":{"id":"CyHkS_ujtuPs"}},{"cell_type":"markdown","source":["18..\n","\n","split()\n","\n","The split() method is used to split a string into a list of substrings based on a specified delimiter (separator). It's a method of the str class in Python.\n","\n","string: The string you want to split.\n","\n","sep (optional): The delimiter (separator) to use for splitting. Defaults to whitespace (any combination of spaces, tabs, or newlines).\n","\n","maxsplit (optional): The maximum number of splits to perform. Defaults to 0 (unlimited splits).\n"],"metadata":{"id":"ie_-wxY8tzPH"}},{"cell_type":"code","source":["sentence = \"This is a sentence. It has multiple words.\"\n","\n","# Split by whitespace (default delimiter)\n","words = sentence.split()\n","print(words)  # Output: ['This', 'is', 'a', 'sentence.', 'It', 'has', 'multiple', 'words.']\n","\n","# Split by comma\n","words = sentence.split(',')\n","print(words)  # Output: ['This is a sentence. It has multiple words.'] (no split because there's no comma)\n","\n","# Split by comma, limiting to 2 splits\n","phrases = sentence.split(',', maxsplit=2)\n","print(phrases)  # Output: ['This is a sentence', ' It has multiple words.']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8I3UlbuQtq0F","executionInfo":{"status":"ok","timestamp":1720280706326,"user_tz":-330,"elapsed":1088,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"b18af5e7-bd0b-488f-a5d0-9d006ae9bf0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'sentence.', 'It', 'has', 'multiple', 'words.']\n","['This is a sentence. It has multiple words.']\n","['This is a sentence. It has multiple words.']\n"]}]},{"cell_type":"markdown","source":["join()\n","\n","The join() method is used to join the elements of an iterable (like a list) into a single string, using a specified separator (delimiter). It's a method of the str class in Python.\n","\n","sep: The separator to use between the elements when joining.\n","\n","iterable: The iterable containing the elements to join. This can be a list, tuple, or any other iterable."],"metadata":{"id":"fXzZqB4AuJgl"}},{"cell_type":"code","source":["words = [\"This\", \"is\", \"a\", \"sentence.\"]\n","\n","# Join with spaces (default delimiter)\n","joined_sentence = \" \".join(words)\n","print(joined_sentence)  # Output: This is a sentence.\n","\n","# Join with commas\n","joined_sentence = \",\".join(words)\n","print(joined_sentence)  # Output: This,is,a,sentence.\n","\n","# Join with hyphens\n","joined_sentence = \"-\".join(words)\n","print(joined_sentence)  # Output: This-is-a-sentence.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swMJz6YxuGL9","executionInfo":{"status":"ok","timestamp":1720280751380,"user_tz":-330,"elapsed":807,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"58f77d65-50ce-477b-ee46-8830be642c1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is a sentence.\n","This,is,a,sentence.\n","This-is-a-sentence.\n"]}]},{"cell_type":"markdown","source":["19..\n","\n","Iterators\n","\n","Concept: An iterator is an object that represents a stream of values and allows you to access them one at a time. It's like a special pointer that keeps track of its position within the sequence.\n","\n","Functionality: Iterators provide a way to efficiently process elements in a sequence without having to load the entire sequence into memory at once. This is particularly useful for large datasets.\n","\n","Methods: Iterators implement the __next__() method, which returns the next element in the sequence. They may also have an __iter__() method, which returns the iterator itself (useful for certain use cases).\n","\n","Example: You can create an iterator from a list using the built-in iter() function:"],"metadata":{"id":"yCK5NfhVufpE"}},{"cell_type":"code","source":["numbers = [1, 2, 3, 4, 5]\n","iterator = iter(numbers)\n","\n","# Access elements one by one using next()\n","print(next(iterator))  # Output: 1\n","print(next(iterator))  # Output: 2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_U4_JEAuQ7z","executionInfo":{"status":"ok","timestamp":1720280831529,"user_tz":-330,"elapsed":527,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"d8bd36cc-aa2e-4dee-e26c-3d9703dc8c71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n"]}]},{"cell_type":"markdown","source":["Iterables\n","\n","Concept: An iterable is an object that can be used to create an iterator. It's like a container that holds the elements of a sequence but doesn't provide a direct way to access them individually.\n","\n","Functionality: Iterables provide a way to iterate over their elements using a for loop or other functions like map() and filter().\n","\n","Examples: Common iterables in Python include lists, tuples, strings, dictionaries (for keys), and sets."],"metadata":{"id":"uuF6Iy2Xupw-"}},{"cell_type":"markdown","source":["Generators\n","\n","Concept: A generator is a special type of function that returns an iterator. It generates values on demand, one at a time, instead of returning a complete list or sequence at once.\n","\n","Functionality: Generators are memory-efficient for creating sequences of elements, especially for infinite or very large sequences. They use the yield keyword to pause execution and return a value, and then resume execution when next() is called on the resulting iterator.\n","\n","Example: Here's a generator function that generates squares up to a certain number:"],"metadata":{"id":"kzG3X-Wkut8P"}},{"cell_type":"code","source":["def square_generator(n):\n","  for i in range(n):\n","    yield i * i\n","\n","# Get the iterator from the generator function\n","squares = square_generator(5)\n","\n","# Access elements using next()\n","print(next(squares))  # Output: 0\n","print(next(squares))  # Output: 1\n","print(next(squares))  # Output: 4\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BXDrh7sulEk","executionInfo":{"status":"ok","timestamp":1720280894904,"user_tz":-330,"elapsed":519,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"120ac037-d982-4658-c0e3-d897618cc0b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","4\n"]}]},{"cell_type":"markdown","source":["20..\n","\n","xrange (Python 2 only)\n","\n","Concept: xrange was a function in Python 2 that returned an iterator, meaning it generated numbers on demand as you iterated through it. This made it memory-efficient, especially for large sequences.\n","\n","Memory Usage: Since xrange didn't create the entire sequence in memory at once, it was ideal for handling very large numbers or infinite sequences without running into memory issues.\n","\n","Drawbacks:\n","\n","Not directly indexable: You couldn't directly access elements by index using xrange objects (e.g., xrange(10)[3]).\n","\n","Deprecated in Python 3: xrange is no longer available in Python 3.\n","\n","range (Python 2 and 3)\n","\n","Concept: range is a function available in both Python 2 and 3. It returns a list containing the sequence of numbers.\n","\n","Memory Usage: range creates the entire sequence in memory, which can be less efficient for very large sequences.\n","\n","Functionality:\n","Direct indexing: You can access elements by index using range objects (e.g., range(10)[3] gives you the fourth element).\n","\n","More versatile: range can be used for various operations like slicing and concatenation, which might not be as straightforward with xrange iterators."],"metadata":{"id":"QOhZT9Y0vAeF"}},{"cell_type":"markdown","source":["21..\n","\n","\n","Abstraction\n","\n","Encapsulation\n","\n","Inheritance\n","\n","Polymorphism\n","\n","These principles help you create reusable, modular, and maintainable code by focusing on objects and their interactions.\n","\n","1. Abstraction\n","\n","Abstraction focuses on providing a simplified view of an object, hiding unnecessary details from the user. It allows you to represent the essential features of an object without exposing its internal workings.\n","Imagine a car. As a driver, you interact with the steering wheel, pedals, and other controls without needing to know about the complex mechanics of the engine, transmission, and other internal components. This is abstraction in action.\n","\n","2. Encapsulation\n","\n","Encapsulation is the process of bundling data (attributes) and methods (functions that operate on that data) together within a single unit called a class. This restricts direct access to an object's internal state and ensures data integrity by controlling how data is manipulated.\n","Continuing the car analogy, the car's engine and other internal components are encapsulated. You can't directly tamper with them; you interact with the car through its exposed controls and interfaces (like the gas pedal) that manage how the engine functions.\n","\n","3. Inheritance\n","\n","Inheritance allows you to create new classes (subclasses) that inherit properties and behaviors from existing classes (parent classes). This promotes code reusability and enables the creation of specialized objects based on more general ones.\n","Think of different types of vehicles. A sports car class might inherit general properties (like wheels and engine) from a vehicle class, while adding its own specific features (like a high-performance engine).\n","\n","4. Polymorphism\n","\n","Polymorphism allows objects of different classes to respond to the same method call in different ways. This provides flexibility and simplifies code that interacts with multiple object types.\n","Consider a move() method. It might have different implementations for a car (driving forward), a bird (flying), or a fish (swimming). The code calling move() doesn't need to know the specific object type; it just receives the appropriate behavior based on the actual object."],"metadata":{"id":"IlLq19TYvPkF"}},{"cell_type":"markdown","source":["22..\n","\n","In Python, you can use the built-in function issubclass() to check if a class is a child (subclass) of another class. Here's how it works:"],"metadata":{"id":"wdjdGy98vjYy"}},{"cell_type":"code","source":["def is_subclass(sub_class, parent_class):\n","  \"\"\"Checks if sub_class is a subclass of parent_class.\n","\n","  Args:\n","      sub_class: The class to check if it's a subclass.\n","      parent_class: The potential parent class.\n","\n","  Returns:\n","      True if sub_class is a subclass of parent_class, False otherwise.\n","  \"\"\"\n","\n","  return issubclass(sub_class, parent_class)\n","\n","# Example usage\n","class Animal:\n","  pass\n","\n","class Dog(Animal):\n","  pass\n","\n","class Cat:\n","  pass\n","\n","print(is_subclass(Dog, Animal))  # Output: True\n","print(is_subclass(Cat, Animal))  # Output: False\n","print(is_subclass(Animal, Dog))  # Output: False (a class is not a subclass of its own subclass)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4hDxQ2swu0Wt","executionInfo":{"status":"ok","timestamp":1720281133299,"user_tz":-330,"elapsed":481,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"11c779bc-f771-4e6c-d65a-172b233f2b9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n","False\n"]}]},{"cell_type":"markdown","source":["23..\n","Inheritance in Python is a powerful mechanism that allows you to create new classes (subclasses) that inherit properties and behaviors from existing classes (parent classes). This promotes code reusability, reduces redundancy, and helps you organize your code in a hierarchical manner. Here's a breakdown of different types of inheritance in Python with examples:\n","\n"," Single Inheritance\n","\n","This is the most basic and common type of inheritance. A subclass inherits from a single parent class.\n","\n","Example:"],"metadata":{"id":"lfRpyz-qvwOc"}},{"cell_type":"code","source":["class Animal:\n","  def __init__(self, name):\n","    self.name = name\n","\n","  def make_sound(self):\n","    print(\"Generic animal sound\")\n","\n","class Dog(Animal):\n","  def __init__(self, name, breed):\n","    super().__init__(name)  # Call parent class constructor\n","    self.breed = breed\n","\n","  def make_sound(self):\n","    print(\"Woof!\")\n","\n","# Create an object of the subclass\n","my_dog = Dog(\"Fido\", \"Labrador\")\n","print(my_dog.name)  # Output: Fido\n","print(my_dog.breed)  # Output: Labrador\n","my_dog.make_sound()  # Output: Woof!\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uC2qZ7XLvutc","executionInfo":{"status":"ok","timestamp":1720281207619,"user_tz":-330,"elapsed":671,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"3b4bbb44-1648-44eb-a50d-1caf7df0ef3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fido\n","Labrador\n","Woof!\n"]}]},{"cell_type":"markdown","source":["Multilevel Inheritance\n","\n","In this type of inheritance, a subclass inherits from another subclass, which in turn inherits from a parent class, forming a chain of inheritance.\n","\n","Example:"],"metadata":{"id":"E7aEQG0ZwCPI"}},{"cell_type":"code","source":["class Vehicle:\n","  def __init__(self, make, model):\n","    self.make = make\n","    self.model = model\n","\n","  def move(self):\n","    print(\"The vehicle is moving.\")\n","\n","class Car(Vehicle):\n","  def __init__(self, make, model, wheels):\n","    super().__init__(make, model)\n","    self.wheels = wheels\n","\n","  def move(self):\n","    print(f\"The {self.make} {self.model} car is moving on {self.wheels} wheels.\")\n","\n","class ElectricCar(Car):\n","  def __init__(self, make, model, wheels, battery_size):\n","    super().__init__(make, model, wheels)\n","    self.battery_size = battery_size\n","\n","  def move(self):\n","    print(f\"The electric {self.make} {self.model} car is moving silently using its {self.battery_size}kWh battery.\")\n","\n","# Create an object of the subclass\n","my_electric_car = ElectricCar(\"Tesla\", \"Model S\", 4, 100)\n","print(my_electric_car.make)  # Output: Tesla\n","print(my_electric_car.wheels)  # Output: 4\n","my_electric_car.move()  # Output: The electric Tesla Model S car is moving silently using its 100kWh battery.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ec2-5lR1wA29","executionInfo":{"status":"ok","timestamp":1720281236213,"user_tz":-330,"elapsed":638,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"3df51ecb-fbf6-4a05-8491-d7dbfb1d6021"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla\n","4\n","The electric Tesla Model S car is moving silently using its 100kWh battery.\n"]}]},{"cell_type":"markdown","source":[" Multiple Inheritance\n","\n","In this type of inheritance, a subclass inherits from multiple parent classes. This can be useful for combining functionalities from different classes, but it can also lead to complexity and potential diamond problems (conflicts when multiple parent classes have the same method name)."],"metadata":{"id":"9m_ruo1kwLQT"}},{"cell_type":"code","source":["class Animal:\n","  def make_sound(self):\n","    print(\"Generic animal sound\")\n","\n","class Drawable:\n","  def draw(self):\n","    print(\"Drawing an object\")\n","\n","class TalkingAnimal(Animal, Drawable):  # MRO: TalkingAnimal, Animal, Drawable\n","  def __init__(self, name):\n","    self.name = name\n","\n","  def make_sound(self):\n","    print(f\"{self.name} says something\")\n","\n","# Create an object of the subclass\n","my_talking_dog = TalkingAnimal(\"Fido\")\n","my_talking_dog.make_sound()   # Output: Fido says something (from TalkingAnimal)\n","my_talking_dog.draw()         # Output: Drawing an object (from Drawable)\n","\n","# Python uses Method Resolution Order\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"erhTxepEwHzU","executionInfo":{"status":"ok","timestamp":1720281264071,"user_tz":-330,"elapsed":492,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"235ccfd3-0e42-4dea-f309-61941eb35cea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fido says something\n","Drawing an object\n"]}]},{"cell_type":"markdown","source":["24..\n","\n","Encapsulation in Python is a fundamental object-oriented programming (OOP) principle that focuses on bundling data (attributes) and methods (functions that operate on that data) together within a single unit called a class. It's a way of promoting data hiding and protecting the internal state of an object.\n","\n","Key Points of Encapsulation:\n","\n","Data Hiding: Encapsulation restricts direct access to an object's attributes. This is achieved by using private attributes, which are prefixed with double underscores (__) by convention. While technically accessible from outside the class, modifying private attributes directly is discouraged, as it can break the class's intended behavior.\n","Controlled Access: Methods within the class can access and modify the object's attributes in a controlled manner. This ensures data integrity and consistency.\n","\n","Benefits of Encapsulation:\n","\n","Improved Data Security: Data hiding protects sensitive information from accidental or unauthorized modification.\n","Increased Maintainability: Changes made to the internal implementation of a class are less likely to affect other parts of the code if the interface (public methods) remains consistent.\n","Modular Code: Encapsulation promotes code reusability by creating well-defined units (classes) with clear boundaries."],"metadata":{"id":"K7UypWfnwT7v"}},{"cell_type":"code","source":["class BankAccount:\n","  def __init__(self, balance):\n","    self.__balance = balance  # Private attribute\n","\n","  def get_balance(self):\n","    \"\"\"Returns the current balance.\"\"\"\n","    return self.__balance\n","\n","  def deposit(self, amount):\n","    \"\"\"Deposits money into the account.\"\"\"\n","    self.__balance += amount\n","\n","  def withdraw(self, amount):\n","    \"\"\"Withdraws money from the account if sufficient funds are available.\"\"\"\n","    if amount <= self.__balance:\n","      self.__balance -= amount\n","      return True\n","    else:\n","      return False\n","\n","# Create a bank account object\n","my_account = BankAccount(1000)\n","\n","# Accessing balance directly is discouraged (data hiding)\n","# print(my_account.__balance)  # Not recommended\n","\n","print(my_account.get_balance())  # Output: 1000 (using public method)\n","\n","my_account.deposit(500)\n","print(my_account.get_balance())  # Output: 1500\n","\n","if my_account.withdraw(800):\n","  print(\"Withdrawal successful\")\n","else:\n","  print(\"Insufficient funds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObAD36zYwOo8","executionInfo":{"status":"ok","timestamp":1720281356859,"user_tz":-330,"elapsed":514,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"52bbb2e0-dda8-4091-a737-062bc8a42d85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1000\n","1500\n","Withdrawal successful\n"]}]},{"cell_type":"markdown","source":["25..\n","\n","Polymorphism in Python is a core concept in object-oriented programming (OOP) that allows objects of different classes to respond to the same method call in different ways. This flexibility promotes code reusability and simplifies interactions with various object types.\n","\n","Key Aspects of Polymorphism:\n","\n","Method Overriding: Subclasses (child classes) can inherit methods from parent classes and redefine their behavior to provide specialized implementations. When you call the same method on objects of different classes, the appropriate implementation based on the object's type is executed.\n","\n","Duck Typing: Python focuses on what an object can do (its methods) rather than its specific class. As long as an object has the required methods, it can be used in a certain context.\n","\n","Benefits of Polymorphism:\n","\n","Code Reusability: By using generic methods that work with different object types, you can write more flexible code that adapts to various scenarios.\n","\n","Maintainability: Changes made to a subclass's method implementation won't necessarily break existing code that interacts with the parent class, as long as the method signature (name and arguments) remains the same.\n","\n","Example:"],"metadata":{"id":"7BZr0yjCwt6N"}},{"cell_type":"code","source":["class Animal:\n","  def make_sound(self):\n","    print(\"Generic animal sound\")\n","\n","class Dog(Animal):\n","  def make_sound(self):\n","    print(\"Woof!\")\n","\n","class Cat(Animal):\n","  def make_sound(self):\n","\n","    print(\"Meow!\")\n","\n","# Create objects of different classes\n","animals = [Dog(), Cat()]\n","\n","# Call the same method on each object (polymorphism in action)\n","for animal in animals:\n","  animal.make_sound()  # Output: Woof! (for Dog), Meow! (for Cat)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9MVjEBiwlEC","executionInfo":{"status":"ok","timestamp":1720281429515,"user_tz":-330,"elapsed":1775,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"9d7da97a-1027-4425-a681-33c74971f171"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Woof!\n","Meow!\n"]}]},{"cell_type":"markdown","source":["1.2\n","\n","Invalid Identifiers:\n","\n","b) 1st_Room: Identifiers cannot start with a digit.\n","\n","c) Hundred: Identifiers can only contain letters (uppercase and lowercase), underscores (_), and numbers (0-9). Special characters like 1 are not allowed.\n","\n","e) total-Marks: Identifiers cannot contain hyphens (-) except as the first character (for special cases).\n","\n","Valid Identifiers:\n","\n","a) Serial_no.: This identifier is valid because it starts with a letter, contains underscores, and ends with letters or numbers.\n","\n","d) Total_Marks: Similar to Serial_no., this identifier adheres to the naming rules.\n","\n","f) Total Marks: While technically not recommended due to the space, Python allows identifiers with spaces. However, it's generally better practice to use underscores for readability (e.g., Total_Marks).\n","\n","g) True: This is a reserved keyword in Python, so using it as an identifier would lead to a syntax error.\n","\n","h) _Percentage: Identifiers can start with an underscore, which is often used for private or internal variables/functions within a class or module."],"metadata":{"id":"fv45bSR1xMxs"}},{"cell_type":"code","source":[],"metadata":{"id":"bjj343VwyOxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1.3\n","\n","\n"],"metadata":{"id":"GYC0b1bDyP8-"}},{"cell_type":"code","source":["\n","name = [\"Mohan\", \"dash\", \"karam\", \"chandra\",\"gandhi\",\"Bapu\"]"],"metadata":{"id":"6qBSrWzgy_q5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#a) Adding an element:\n","\n","name.insert(0, \"freedom_fighter\")\n"],"metadata":{"id":"Xs03wSGQzDLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Gg8I3m5rzK0j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find the Output of the Given Code:\n","\n","The given code snippet is"],"metadata":{"id":"HgA4vRCvzpRN"}},{"cell_type":"code","source":["name = [\"freedomFighter\", \"Bapuji\", \"Mohan\" \"dash\", \"karam\", \"chandra\", \"gandhi\"]\n","Length1 = len(name[-len(name) + 1: -1: 2])\n","Length2 = len(name[-len(name) + 1: -1])\n","print(Length1 + Length2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFTVk-3j1JtL","executionInfo":{"status":"ok","timestamp":1720282558319,"user_tz":-330,"elapsed":624,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"d992b7e1-f18c-490a-cf13-30e0859e8be3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6\n"]}]},{"cell_type":"markdown","source":["Let’s break it down:\n","\n","name[-len(name) + 1: -1: 2] selects every second element from the reversed list (excluding the last element).\n","\n","Length1 will be the length of this selected sublist.\n","\n","Length2 will be the length of the same sublist without the step size.\n","\n","The output will be the sum of Length1 and Length2.\n","\n","Add Two More Elements at the End:\n","\n","We can simply use the extend() method to add the elements “Netaji” and “Bose” at the end of the list:"],"metadata":{"id":"qDuzqIHq1QM9"}},{"cell_type":"code","source":["name = [\"Mohan\", \"dash\", \"karam\", \"chandra\", \"gandhi\"]\n","name.insert(0, \"freedom_fighter\")\n","Length1 = len(name[-len(name) + 1: -1: 2])\n","Length2 = len(name[-len(name) + 1: -1])\n","print(\"Modified list:\", name)\n","print(\"Output of code:\", Length1 + Length2)\n","\n","# Adding two more elements\n","name.extend([\"Netaji\", \"Bose\"])\n","print(\"Updated list:\", name)\n","\n","# Value of temp\n","temp = name[-1]\n","print(\"Value of temp:\", temp)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIPOzYCw1Kno","executionInfo":{"status":"ok","timestamp":1720282616844,"user_tz":-330,"elapsed":504,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"2543cbae-2987-46ae-e861-dae1cc53e2b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Modified list: ['freedom_fighter', 'Mohan', 'dash', 'karam', 'chandra', 'gandhi']\n","Output of code: 6\n","Updated list: ['freedom_fighter', 'Mohan', 'dash', 'karam', 'chandra', 'gandhi', 'Netaji', 'Bose']\n","Value of temp: Bose\n"]}]},{"cell_type":"markdown","source":["1.4\n"],"metadata":{"id":"X-QhgnjisGPn"}},{"cell_type":"code","source":["2\n","4\n","7\n"],"metadata":{"id":"YuO7MiG-1Y0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1.5\n"],"metadata":{"id":"Zfke5SSgugO6"}},{"cell_type":"code","source":["tuple1 = (10, 20, \"Apple\", 3.4, 'a', [\"master\", \"ji\"], (\"sita\", \"geeta\", 22), [{\"roll_no\": \"N1\"}, {\"name\": \"Navneet\"}])\n"],"metadata":{"id":"nxA1MWt_wQZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(tuple1))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qNGmYz9vX2k","executionInfo":{"status":"ok","timestamp":1720449235684,"user_tz":-330,"elapsed":8,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"0a8eb743-9798-4614-c58c-11cfbbd9fe25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}]},{"cell_type":"markdown","source":["b) Print(tuple1[-1][-1][\"name\"])\n","\n","This code accesses the nested dictionary within the last element of the tuple (tuple1[-1]) and then retrieves the value associated with the key \"name\" within that dictionary. However, there appears to be a typo in the original code (N\"Navneet\"). Assuming it's meant to be a string \"Navneet\", the output will be:"],"metadata":{"id":"rvpXWFESsz1w"}},{"cell_type":"code","source":["print(tuple1[-1][-1][\"name\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OuS0c0J0viiC","executionInfo":{"status":"ok","timestamp":1720449051179,"user_tz":-330,"elapsed":5,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"0f6a2e04-a405-4c3b-e1f3-972749db1327"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Navneet\n"]}]},{"cell_type":"markdown","source":["c) Fetch the value of roll_no from this tuple.\n","\n","To access the value of roll_no, you'll need to iterate through the second dictionary in the last element (tuple1[-1][1]). The original code seems to have an error (N1), so let's assume it's a placeholder for an actual numeric value. Here's the corrected code:"],"metadata":{"id":"cqSuLa93s85g"}},{"cell_type":"code","source":["print(tuple1[-1][0][\"roll_no\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Od8LpF5evpI0","executionInfo":{"status":"ok","timestamp":1720449055286,"user_tz":-330,"elapsed":448,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"1cce6b47-299d-40ea-9709-5deff4d666b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["N1\n"]}]},{"cell_type":"markdown","source":["d) Print(tuple1[-3][1])\n","\n","This code retrieves the second element ([1]) from the third element from the end of the tuple (tuple1[-3]). In this case, it accesses the value \"geeta\" from the tuple within tuple1. The output will be:"],"metadata":{"id":"8BZTZrVusQcI"}},{"cell_type":"code","source":["print(tuple1[-3][1])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BC9jL-Mfvw_7","executionInfo":{"status":"ok","timestamp":1720449423780,"user_tz":-330,"elapsed":1183,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"67dbb5d4-c898-4fa1-c854-2b4af41b6db5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ji\n"]}]},{"cell_type":"markdown","source":["e) Fetch the element \"22\" from this tuple.\n","\n","The element \"22\" exists within a nested tuple ((\"sita\", \"geeta\", 22)) but cannot be directly extracted as a single element. You can either access it using its position within the nested tuple or iterate through the entire tuple to find it. Here are two approaches:\n","\n","Accessing by position:"],"metadata":{"id":"CHmzXBTBuRVX"}},{"cell_type":"code","source":["# Attempt to access the element and handle the potential IndexError\n","try:\n","    print(tuple1[-3][2])\n","except IndexError:\n","    print(\"Error: Index out of range. The element at tuple1[-3] does not have a third element.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nvz5ls1uMr-","executionInfo":{"status":"ok","timestamp":1720449391647,"user_tz":-330,"elapsed":462,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"28560f41-cbb0-4c17-a8b0-00bb460f959e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: Index out of range. The element at tuple1[-3] does not have a third element.\n"]}]},{"cell_type":"markdown","source":["1.6\n"],"metadata":{"id":"biesMUD8x0Yx"}},{"cell_type":"code","source":["def traffic_signal(color):\n","    if color.lower() == 'red':\n","        print(\"Stop\")\n","    elif color.lower() == 'yellow':\n","        print(\"Stay\")\n","    elif color.lower() == 'green':\n","        print(\"Go\")\n","    else:\n","        print(\"Invalid color\")\n","\n","# Test cases\n","traffic_signal(\"RED\")\n","traffic_signal(\"Yellow\")\n","traffic_signal(\"Green\")\n","traffic_signal(\"Blue\")\n"],"metadata":{"id":"JClhQlw3uUMn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720449505297,"user_tz":-330,"elapsed":7,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"f78184ba-555f-4188-f433-d8ccee254b11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stop\n","Stay\n","Go\n","Invalid color\n"]}]},{"cell_type":"markdown","source":["1.7"],"metadata":{"id":"d5458ebZyHKy"}},{"cell_type":"code","source":["# Function to add two numbers\n","def add(x, y):\n","    return x + y\n","\n","# Function to subtract two numbers\n","def subtract(x, y):\n","    return x - y\n","\n","# Function to multiply two numbers\n","def multiply(x, y):\n","    return x * y\n","\n","# Function to divide two numbers\n","def divide(x, y):\n","    if y == 0:\n","        return \"Cannot divide by zero\"\n","    else:\n","        return x / y\n","\n","# Main function to perform calculations\n","def calculator():\n","    print(\"Select operation:\")\n","    print(\"1. Addition\")\n","    print(\"2. Subtraction\")\n","    print(\"3. Multiplication\")\n","    print(\"4. Division\")\n","\n","    choice = input(\"Enter choice (1/2/3/4): \")\n","\n","    num1 = float(input(\"Enter first number: \"))\n","    num2 = float(input(\"Enter second number: \"))\n","\n","    if choice == '1':\n","        print(f\"{num1} + {num2} = {add(num1, num2)}\")\n","    elif choice == '2':\n","        print(f\"{num1} - {num2} = {subtract(num1, num2)}\")\n","    elif choice == '3':\n","        print(f\"{num1} * {num2} = {multiply(num1, num2)}\")\n","    elif choice == '4':\n","        print(f\"{num1} / {num2} = {divide(num1, num2)}\")\n","    else:\n","        print(\"Invalid input\")\n","\n","# Calling the calculator function to start the program\n","calculator()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P88j5kMxyBI5","executionInfo":{"status":"ok","timestamp":1720449581158,"user_tz":-330,"elapsed":14330,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"1834c35b-b72f-4f2d-cc17-2ff9a77e108f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Select operation:\n","1. Addition\n","2. Subtraction\n","3. Multiplication\n","4. Division\n","Enter choice (1/2/3/4): 1\n","Enter first number: 55\n","Enter second number: 55\n","55.0 + 55.0 = 110.0\n"]}]},{"cell_type":"markdown","source":["1.8"],"metadata":{"id":"AfxfpjhdyXdK"}},{"cell_type":"code","source":["# Pre-specified numbers\n","num1 = 25\n","num2 = 40\n","num3 = 15\n","\n","# Using conditional expressions to find the largest number\n","largest = num1 if (num1 >= num2 and num1 >= num3) else (num2 if (num2 >= num1 and num2 >= num3) else num3)\n","\n","# Displaying the largest number\n","print(f\"The largest number among {num1}, {num2}, and {num3} is: {largest}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fRH5ZaeVyGlH","executionInfo":{"status":"ok","timestamp":1720449637039,"user_tz":-330,"elapsed":446,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"2a0ca611-1ae3-4a33-f16c-ef1586229559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The largest number among 25, 40, and 15 is: 40\n"]}]},{"cell_type":"markdown","source":["1.9"],"metadata":{"id":"i5lc5Gmlyj-k"}},{"cell_type":"code","source":["def find_factors(number):\n","    # Initialize variables\n","    i = 1\n","    factors = []\n","\n","    # Iterate through numbers from 1 to number\n","    while i <= number:\n","        if number % i == 0:\n","            factors.append(i)  # i is a factor of the number\n","        i += 1\n","\n","    return factors\n","\n","# Input the number for which you want to find factors\n","number = int(input(\"Enter a whole number: \"))\n","\n","# Call the function to find factors and print the result\n","factors = find_factors(number)\n","print(f\"The factors of {number} are: {factors}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hXg6u05yhZK","executionInfo":{"status":"ok","timestamp":1720449694011,"user_tz":-330,"elapsed":9545,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"fe4b0046-6a03-427e-a0fa-ac0636d12b9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a whole number: 5\n","The factors of 5 are: [1, 5]\n"]}]},{"cell_type":"markdown","source":["1.10"],"metadata":{"id":"1yh6pExfyxZg"}},{"cell_type":"code","source":["def sum_positive_numbers():\n","    total_sum = 0\n","    while True:\n","        num = float(input(\"Enter a number (enter a negative number to stop): \"))\n","        if num < 0:\n","            break\n","        total_sum += num\n","    print(f\"The sum of all positive numbers entered is: {total_sum}\")\n","\n","# Call the function to compute the sum of positive numbers\n","sum_positive_numbers()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3gAnVk2tyszh","executionInfo":{"status":"ok","timestamp":1720449750745,"user_tz":-330,"elapsed":11356,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"47558eba-a70d-4bb5-9805-457273b48f4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a number (enter a negative number to stop): 25\n","Enter a number (enter a negative number to stop): -66\n","The sum of all positive numbers entered is: 25.0\n"]}]},{"cell_type":"markdown","source":["1.11"],"metadata":{"id":"8EikbrFhzJOf"}},{"cell_type":"code","source":["# Initialize an empty list to store prime numbers\n","prime_numbers = []\n","\n","# Iterate through numbers from 2 to 100\n","for num in range(2, 101):\n","    is_prime = True\n","\n","    # Check for factors from 2 to the square root of num\n","    for i in range(2, int(num**0.5) + 1):\n","        if num % i == 0:\n","            is_prime = False\n","            break\n","\n","    # If num is prime, add it to the list of prime_numbers\n","    if is_prime:\n","        prime_numbers.append(num)\n","\n","# Print the prime numbers found\n","print(\"Prime numbers between 2 and 100:\")\n","print(prime_numbers)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnZw6Tcsy6Rq","executionInfo":{"status":"ok","timestamp":1720449805165,"user_tz":-330,"elapsed":708,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"17a5c914-73a3-46bf-9e24-b63e7fe87872"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prime numbers between 2 and 100:\n","[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"]}]},{"cell_type":"markdown","source":["1.12"],"metadata":{"id":"27mdY2Jy01x6"}},{"cell_type":"code","source":["# Accept the marks of students in five major subjects\n","marks = []\n","for i in range(5):\n","    subject_marks = float(input(f\"Enter marks for subject {i+1}: \"))\n","    marks.append(subject_marks)\n","\n","# Calculate the sum of marks\n","total_marks = sum(marks)\n","\n","# Calculate the percentage\n","percentage = total_marks / 5\n","\n","# Determine the grade based on percentage\n","if percentage > 85:\n","    grade = 'A'\n","elif 75 <= percentage <= 85:\n","    grade = 'B'\n","elif 50 <= percentage < 75:\n","    grade = 'C'\n","elif 30 <= percentage < 50:\n","    grade = 'D'\n","else:\n","    grade = 'Reappear'\n","\n","# Display results\n","print(f\"Total Marks: {total_marks}\")\n","print(f\"Percentage: {percentage:.2f}%\")\n","print(f\"Grade: {grade}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpVNZNMdzKXn","executionInfo":{"status":"ok","timestamp":1720450287942,"user_tz":-330,"elapsed":31178,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"0984f114-e7fe-4ed4-fd03-8f93cf7fece4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter marks for subject 1: 555\n","Enter marks for subject 2: 55\n","Enter marks for subject 3: 45\n","Enter marks for subject 4: 44\n","Enter marks for subject 5: 55\n","Total Marks: 754.0\n","Percentage: 150.80%\n","Grade: A\n"]}]},{"cell_type":"markdown","source":["1.13"],"metadata":{"id":"o0Zkifu21jGb"}},{"cell_type":"code","source":["# Define the VIBGYOR spectrum with their respective wavelength ranges in nanometers.\n","VIBGYOR_spectrum = {\n","    'Violet': (400.0, 440.0),\n","    'Indigo': (440.0, 460.0),\n","    'Blue': (460.0, 500.0),\n","    'Green': (500.0, 570.0),\n","    'Yellow': (570.0, 590.0),\n","    'Orange': (590.0, 620.0),\n","    'Red': (620.0, 720.0)\n","}\n","\n","# Function to determine the color based on wavelength.\n","def get_color(wavelength):\n","    for color, (start, end) in VIBGYOR_spectrum.items():\n","        if start <= wavelength <= end:\n","            return color\n","    return \"Wavelength not within the visible spectrum.\"\n","\n","# Example usage:\n","wavelength_input = float(input(\"Enter a wavelength in nanometers: \"))\n","color_output = get_color(wavelength_input)\n","print(f\"The color corresponding to {wavelength_input}nm is {color_output}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYI7hgkl04op","executionInfo":{"status":"ok","timestamp":1720450449916,"user_tz":-330,"elapsed":9418,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"21aea8d1-f451-4143-d401-3b6075bafc8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a wavelength in nanometers: 444\n","The color corresponding to 444.0nm is Indigo.\n"]}]},{"cell_type":"markdown","source":["1.14"],"metadata":{"id":"Tgg8VMwp4107"}},{"cell_type":"code","source":["#1.\n","\n","# Given data\n","mass_earth = 5.972e24  # Mass of Earth in kilograms\n","mass_sun = 0.989e30    # Mass of Sun in kilograms\n","dist_earth_sun = 149.6e9  # Average distance between Earth and Sun in meters\n","\n","# Gravitational constant\n","G = 6.67430e-11  # m^3 kg^(-1) s^(-2)\n","\n","# Calculate gravitational force\n","F_earth_sun = G * (mass_earth * mass_sun) / dist_earth_sun**2\n","\n","print(f\"Gravitational force between Earth and Sun: {F_earth_sun:.2e} N\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-V1UNNRS45hd","executionInfo":{"status":"ok","timestamp":1720451453366,"user_tz":-330,"elapsed":10,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"2f73f21c-1fc6-4f17-be23-72dd9fdc3807"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gravitational force between Earth and Sun: 1.76e+22 N\n"]}]},{"cell_type":"code","source":["#2\n","\n","# Given data\n","mass_moon = 7.34767309e22  # Mass of Moon in kilograms\n","dist_moon_earth = 384.4e6  # Average distance between Moon and Earth in meters\n","\n","# Calculate gravitational force\n","F_moon_earth = G * (mass_moon * mass_earth) / dist_moon_earth**2\n","\n","print(f\"Gravitational force between Moon and Earth: {F_moon_earth:.2e} N\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6P5-a6J5c5Y","executionInfo":{"status":"ok","timestamp":1720451480231,"user_tz":-330,"elapsed":605,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"4c5e207c-ee43-46b0-9086-f6f138ad9718"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gravitational force between Moon and Earth: 1.98e+20 N\n"]}]},{"cell_type":"markdown","source":["3. Gravitational force between Earth and Sun: 1.76e+22 N\n"," is stronger\n"],"metadata":{"id":"Vm0kpfP3527F"}},{"cell_type":"markdown","source":["4.The celestial body with the stronger gravitational pull will be the one experiencing a greater gravitational force. This means the body with the higher value of F (force) is more attracted to the other."],"metadata":{"id":"aOYBgjH16HXD"}},{"cell_type":"markdown","source":["2.\n"],"metadata":{"id":"fP0t-HEj6Oyp"}},{"cell_type":"code","source":["class Student:\n","    def __init__(self, name, age, roll_number):\n","        self.__name = name  # Private attribute for name\n","        self.__age = age  # Private attribute for age\n","        self.__roll_number = roll_number  # Private attribute for roll number\n","\n","    # Getter for name\n","    def get_name(self):\n","        return self.__name\n","\n","    # Setter for name\n","    def set_name(self, name):\n","        self.__name = name\n","\n","    # Getter for age\n","    def get_age(self):\n","        return self.__age\n","\n","    # Setter for age\n","    def set_age(self, age):\n","        if age > 0:  # Ensure age is positive\n","            self.__age = age\n","        else:\n","            print(\"Invalid age. Please enter a positive number.\")\n","\n","    # Getter for roll number\n","    def get_roll_number(self):\n","        return self.__roll_number\n","\n","    # Setter for roll number\n","    def set_roll_number(self, roll_number):\n","        self.__roll_number = roll_number\n","\n","    # Method to display student information\n","    def display_info(self):\n","        print(f\"Name: {self.__name}\")\n","        print(f\"Age: {self.__age}\")\n","        print(f\"Roll Number: {self.__roll_number}\")\n","\n","    # Method to update student details\n","    def update_details(self, name=None, age=None, roll_number=None):\n","        if name:\n","            self.set_name(name)\n","        if age:\n","            self.set_age(age)\n","        if roll_number:\n","            self.set_roll_number(roll_number)\n","\n","# Create instances of the Student class\n","student1 = Student(\"Alice\", 20, \"S1001\")\n","student2 = Student(\"Bob\", 22, \"S1002\")\n","\n","# Display initial student information\n","print(\"Initial Information:\")\n","student1.display_info()\n","print()  # New line for better readability\n","student2.display_info()\n","\n","# Update student details\n","student1.update_details(name=\"Alicia\", age=21)\n","student2.update_details(age=23, roll_number=\"S2002\")\n","\n","# Display updated student information\n","print(\"\\nUpdated Information:\")\n","student1.display_info()\n","print()  # New line for better readability\n","student2.display_info()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJViQYfC6t5A","executionInfo":{"status":"ok","timestamp":1720452099608,"user_tz":-330,"elapsed":497,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"a3f5bc9e-8b93-4950-bcc0-4d5787449cc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Information:\n","Name: Alice\n","Age: 20\n","Roll Number: S1001\n","\n","Name: Bob\n","Age: 22\n","Roll Number: S1002\n","\n","Updated Information:\n","Name: Alicia\n","Age: 21\n","Roll Number: S1001\n","\n","Name: Bob\n","Age: 23\n","Roll Number: S2002\n"]}]},{"cell_type":"markdown","source":["3.\n"],"metadata":{"id":"QNLC75jy8PuC"}},{"cell_type":"code","source":["class LibraryBook:\n","    def __init__(self, book_name, author, available=True):\n","        self.__book_name = book_name  # Private attribute for book name\n","        self.__author = author  # Private attribute for author\n","        self.__available = available  # Private attribute for availability status\n","\n","    # Method to borrow the book\n","    def borrow_book(self):\n","        if self.__available:\n","            self.__available = False\n","            print(f\"You have successfully borrowed '{self.__book_name}' by {self.__author}.\")\n","        else:\n","            print(f\"Sorry, '{self.__book_name}' by {self.__author} is currently unavailable.\")\n","\n","    # Method to return the book\n","    def return_book(self):\n","        if not self.__available:\n","            self.__available = True\n","            print(f\"Thank you for returning '{self.__book_name}'.\")\n","        else:\n","            print(f\"'{self.__book_name}' by {self.__author} was not borrowed.\")\n","\n","    # Getter for book name\n","    def get_book_name(self):\n","        return self.__book_name\n","\n","    # Getter for author\n","    def get_author(self):\n","        return self.__author\n","\n","    # Getter for availability status\n","    def is_available(self):\n","        return self.__available\n","\n","\n","# Create instances of the LibraryBook class\n","book1 = LibraryBook(\"1984\", \"George Orwell\")\n","book2 = LibraryBook(\"To Kill a Mockingbird\", \"Harper Lee\")\n","\n","# Display initial availability\n","print(f\"Is '{book1.get_book_name()}' available? {book1.is_available()}\")\n","print(f\"Is '{book2.get_book_name()}' available? {book2.is_available()}\")\n","\n","# Borrow books\n","book1.borrow_book()\n","book2.borrow_book()\n","\n","# Try to borrow again to check the availability status\n","book1.borrow_book()\n","\n","# Return books\n","book1.return_book()\n","book2.return_book()\n","\n","# Try to return again to check the status\n","book1.return_book()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6SXXGKu7F_R","executionInfo":{"status":"ok","timestamp":1720452194822,"user_tz":-330,"elapsed":745,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"123d9cff-3d59-42ef-be50-03988a72ad0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Is '1984' available? True\n","Is 'To Kill a Mockingbird' available? True\n","You have successfully borrowed '1984' by George Orwell.\n","You have successfully borrowed 'To Kill a Mockingbird' by Harper Lee.\n","Sorry, '1984' by George Orwell is currently unavailable.\n","Thank you for returning '1984'.\n","Thank you for returning 'To Kill a Mockingbird'.\n","'1984' by George Orwell was not borrowed.\n"]}]},{"cell_type":"markdown","source":["4.\n"],"metadata":{"id":"I01L0De88XbA"}},{"cell_type":"code","source":["class BankAccount:\n","    def __init__(self, account_holder, account_number, balance=0.0):\n","        self.account_holder = account_holder\n","        self.account_number = account_number\n","        self.balance = balance\n","\n","    # Method to deposit money\n","    def deposit(self, amount):\n","        if amount > 0:\n","            self.balance += amount\n","            print(f\"Deposited ${amount:.2f} to account {self.account_number}. New balance: ${self.balance:.2f}\")\n","        else:\n","            print(\"Deposit amount must be positive.\")\n","\n","    # Method to withdraw money\n","    def withdraw(self, amount):\n","        if amount > 0:\n","            if amount <= self.balance:\n","                self.balance -= amount\n","                print(f\"Withdrew ${amount:.2f} from account {self.account_number}. New balance: ${self.balance:.2f}\")\n","            else:\n","                print(\"Insufficient funds.\")\n","        else:\n","            print(\"Withdrawal amount must be positive.\")\n","\n","    # Method to check balance\n","    def check_balance(self):\n","        print(f\"Account {self.account_number} balance: ${self.balance:.2f}\")\n","\n","# SavingsAccount subclass\n","class SavingsAccount(BankAccount):\n","    def __init__(self, account_holder, account_number, balance=0.0, interest_rate=0.01):\n","        super().__init__(account_holder, account_number, balance)\n","        self.interest_rate = interest_rate\n","\n","    # Method to apply interest\n","    def apply_interest(self):\n","        interest = self.balance * self.interest_rate\n","        self.balance += interest\n","        print(f\"Interest of ${interest:.2f} applied. New balance: ${self.balance:.2f}\")\n","\n","# CheckingAccount subclass\n","class CheckingAccount(BankAccount):\n","    def __init__(self, account_holder, account_number, balance=0.0, overdraft_limit=100.0):\n","        super().__init__(account_holder, account_number, balance)\n","        self.overdraft_limit = overdraft_limit\n","\n","    # Override withdraw method to include overdraft limit\n","    def withdraw(self, amount):\n","        if amount > 0:\n","            if amount <= self.balance + self.overdraft_limit:\n","                self.balance -= amount\n","                print(f\"Withdrew ${amount:.2f} from account {self.account_number}. New balance: ${self.balance:.2f}\")\n","            else:\n","                print(f\"Exceeded overdraft limit of ${self.overdraft_limit:.2f}.\")\n","        else:\n","            print(\"Withdrawal amount must be positive.\")\n","\n","# Testing the banking system\n","\n","# Create a SavingsAccount\n","savings = SavingsAccount(\"Alice\", \"S1001\", 1000.0, interest_rate=0.05)\n","savings.check_balance()\n","savings.deposit(500)\n","savings.withdraw(200)\n","savings.apply_interest()\n","savings.check_balance()\n","\n","print()\n","\n","# Create a CheckingAccount\n","checking = CheckingAccount(\"Bob\", \"C2001\", 500.0, overdraft_limit=200.0)\n","checking.check_balance()\n","checking.deposit(300)\n","checking.withdraw(600)\n","checking.withdraw(300)  # This should exceed the overdraft limit\n","checking.check_balance()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhZVy7U58Rw_","executionInfo":{"status":"ok","timestamp":1720452274276,"user_tz":-330,"elapsed":505,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"de70ed2c-9586-42b3-dfdf-8093a7365565"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Account S1001 balance: $1000.00\n","Deposited $500.00 to account S1001. New balance: $1500.00\n","Withdrew $200.00 from account S1001. New balance: $1300.00\n","Interest of $65.00 applied. New balance: $1365.00\n","Account S1001 balance: $1365.00\n","\n","Account C2001 balance: $500.00\n","Deposited $300.00 to account C2001. New balance: $800.00\n","Withdrew $600.00 from account C2001. New balance: $200.00\n","Withdrew $300.00 from account C2001. New balance: $-100.00\n","Account C2001 balance: $-100.00\n"]}]},{"cell_type":"markdown","source":["5."],"metadata":{"id":"_XdQAv9i84vT"}},{"cell_type":"code","source":["# Define the base class Animal\n","class Animal:\n","    def make_sound(self):\n","        pass  # This will be overridden by subclasses\n","\n","# Define Dog subclass\n","class Dog(Animal):\n","    def make_sound(self):\n","        return \"Woof!\"\n","\n","# Define Cat subclass\n","class Cat(Animal):\n","    def make_sound(self):\n","        return \"Meow!\"\n","\n","# Test the program\n","if __name__ == \"__main__\":\n","    dog = Dog()\n","    cat = Cat()\n","\n","    print(\"Dog says:\", dog.make_sound())\n","    print(\"Cat says:\", cat.make_sound())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQLvBSxY8lCB","executionInfo":{"status":"ok","timestamp":1720452361799,"user_tz":-330,"elapsed":789,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"ee9dbc91-e06d-4899-c693-40e65bc6a1e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dog says: Woof!\n","Cat says: Meow!\n"]}]},{"cell_type":"markdown","source":["6."],"metadata":{"id":"emK31_BJ9H6v"}},{"cell_type":"code","source":["import uuid\n","\n","# Define MenuItem class\n","class MenuItem:\n","    def __init__(self, name, description, price, category):\n","        self.__menu_item_id = uuid.uuid4()  # Unique identifier using UUID\n","        self.name = name\n","        self.description = description\n","        self.price = price\n","        self.category = category\n","\n","    def get_menu_item_id(self):\n","        return str(self.__menu_item_id)  # Return the UUID as a string\n","\n","    def update(self, name=None, description=None, price=None, category=None):\n","        if name:\n","            self.name = name\n","        if description:\n","            self.description = description\n","        if price:\n","            self.price = price\n","        if category:\n","            self.category = category\n","\n","    def __str__(self):\n","        return f\"{self.name} - {self.description} | ${self.price} | Category: {self.category}\"\n","\n","# Define FoodItem subclass\n","class FoodItem(MenuItem):\n","    def __init__(self, name, description, price, category, cuisine):\n","        super().__init__(name, description, price, category)\n","        self.cuisine = cuisine\n","\n","    def update(self, name=None, description=None, price=None, category=None, cuisine=None):\n","        super().update(name, description, price, category)\n","        if cuisine:\n","            self.cuisine = cuisine\n","\n","    def __str__(self):\n","        return f\"{super().__str__()} | Cuisine: {self.cuisine}\"\n","\n","# Define BeverageItem subclass\n","class BeverageItem(MenuItem):\n","    def __init__(self, name, description, price, category, size):\n","        super().__init__(name, description, price, category)\n","        self.size = size\n","\n","    def update(self, name=None, description=None, price=None, category=None, size=None):\n","        super().update(name, description, price, category)\n","        if size:\n","            self.size = size\n","\n","    def __str__(self):\n","        return f\"{super().__str__()} | Size: {self.size}\"\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Create menu items\n","    food_item1 = FoodItem(\"Spaghetti Carbonara\", \"Creamy pasta with bacon\", 12.99, \"Main Course\", \"Italian\")\n","    beverage_item1 = BeverageItem(\"Coca-Cola\", \"Classic cola drink\", 2.49, \"Beverage\", \"Medium\")\n","\n","    # Print menu items\n","    print(\"Food Item:\", food_item1)\n","    print(\"Beverage Item:\", beverage_item1)\n","\n","    # Update menu item\n","    food_item1.update(price=14.99)  # Updating price\n","    print(\"\\nUpdated Food Item:\", food_item1)\n","\n","    # Remove menu item (not implemented in this example)\n","    # This would typically involve managing a list or dictionary of menu items and removing an item by its ID\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPwLMJFH86he","executionInfo":{"status":"ok","timestamp":1720452422387,"user_tz":-330,"elapsed":797,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"05f91e9c-d69b-4e5a-fe7d-2bb9a07c9835"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Food Item: Spaghetti Carbonara - Creamy pasta with bacon | $12.99 | Category: Main Course | Cuisine: Italian\n","Beverage Item: Coca-Cola - Classic cola drink | $2.49 | Category: Beverage | Size: Medium\n","\n","Updated Food Item: Spaghetti Carbonara - Creamy pasta with bacon | $14.99 | Category: Main Course | Cuisine: Italian\n"]}]},{"cell_type":"markdown","source":["7."],"metadata":{"id":"MhsvWAZY9Nkm"}},{"cell_type":"code","source":["import uuid\n","\n","# Define Room class\n","class Room:\n","    def __init__(self, room_number, room_type, rate):\n","        self.__room_id = uuid.uuid4()  # Unique identifier using UUID\n","        self.room_number = room_number\n","        self.room_type = room_type\n","        self.rate = rate\n","        self.__availability = True  # Private attribute for availability\n","\n","    def get_room_id(self):\n","        return str(self.__room_id)  # Return the UUID as a string\n","\n","    def is_available(self):\n","        return self.__availability\n","\n","    def book_room(self):\n","        if self.__availability:\n","            self.__availability = False\n","            return True\n","        else:\n","            return False\n","\n","    def check_in_guest(self):\n","        if not self.__availability:\n","            # Additional logic for checking in a guest\n","            print(f\"Guest checked into Room {self.room_number}.\")\n","        else:\n","            print(\"Room is not booked yet.\")\n","\n","    def check_out_guest(self):\n","        if not self.__availability:\n","            # Additional logic for checking out a guest\n","            print(f\"Guest checked out of Room {self.room_number}.\")\n","            self.__availability = True\n","        else:\n","            print(\"No guest checked in.\")\n","\n","# Define SuiteRoom subclass\n","class SuiteRoom(Room):\n","    def __init__(self, room_number, rate, jacuzzi):\n","        super().__init__(room_number, \"Suite\", rate)\n","        self.jacuzzi = jacuzzi\n","\n","    def upgrade_suite(self, jacuzzi):\n","        self.jacuzzi = jacuzzi\n","\n","    def __str__(self):\n","        return f\"Suite Room {self.room_number} | Rate: ${self.rate} per night | Jacuzzi: {'Yes' if self.jacuzzi else 'No'}\"\n","\n","# Define StandardRoom subclass\n","class StandardRoom(Room):\n","    def __init__(self, room_number, rate, balcony):\n","        super().__init__(room_number, \"Standard\", rate)\n","        self.balcony = balcony\n","\n","    def upgrade_room(self, balcony):\n","        self.balcony = balcony\n","\n","    def __str__(self):\n","        return f\"Standard Room {self.room_number} | Rate: ${self.rate} per night | Balcony: {'Yes' if self.balcony else 'No'}\"\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Create room instances\n","    suite_room1 = SuiteRoom(\"101\", 250, True)\n","    standard_room1 = StandardRoom(\"201\", 150, False)\n","\n","    # Print room details\n","    print(suite_room1)\n","    print(standard_room1)\n","\n","    # Book a room\n","    if suite_room1.book_room():\n","        print(f\"Room {suite_room1.room_number} booked successfully.\")\n","    else:\n","        print(f\"Room {suite_room1.room_number} is already booked.\")\n","\n","    # Check-in a guest\n","    suite_room1.check_in_guest()\n","\n","    # Check-out a guest\n","    suite_room1.check_out_guest()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OBD5hLPV9JTQ","executionInfo":{"status":"ok","timestamp":1720452508406,"user_tz":-330,"elapsed":707,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"3e4e858d-e5a9-4e57-de75-455b5f7b5185"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Suite Room 101 | Rate: $250 per night | Jacuzzi: Yes\n","Standard Room 201 | Rate: $150 per night | Balcony: No\n","Room 101 booked successfully.\n","Guest checked into Room 101.\n","Guest checked out of Room 101.\n"]}]},{"cell_type":"markdown","source":["8."],"metadata":{"id":"h6rxoPUS9r_W"}},{"cell_type":"code","source":["import uuid\n","\n","# Define Member class\n","class Member:\n","    def __init__(self, name, age, membership_type):\n","        self.__member_id = uuid.uuid4()  # Unique identifier using UUID\n","        self.name = name\n","        self.age = age\n","        self.membership_type = membership_type\n","        self.__membership_status = True  # Private attribute for membership status\n","\n","    def get_member_id(self):\n","        return str(self.__member_id)  # Return the UUID as a string\n","\n","    def is_active_member(self):\n","        return self.__membership_status\n","\n","    def register_member(self):\n","        if not self.__membership_status:\n","            self.__membership_status = True\n","            print(f\"Member {self.name} registered successfully.\")\n","        else:\n","            print(f\"Member {self.name} is already registered.\")\n","\n","    def renew_membership(self):\n","        if self.__membership_status:\n","            print(f\"Membership for {self.name} renewed successfully.\")\n","        else:\n","            print(f\"Cannot renew membership for {self.name} as membership is canceled.\")\n","\n","    def cancel_membership(self):\n","        if self.__membership_status:\n","            self.__membership_status = False\n","            print(f\"Membership for {self.name} canceled successfully.\")\n","        else:\n","            print(f\"Member {self.name} does not have an active membership.\")\n","\n","# Define FamilyMember subclass\n","class FamilyMember(Member):\n","    def __init__(self, name, age, membership_type, family_discount):\n","        super().__init__(name, age, membership_type)\n","        self.family_discount = family_discount\n","\n","    def apply_family_discount(self):\n","        # Example method for applying family discount\n","        print(f\"Applied {self.family_discount}% family discount for {self.name}.\")\n","\n","# Define IndividualMember subclass\n","class IndividualMember(Member):\n","    def __init__(self, name, age, membership_type, subscription_type):\n","        super().__init__(name, age, membership_type)\n","        self.subscription_type = subscription_type\n","\n","    def change_subscription(self, new_subscription):\n","        self.subscription_type = new_subscription\n","        print(f\"Changed subscription type for {self.name} to {self.subscription_type}.\")\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Create member instances\n","    family_member1 = FamilyMember(\"John Doe\", 35, \"Family\", 10)\n","    individual_member1 = IndividualMember(\"Jane Smith\", 28, \"Individual\", \"Premium\")\n","\n","    # Print member details\n","    print(family_member1.name, family_member1.age, family_member1.membership_type, family_member1.family_discount)\n","    print(individual_member1.name, individual_member1.age, individual_member1.membership_type, individual_member1.subscription_type)\n","\n","    # Register a member\n","    family_member1.register_member()\n","\n","    # Renew membership\n","    individual_member1.renew_membership()\n","\n","    # Cancel membership\n","    family_member1.cancel_membership()\n","\n","    # Apply family discount\n","    family_member1.apply_family_discount()\n","\n","    # Change subscription type\n","    individual_member1.change_subscription(\"Basic\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUoGk1Ac9eTI","executionInfo":{"status":"ok","timestamp":1720452575266,"user_tz":-330,"elapsed":6,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"ef6478f3-b55a-4adb-ff07-270094fe5236"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["John Doe 35 Family 10\n","Jane Smith 28 Individual Premium\n","Member John Doe is already registered.\n","Membership for Jane Smith renewed successfully.\n","Membership for John Doe canceled successfully.\n","Applied 10% family discount for John Doe.\n","Changed subscription type for Jane Smith to Basic.\n"]}]},{"cell_type":"markdown","source":["9."],"metadata":{"id":"vszUszl49xsp"}},{"cell_type":"code","source":["import uuid\n","\n","# Define Event class\n","class Event:\n","    def __init__(self, name, date, time, location):\n","        self.__event_id = uuid.uuid4()  # Unique identifier using UUID\n","        self.name = name\n","        self.date = date\n","        self.time = time\n","        self.location = location\n","        self.__attendees = []  # Private attribute for attendees\n","\n","    def get_event_id(self):\n","        return str(self.__event_id)  # Return the UUID as a string\n","\n","    def add_attendee(self, attendee):\n","        self.__attendees.append(attendee)\n","\n","    def remove_attendee(self, attendee):\n","        if attendee in self.__attendees:\n","            self.__attendees.remove(attendee)\n","\n","    def get_attendee_count(self):\n","        return len(self.__attendees)\n","\n","    def __str__(self):\n","        return f\"{self.name} | Date: {self.date} | Time: {self.time} | Location: {self.location} | Attendees: {self.get_attendee_count()}\"\n","\n","# Define PrivateEvent subclass\n","class PrivateEvent(Event):\n","    def __init__(self, name, date, time, location, invited_guests):\n","        super().__init__(name, date, time, location)\n","        self.invited_guests = invited_guests\n","\n","    def invite_guest(self, guest):\n","        self.invited_guests.append(guest)\n","\n","    def __str__(self):\n","        return f\"Private Event: {super().__str__()} | Invited Guests: {', '.join(self.invited_guests)}\"\n","\n","# Define PublicEvent subclass\n","class PublicEvent(Event):\n","    def __init__(self, name, date, time, location, max_capacity):\n","        super().__init__(name, date, time, location)\n","        self.max_capacity = max_capacity\n","\n","    def is_full(self):\n","        return self.get_attendee_count() >= self.max_capacity\n","\n","    def __str__(self):\n","        return f\"Public Event: {super().__str__()} | Max Capacity: {self.max_capacity}\"\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Create event instances\n","    private_event1 = PrivateEvent(\"Birthday Party\", \"2024-07-15\", \"18:00\", \"123 Main St\", [\"John\", \"Jane\", \"Alice\"])\n","    public_event1 = PublicEvent(\"Community Fair\", \"2024-08-10\", \"10:00\", \"City Park\", 100)\n","\n","    # Print event details\n","    print(private_event1)\n","    print(public_event1)\n","\n","    # Add attendees\n","    private_event1.add_attendee(\"Mark\")\n","    private_event1.add_attendee(\"Sarah\")\n","    public_event1.add_attendee(\"David\")\n","\n","    # Remove attendee\n","    private_event1.remove_attendee(\"Alice\")\n","\n","    # Print updated event details\n","    print(private_event1)\n","    print(public_event1)\n","\n","    # Check if public event is full\n","    if public_event1.is_full():\n","        print(\"Public event is full.\")\n","    else:\n","        print(\"There is still space available.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoIrOWtP9ulx","executionInfo":{"status":"ok","timestamp":1720452712822,"user_tz":-330,"elapsed":578,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"880f6fa1-0450-4369-dcfb-1cf0f7eb847c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Private Event: Birthday Party | Date: 2024-07-15 | Time: 18:00 | Location: 123 Main St | Attendees: 0 | Invited Guests: John, Jane, Alice\n","Public Event: Community Fair | Date: 2024-08-10 | Time: 10:00 | Location: City Park | Attendees: 0 | Max Capacity: 100\n","Private Event: Birthday Party | Date: 2024-07-15 | Time: 18:00 | Location: 123 Main St | Attendees: 2 | Invited Guests: John, Jane, Alice\n","Public Event: Community Fair | Date: 2024-08-10 | Time: 10:00 | Location: City Park | Attendees: 1 | Max Capacity: 100\n","There is still space available.\n"]}]},{"cell_type":"markdown","source":["10."],"metadata":{"id":"azbLnENk-Sli"}},{"cell_type":"code","source":["import uuid\n","\n","# Define Flight class\n","class Flight:\n","    def __init__(self, flight_number, departure_airport, arrival_airport, departure_time, arrival_time, total_seats):\n","        self.__flight_id = uuid.uuid4()  # Unique identifier using UUID\n","        self.flight_number = flight_number\n","        self.departure_airport = departure_airport\n","        self.arrival_airport = arrival_airport\n","        self.departure_time = departure_time\n","        self.arrival_time = arrival_time\n","        self.__total_seats = total_seats\n","        self.__available_seats = total_seats  # Private attribute for available seats\n","\n","    def get_flight_id(self):\n","        return str(self.__flight_id)  # Return the UUID as a string\n","\n","    def get_available_seats(self):\n","        return self.__available_seats\n","\n","    def book_seat(self):\n","        if self.__available_seats > 0:\n","            self.__available_seats -= 1\n","            return True\n","        else:\n","            return False\n","\n","    def cancel_reservation(self):\n","        if self.__available_seats < self.__total_seats:\n","            self.__available_seats += 1\n","\n","    def __str__(self):\n","        return f\"Flight {self.flight_number} | Departure: {self.departure_airport} ({self.departure_time}) | Arrival: {self.arrival_airport} ({self.arrival_time}) | Available Seats: {self.__available_seats}/{self.__total_seats}\"\n","\n","# Define DomesticFlight subclass\n","class DomesticFlight(Flight):\n","    def __init__(self, flight_number, departure_airport, arrival_airport, departure_time, arrival_time, total_seats, baggage_allowance):\n","        super().__init__(flight_number, departure_airport, arrival_airport, departure_time, arrival_time, total_seats)\n","        self.baggage_allowance = baggage_allowance\n","\n","    def __str__(self):\n","        return f\"Domestic {super().__str__()} | Baggage Allowance: {self.baggage_allowance} kg\"\n","\n","# Define InternationalFlight subclass\n","class InternationalFlight(Flight):\n","    def __init__(self, flight_number, departure_airport, arrival_airport, departure_time, arrival_time, total_seats, meal_service):\n","        super().__init__(flight_number, departure_airport, arrival_airport, departure_time, arrival_time, total_seats)\n","        self.meal_service = meal_service\n","\n","    def __str__(self):\n","        return f\"International {super().__str__()} | Meal Service: {'Yes' if self.meal_service else 'No'}\"\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Create flight instances\n","    domestic_flight1 = DomesticFlight(\"DL123\", \"JFK\", \"LAX\", \"08:00\", \"11:00\", 150, 25)\n","    international_flight1 = InternationalFlight(\"AI456\", \"JFK\", \"LHR\", \"14:00\", \"18:00\", 200, True)\n","\n","    # Print flight details\n","    print(domestic_flight1)\n","    print(international_flight1)\n","\n","    # Book seats\n","    if domestic_flight1.book_seat():\n","        print(\"Seat booked on Domestic Flight.\")\n","    else:\n","        print(\"No seats available on Domestic Flight.\")\n","\n","    if international_flight1.book_seat():\n","        print(\"Seat booked on International Flight.\")\n","    else:\n","        print(\"No seats available on International Flight.\")\n","\n","    # Cancel reservation\n","    domestic_flight1.cancel_reservation()\n","    international_flight1.cancel_reservation()\n","\n","    # Print updated flight details\n","    print(domestic_flight1)\n","    print(international_flight1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBbg37CM-QQp","executionInfo":{"status":"ok","timestamp":1720452787058,"user_tz":-330,"elapsed":779,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"689ba7ae-3612-4741-c6b9-75b20709a884"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Domestic Flight DL123 | Departure: JFK (08:00) | Arrival: LAX (11:00) | Available Seats: 150/150 | Baggage Allowance: 25 kg\n","International Flight AI456 | Departure: JFK (14:00) | Arrival: LHR (18:00) | Available Seats: 200/200 | Meal Service: Yes\n","Seat booked on Domestic Flight.\n","Seat booked on International Flight.\n","Domestic Flight DL123 | Departure: JFK (08:00) | Arrival: LAX (11:00) | Available Seats: 150/150 | Baggage Allowance: 25 kg\n","International Flight AI456 | Departure: JFK (14:00) | Arrival: LHR (18:00) | Available Seats: 200/200 | Meal Service: Yes\n"]}]},{"cell_type":"markdown","source":["11."],"metadata":{"id":"9kNZ-6Wq-xV3"}},{"cell_type":"code","source":["# Example usage in another Python script\n","PI = 3.14159\n","SPEED_OF_LIGHT = 299792458  # meters per second\n","\n","print(\"The value of pi is:\", PI)\n","print(\"The speed of light is:\", SPEED_OF_LIGHT, \"m/s\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7jfxnc4-iU4","executionInfo":{"status":"ok","timestamp":1720452976950,"user_tz":-330,"elapsed":507,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"d13641cb-29ae-4567-9388-c17492d4c0ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The value of pi is: 3.14159\n","The speed of light is: 299792458 m/s\n"]}]},{"cell_type":"markdown","source":["12."],"metadata":{"id":"Hat3gqIR_hbj"}},{"cell_type":"code","source":["# calculator.py\n","\n","def add(a, b):\n","    \"\"\"Addition function.\"\"\"\n","    return a + b\n","\n","def subtract(a, b):\n","    \"\"\"Subtraction function.\"\"\"\n","    return a - b\n","\n","def multiply(a, b):\n","    \"\"\"Multiplication function.\"\"\"\n","    return a * b\n","\n","def divide(a, b):\n","    \"\"\"Division function.\"\"\"\n","    if b != 0:\n","        return a / b\n","    else:\n","        raise ValueError(\"Cannot divide by zero.\")\n","\n","# Example usage:\n","if __name__ == \"__main__\":\n","    x, y = 10, 5\n","    print(f\"Addition: {add(x, y)}\")\n","    print(f\"Subtraction: {subtract(x, y)}\")\n","    print(f\"Multiplication: {multiply(x, y)}\")\n","    print(f\"Division: {divide(x, y)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBgpKqIW_mQH","executionInfo":{"status":"ok","timestamp":1720454284790,"user_tz":-330,"elapsed":638,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"93ec9dd1-90d0-4304-d6ac-44dafe2c2d80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Addition: 15\n","Subtraction: 5\n","Multiplication: 50\n","Division: 2.0\n"]}]},{"cell_type":"markdown","source":["20.\n","\n","Mean (Average):\n","\n","The mean represents the typical value in a dataset.\n","To calculate the mean in Python, you can use the mean() function from the statistics module or the numpy library:"],"metadata":{"id":"vuanhbiUEhHN"}},{"cell_type":"code","source":["import statistics\n","import numpy as np\n","\n","data = [10, 20, 30, 40]\n","mean_value = statistics.mean(data)  # Using the statistics module\n","mean_np = np.mean(data)  # Using numpy\n","\n","print(f\"Mean (statistics): {mean_value:.2f}\")\n","print(f\"Mean (numpy): {mean_np:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUErSPpOYC2Q","executionInfo":{"status":"ok","timestamp":1720795029128,"user_tz":-330,"elapsed":12,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"a5451f02-28cc-469b-a596-31cc82f0d043"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean (statistics): 25.00\n","Mean (numpy): 25.00\n"]}]},{"cell_type":"markdown","source":["Median:\n","\n","The median is the middle value in a sorted dataset.\n","To calculate the median"],"metadata":{"id":"g4detfy8YLwm"}},{"cell_type":"code","source":["median_value = statistics.median(data)\n","print(f\"Median: {median_value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ya9RaevvYDlk","executionInfo":{"status":"ok","timestamp":1720795064538,"user_tz":-330,"elapsed":508,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"dfa94bfc-b3b1-4820-ce7b-ec68516be9e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Median: 25.0\n"]}]},{"cell_type":"markdown","source":["Mode:\n","\n","The mode is the most frequent value in the dataset.\n","To find the mode"],"metadata":{"id":"A-p-_GFwYUn1"}},{"cell_type":"code","source":["mode_value = statistics.mode(data)\n","print(f\"Mode: {mode_value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oznoj3sPYUJI","executionInfo":{"status":"ok","timestamp":1720795103148,"user_tz":-330,"elapsed":514,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"f9ecb754-d222-4f7e-af14-5dba1ff7ff08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode: 10\n"]}]},{"cell_type":"markdown","source":["Measures of Dispersion:\n","\n","Range:\n","\n","The range is the difference between the maximum and minimum values.\n","\n","Calculate it directly:"],"metadata":{"id":"YF0FvAq6Yau9"}},{"cell_type":"code","source":["data_range = max(data) - min(data)\n","print(f\"Range: {data_range}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-GuHlzlIYOQT","executionInfo":{"status":"ok","timestamp":1720795137134,"user_tz":-330,"elapsed":522,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"72b98ca0-ed76-4cb8-e944-e757bf603dc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Range: 30\n"]}]},{"cell_type":"markdown","source":["Variance and Standard Deviation:\n","\n","Variance measures the spread of data points around the mean.\n","\n","Standard deviation is the square root of the variance.\n","\n","Use variance() and stdev() from the statistics module:"],"metadata":{"id":"DG4NR1ZfYjkG"}},{"cell_type":"code","source":["variance_value = statistics.variance(data)\n","stdev_value = statistics.stdev(data)\n","\n","print(f\"Variance: {variance_value:.2f}\")\n","print(f\"Standard Deviation: {stdev_value:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dabj7y-FYf_I","executionInfo":{"status":"ok","timestamp":1720795166095,"user_tz":-330,"elapsed":523,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"c9bc7208-bbf0-4b4c-8de6-9a18a75d2349"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Variance: 166.67\n","Standard Deviation: 12.91\n"]}]},{"cell_type":"markdown","source":["Interquartile Range (IQR):\n","\n","IQR is the range of the middle 50% of data.\n","\n","Calculate it using numpy"],"metadata":{"id":"g4jn72QxYuRA"}},{"cell_type":"code","source":["q1 = np.percentile(data, 25)\n","q3 = np.percentile(data, 75)\n","iqr = q3 - q1\n","\n","print(f\"IQR: {iqr:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eb_oI7flYnDJ","executionInfo":{"status":"ok","timestamp":1720795206343,"user_tz":-330,"elapsed":468,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"b61111d9-f028-42ad-e6db-90a5841a91ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IQR: 15.00\n"]}]},{"cell_type":"markdown","source":["21.\n","\n","Skewness:\n","\n","Skewness quantifies the departure from symmetry in a dataset.\n","\n","It is calculated based on the third standardized moment.\n","\n","A perfectly symmetric distribution has a skewness of 0.\n","\n","Positive skewness (right-skewed) indicates a longer tail on the right side.\n","\n","Negative skewness (left-skewed) indicates a longer tail on the left side.\n","\n","Types of Skewness:\n","\n","Positive Skewness (Right-Skewed):\n","\n","The tail extends more to the right (higher values).\n","\n","The mean is greater than the median.\n","\n","Common in financial data (e.g., stock returns).\n","\n","Example: Income distribution (few high earners).\n","\n","Graphically: !Right-Skewed Distribution\n","\n","Negative Skewness (Left-Skewed):\n","\n","The tail extends more to the left (lower values).\n","\n","The median is greater than the mean.\n","\n","Common in data related to response times (e.g., website loading times).\n","\n","Example: Test scores (few low scores).\n","\n","Graphically: !Left-Skewed Distribution\n","\n","Calculating Skewness in Python:\n","\n","You can use the scipy.\n","\n","stats library to compute skewness.\n"],"metadata":{"id":"fYn42HYFZHEB"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import skew\n","\n","# Generate example data (right-skewed)\n","data_right_skewed = np.random.exponential(scale=2, size=1000)\n","\n","# Calculate skewness\n","skewness_right = skew(data_right_skewed)\n","\n","# Plot the distribution\n","plt.hist(data_right_skewed, bins=30, alpha=0.7, color='b')\n","plt.title(f\"Right-Skewed (Skewness = {skewness_right:.2f})\")\n","plt.xlabel(\"Values\")\n","plt.ylabel(\"Frequency\")\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"IJGRHQ9AYw49","executionInfo":{"status":"ok","timestamp":1720795364975,"user_tz":-330,"elapsed":1909,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"bd7f6d5b-41be-4b4c-8772-7f8ea9cf4001"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["22.\n","\n","Probability Mass Function (PMF):\n","\n","The PMF is associated with discrete random variables. It describes the probability distribution of such variables.\n","\n","Discrete random variables take on specific, distinct values (usually integers) within a given range.\n","\n","The PMF assigns a probability to each possible value in the sample space.\n","\n","In other words, it tells us how likely it is for the random variable to take a particular value.\n","\n","Example: Rolling a fair six-sided die—each face has a 1/6 chance.\n","\n","Probability Density Function (PDF):\n","\n","The PDF is associated with continuous random variables. It describes the probability distribution of continuous variables.\n","\n","Continuous random variables can take any value within a range (e.g., real numbers).\n","\n","The PDF provides information about the likelihood of the random variable falling within a specific interval.\n","\n","Unlike the PMF, which assigns probabilities to individual points, the PDF gives the probability density per unit length.\n","\n","The integral of the PDF over an interval yields the probability of the random variable falling within that interval.\n","\n","Example: The height of a person—height can be any real number within a certain range.\n","\n","Difference Between PMF and PDF:\n","\n","Nature of Random Variable:\n","\n","PMF: Associated with discrete random variables.\n","\n","PDF: Associated with continuous random variables.\n","\n","Values and Intervals:\n","\n","PMF: Assigns probabilities to specific values in the sample space.\n","\n","PDF: Provides probabilities for intervals (ranges) of values.\n","\n","Units:\n","\n","PMF: Measures probability as a mass (hence the term “probability mass function”).\n","\n","PDF: Measures probability density (per unit length).\n","\n","Interpretation:\n","\n","PMF: Describes the likelihood of observing a particular outcome.\n","\n","PDF: Describes how dense the probability is near a specific value.\n","\n","In Python, you can calculate and visualize both PMF and PDF using various libraries.\n"," For example:\n","\n","\n","To compute PMF for a discrete distribution (e.g., binomial), use functions like binom.pmf() from scipy.stats.\n","\n","To compute PDF for a continuous distribution (e.g., normal), use functions like norm.pdf() from the same library."],"metadata":{"id":"yWt3_g0BZaEW"}},{"cell_type":"markdown","source":["23.\n","\n","Correlation:\n","\n","Definition: Correlation measures the strength and direction of the linear relationship between two variables.\n","\n","Range: The correlation coefficient ranges from -1 to 1.\n","\n","A correlation coefficient of 1 indicates a perfect positive correlation: As one variable increases, the other also increases.\n","\n","A correlation coefficient of -1 indicates a perfect negative correlation: As one variable increases, the other decreases.\n","\n","A correlation coefficient of 0 means there’s no linear relationship between the two variables.\n","\n","Types of Correlation:\n","\n","Pearson Correlation (Parametric):\n","\n","Most common type of correlation analysis.\n","\n","Assumes that both variables are normally distributed and have equal variances.\n","\n","Measures the linear relationship between two continuous variables.\n","\n","Example: Studying the relationship between height and weight.\n","\n","Spearman Rank Correlation (Non-parametric):\n","\n","Also known as Spearman’s rho.\n","\n","Appropriate for ordinal or non-normally distributed data.\n","\n","Measures the strength of the monotonic relationship (whether one variable tends to increase as the other does).\n","\n","Example: Ranking students based on their test scores.\n","\n","Kendall’s Tau (Non-parametric):\n","\n","Another non-parametric measure.\n","\n","Similar to Spearman’s rank correlation but focuses on concordant and discordant pairs.\n","\n","Robust against outliers.\n","\n","Example: Analyzing the agreement between two judges’ ratings.\n","\n","Point-Biserial Correlation:\n","\n","Specifically used when one variable is dichotomous (binary) and the other is continuous.\n","\n","Example: Correlation between gender (male/female) and exam scores.\n","\n","Phi Coefficient:\n","\n","Used for categorical data arranged in a 2x2 contingency table.\n","\n","Measures association between two dichotomous variables.\n","\n","Example: Relationship between smoking status (yes/no) and lung cancer.\n","\n","Canonical Correlation:\n","\n","Examines the relationship between sets of variables (multivariate)\n",".\n","Finds linear combinations of variables that maximize correlation.\n","\n","Example: Studying the relationship between health behaviors and health outcomes.\n","Partial and Semi-Partial (Part) Correlations:\n","\n","Control for the influence of other variables.\n","\n","Partial correlation measures the relationship between two variables while holding constant the effect of a third variable.\n","\n","Semi-partial correlation considers only one variable’s unique contribution.\n","Example: Investigating the relationship between income and happiness, controlling for education level.\n","\n","Cross-Correlation:\n","\n","Used in time series analysis.\n","\n","Measures the similarity between two signals at different time lags.\n","\n","Example: Analyzing the relationship between stock prices of two companies over time.\n","\n","Methods of Determining Correlation:\n","\n","Scatter Diagram:\n","\n","Visual representation of the relationship between two variables.\n","\n","Scatter plots help identify patterns (linear, quadratic, etc.).\n","\n","Computing the Correlation Coefficient:\n","\n","Calculate the correlation coefficient using appropriate methods (Pearson, Spearman, etc.).\n","\n","Use statistical software (Python, R, etc.) or manual calculations.\n","\n","Testing Significance:\n","\n","Perform hypothesis tests (usually t-tests) to determine if the observed correlation is statistically significant.\n","\n","Small p-values (typically < 0.05) indicate significant correlation."],"metadata":{"id":"fs_AR4EYaCs-"}},{"cell_type":"markdown","source":["24..\n","\n","Accountancy: 45, 70, 65, 30, 90, 40, 50, 75, 65, 60\n","\n","Statistics: 35, 90, 70, 40, 95, 60, 80, 80, 50, 50"],"metadata":{"id":"RXFsDBHtaot1"}},{"cell_type":"markdown","source":["Calculate the necessary sums:"],"metadata":{"id":"oDF-mJNWbfKU"}},{"cell_type":"code","source":["accountancy_marks = [45, 70, 65, 30, 90, 40, 50, 75, 65, 60]\n","statistics_marks = [35, 90, 70, 40, 95, 60, 80, 80, 50, 50]\n","\n","sum_xy = sum(x * y for x, y in zip(accountancy_marks, statistics_marks))\n","sum_x = sum(accountancy_marks)\n","sum_y = sum(statistics_marks)\n","sum_x_squared = sum(x ** 2 for x in accountancy_marks)\n","sum_y_squared = sum(y ** 2 for y in statistics_marks)\n"],"metadata":{"id":"Hvt5pjiybemH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculate the correlation coefficient (Pearson’s ( r ))\n"],"metadata":{"id":"CP2C_aAWblzr"}},{"cell_type":"code","source":["n = len(accountancy_marks)\n","numerator = n * sum_xy - sum_x * sum_y\n","denominator = ((n * sum_x_squared - sum_x ** 2) * (n * sum_y_squared - sum_y ** 2)) ** 0.5\n","\n","correlation_coefficient = numerator / denominator\n"],"metadata":{"id":"fGQP5udrZXRA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Print the result:"],"metadata":{"id":"GWsWPAbrbsR9"}},{"cell_type":"code","source":["print(f\"Pearson's correlation coefficient (r): {correlation_coefficient:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvV_fiAnboYH","executionInfo":{"status":"ok","timestamp":1720795984161,"user_tz":-330,"elapsed":733,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"99a2e11d-a9c6-4744-f619-c1c7fa8a81fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pearson's correlation coefficient (r): 0.73\n"]}]},{"cell_type":"markdown","source":["25\n","\n","\n","Nature:\n","\n","Correlation:\n","\n","Measures the linear association between two variables (e.g., how they move together).\n","\n","Answers questions like whether two variables increase or decrease together.\n","Provides information about the strength and direction of the relationship.\n","\n","Regression:\n","\n","Predicts the value of one variable based on the value of another variable.\n","Focuses on understanding how changes in the predictor variable affect the response variable.\n","\n","Reveals cause-and-effect relationships.\n","\n","Purpose:\n","\n","\n","\n","Correlation:\n","\n","Descriptive: Helps summarize the relationship.\n","\n","No prediction or causality.\n","\n","Regression:\n","\n","Predictive: Used for making predictions.\n","\n","Establishes a functional relationship.\n","\n","Equation:\n","\n","Correlation:\n","\n","No equation; only a correlation coefficient (e.g., Pearson’s ( r )).\n","\n","Regression:\n","\n","Uses an equation (e.g., linear regression equation) to predict the response variable based on the predictor variable.\n","\n","Direction of Relationship:\n","\n","Correlation:\n","\n","Only indicates the direction (positive or negative) and strength.\n","\n","Regression:\n","\n","Quantifies the relationship (slope) and predicts the magnitude of change."],"metadata":{"id":"G-46g0_bcON-"}},{"cell_type":"markdown","source":["26..\n","\n","Let's assume the price in Delhi (Y) and the price in Agra (X) have a linear relationship:\n","𝑌\n","=\n","𝑎\n","+\n","𝑏\n","𝑋\n","Y=a+bX\n","\n","Where:\n","\n","𝑌\n","Y is the price in Delhi.\n","𝑋\n","X is the price in Agra.\n","𝑎\n","a is the intercept.\n","𝑏\n","b is the slope.\n","The slope\n","𝑏\n","b can be found using the formula:\n","𝑏\n","=\n","𝑟\n","⋅\n","𝜎\n","𝑌\n","𝜎\n","𝑋\n","b=r⋅\n","σ\n","X\n","​\n","\n","σ\n","Y\n","​\n","\n","​\n","\n","\n","To find the most likely price at Delhi, we need to know the means (\n","𝜇\n","𝑋\n","μ\n","X\n","​\n"," ,\n","𝜇\n","𝑌\n","μ\n","Y\n","​\n"," ) and standard deviations (\n","𝜎\n","𝑋\n","σ\n","X\n","​\n"," ,\n","𝜎\n","𝑌\n","σ\n","Y\n","​\n"," ) of the prices at Agra and Delhi.\n","\n","Given data:\n","\n","𝑟\n","=\n","0.8\n","r=0.8\n","𝑋\n","=\n","70\n","X=70 (price in Agra)\n","Without specific means and standard deviations, we can assume that the prices are linearly related. The regression equation for estimating the price in Delhi can be simplified to:\n","\n","𝑌\n","=\n","𝜇\n","𝑌\n","+\n","𝑟\n","⋅\n","𝜎\n","𝑌\n","𝜎\n","𝑋\n","⋅\n","(\n","𝑋\n","−\n","𝜇\n","𝑋\n",")\n","Y=μ\n","Y\n","​\n"," +r⋅\n","σ\n","X\n","​\n","\n","σ\n","Y\n","​\n","\n","​\n"," ⋅(X−μ\n","X\n","​\n"," )"],"metadata":{"id":"Oqf4vLlucbgr"}},{"cell_type":"markdown","source":["27..\n","\n","Mean Values of x and y:\n","\n","We have two regression equations:\n","\n","Equation (i): (8x - 10y = -66)\n","\n","Equation (ii): (40x - 18y = 214)\n","\n","Solving this system of equations, we find:\n","\n","Mean of x ((\\bar{x})) = 5\n","\n","Mean of y ((\\bar{y})) = 3\n","\n","Coefficient of Correlation ®:\n","\n","We can use the regression equations to find the coefficient of correlation ®.\n","The coefficient of correlation is the ratio of the common coefficient in both equations.\n","\n","From Equation (i), the common coefficient is 8.\n","\n","Therefore, (r = \\frac{8}{3 \\cdot \\sigma_y})\n","\n","Standard Deviation of y ((\\sigma_y)):\n","\n","We can use Equation (ii) to find (\\sigma_y):\n","(40x - 18y = 214)\n","\n","Solving for y, we get (y = 3)\n","\n","Therefore, (\\sigma_y = 3)"],"metadata":{"id":"sTlYXfNNdUod"}},{"cell_type":"code","source":["# Given data\n","variance_x = 9\n","common_coefficient = 8\n","mean_x = 5\n","mean_y = 3\n","\n","# Calculate the coefficient of correlation (r)\n","sigma_y = 3\n","correlation_coefficient = common_coefficient / (3 * sigma_y)\n","\n","print(f\"Mean of x: {mean_x}\")\n","print(f\"Mean of y: {mean_y}\")\n","print(f\"Coefficient of correlation (r): {correlation_coefficient:.2f}\")\n","print(f\"Standard deviation of y (σ_y): {sigma_y}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhSBfHbxbut8","executionInfo":{"status":"ok","timestamp":1720796563021,"user_tz":-330,"elapsed":1396,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"266952e9-fa32-4eff-ff9b-28bb6c254fad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean of x: 5\n","Mean of y: 3\n","Coefficient of correlation (r): 0.89\n","Standard deviation of y (σ_y): 3\n"]}]},{"cell_type":"markdown","source":["28..\n","\n","Normal Distribution (Gaussian Distribution or Bell Curve):\n","\n","Definition:\n","\n","The normal distribution is a continuous probability distribution that is symmetrical around its mean (average).\n","\n","It is characterized by its bell-shaped curve.\n","\n","In a normal distribution:\n","\n","Most observations cluster around the central peak (mean).\n","\n","Probabilities for values further away from the mean taper off equally in both directions.\n","\n","Extreme values in both tails of the distribution are similarly unlikely.\n","\n","Properties of Normal Distributions:\n","\n","Symmetry:\n","\n","The normal distribution is symmetric about its mean.\n","\n","Half the values fall below the mean, and half above the mean.\n","\n","Mean, Median, and Mode:\n","\n","In a normal distribution, the mean, median, and mode are exactly the same.\n","Standard Deviation:\n","\n","The standard deviation determines the spread (width) of the curve.\n","\n","A small standard deviation results in a narrow curve, while a large standard deviation leads to a wide curve.\n","\n","\n","Empirical Rule (68-95-99.7 Rule):\n","\n","The empirical rule tells you where most of your values lie in a normal\n","distribution:\n","\n","Around 68% of values are within 1 standard deviation from the mean.\n","\n","Around 95% of values are within 2 standard deviations from the mean.\n","\n","Around 99.7% of values are within 3 standard deviations from the mean.\n","\n","Assumptions of Normal Distribution:\n","\n","Symmetry:\n","\n","The normal distribution is symmetric around its mean.\n","\n","This means that the left and right tails of the distribution are mirror images of each other.\n","\n","Unimodality:\n","\n","The normal distribution has a single peak (mode) at its mean.\n","\n","It does not have multiple modes (bimodal or multimodal).\n","\n","Continuous Data:\n","\n","The normal distribution is appropriate for continuous data (e.g., height, weight, temperature).\n","\n","It is not suitable for discrete data (e.g., number of children, number of cars).\n","Independence:\n","\n","The normal distribution assumes that observations are independent of each other.\n","This assumption is crucial for statistical inference."],"metadata":{"id":"wS0Q97ebeRW_"}},{"cell_type":"markdown","source":["29..\n","\n","\n","Symmetry:\n","\n","The normal distribution is symmetric around its mean (average).\n","\n","The left and right tails of the curve are mirror images of each other.\n","\n","This symmetry is a fundamental property of the bell curve.\n","\n","Mean, Median, and Mode:\n","\n","In a normal distribution:\n","\n","The mean (μ) is the center of the curve.\n","\n","The median is also at the center (equal to the mean).\n","\n","The mode (most frequent value) is also at the center.\n","\n","Therefore, mean = median = mode.\n","\n","Area Under the Curve:\n","\n","The total area under the normal curve equals 1.0 (100%).\n","\n","The probability of any event occurring within the entire distribution is 1.\n","\n","Density in the Center:\n","\n","The normal distribution is denser in the center (around the mean) and less dense in the tails.\n","\n","Most observations cluster around the central peak.\n"],"metadata":{"id":"umyGUInXeeDg"}},{"cell_type":"markdown","source":["30..\n","\n","(a): Within a range of 0.6745 standard deviations (σ) on both sides, the middle 50% of the observations occur (i.e., mean ± 0.6745σ) covers 50% of the area (25% on each side).\n","\n","Correct: This statement is accurate. The middle 50% of the data lies within\n","approximately one standard deviation from the mean.\n","\n","(b): Mean ± 1 standard deviation (μ ± 1σ) covers 68.268% of the area, with 34.134% on either side of the mean.\n","\n","Correct: This is consistent with the empirical rule (68-95-99.7 rule).\n","\n","©: Mean ± 2 standard deviations (μ ± 2σ) covers 95.45% of the area, with 47.725% on either side of the mean.\n","\n","Correct: This aligns with the empirical rule.\n","\n","(d): Mean ± 3 standard deviations (μ ± 3σ) covers 99.73% of the area, with 49.856% on either side of the mean.\n","\n","Correct: Again, this follows the empirical rule.\n","\n","(e): Only 0.27% of the area is outside the range μ ± 3σ.\n","\n","Correct: This is consistent with the tails of the normal distribution being very small (approximately 0.15% in each tail).\n","\n"],"metadata":{"id":"MSIV46lOflvG"}},{"cell_type":"markdown","source":["31..\n","\n","Between 60 and 72:\n","\n","We need to find the area under the curve between these two values.\n","\n","\n","Using the Z-score formula: (Z = \\frac{{X - \\mu}}{{\\sigma}}), where (X) is the\n","value (72), (\\mu) is the mean (60), and (\\sigma) is the standard deviation (10):\n","\n","For 60: (Z_{60} = \\frac{{60 - 60}}{{10}} = 0)\n","\n","For 72: (Z_{72} = \\frac{{72 - 60}}{{10}} = 1.2)\n","\n","We can use a standard normal distribution table or a calculator to find the probabilities associated with these Z-scores:\n","\n","(P(0 \\leq Z \\leq 1.2))\n","\n","The area between 60 and 72 is approximately 0.3849 (or 38.49%).\n","\n","Between 50 and 60:\n","\n","Similar to the previous step, we calculate the Z-scores for 50 and 60:\n","\n","For 50: (Z_{50} = \\frac{{50 - 60}}{{10}} = -1)\n","\n","For 60: (Z_{60} = 0) (as calculated earlier)\n","\n","The area between 50 and 60 is approximately 0.3413 (or 34.13%).\n","\n","Beyond 72:\n","\n","We need to find the area beyond 72 (to the right of 72).\n","\n","Using the Z-score for 72: (Z_{72} = 1.2) (as calculated earlier)\n","\n","The area beyond 72 is approximately 0.1151 (or 11.51%).\n","\n","Between 70 and 80:\n","\n","Similar to the previous steps, we calculate the Z-scores for 70 and 80:\n","\n","For 70: (Z_{70} = \\frac{{70 - 60}}{{10}} = 1)\n","\n","For 80: (Z_{80} = \\frac{{80 - 60}}{{10}} = 2)\n","\n","The area between 70 and 80 is approximately 0.3413 (or 34.13%)."],"metadata":{"id":"JuDva_EZgElx"}},{"cell_type":"markdown","source":["32..\n"],"metadata":{"id":"Ua4nIbq_gh3T"}},{"cell_type":"code","source":["import scipy.stats as stats\n","\n","# Given data\n","mean_marks = 49\n","std_dev = 6\n","\n","# Create a normal distribution object\n","normal_dist = stats.norm(loc=mean_marks, scale=std_dev)\n","\n","# (a) Probability of scoring more than 55 marks\n","prob_more_than_55 = 1 - normal_dist.cdf(55)\n","\n","# (b) Probability of scoring more than 70 marks\n","prob_more_than_70 = 1 - normal_dist.cdf(70)\n","\n","print(f\"Probability of scoring more than 55 marks: {prob_more_than_55:.4f}\")\n","print(f\"Probability of scoring more than 70 marks: {prob_more_than_70:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqIeHUVod74W","executionInfo":{"status":"ok","timestamp":1720797327862,"user_tz":-330,"elapsed":11,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"0103c3fe-9253-4899-f952-7fcdcaceff89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Probability of scoring more than 55 marks: 0.1587\n","Probability of scoring more than 70 marks: 0.0002\n"]}]},{"cell_type":"markdown","source":["33..\n","\n"],"metadata":{"id":"0_9znr54hI_m"}},{"cell_type":"code","source":["import scipy.stats as stats\n","\n","# Given data\n","mean_height = 65\n","std_dev = 5\n","\n","# Create a normal distribution object\n","normal_dist = stats.norm(loc=mean_height, scale=std_dev)\n","\n","# (a) Probability of height greater than 70 inches\n","prob_more_than_70 = 1 - normal_dist.cdf(70)\n","\n","# (b) Probability of height between 60 and 70 inches\n","prob_between_60_and_70 = normal_dist.cdf(70) - normal_dist.cdf(60)\n","\n","print(f\"Probability of height greater than 70 inches: {prob_more_than_70:.4f}\")\n","print(f\"Probability of height between 60 and 70 inches: {prob_between_60_and_70:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMZwONX3g23t","executionInfo":{"status":"ok","timestamp":1720797410817,"user_tz":-330,"elapsed":536,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"4cc62e56-51be-4a61-b8a8-e3b689a4a802"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Probability of height greater than 70 inches: 0.1587\n","Probability of height between 60 and 70 inches: 0.6827\n"]}]},{"cell_type":"markdown","source":["34..\n","\n","1. Statistical Hypothesis:\n","\n","A statistical hypothesis is a statement or assumption about a population parameter (such as the mean, proportion, or variance) that we want to test using sample data.\n","\n","Hypotheses are essential for making decisions based on data.\n","\n","There are two types of hypotheses:\n","\n","Null Hypothesis (H₀): Represents the status quo or no effect. It assumes that there is no significant difference or relationship.\n","\n","Alternative Hypothesis (H₁ or Ha): Represents what we want to prove or find evidence for. It suggests that there is a significant difference or relationship.\n","\n","Hypothesis testing involves comparing sample data to the null hypothesis to determine whether there is enough evidence to reject it in favor of the alternative hypothesis.\n","\n","Errors in Hypothesis Testing:\n","\n","Type I Error (False Positive):\n","\n","Occurs when we reject the null hypothesis when it is actually true.\n","Probability of Type I error is denoted by α (alpha).\n","\n","Reducing α increases the risk of Type II error.\n","\n","Type II Error (False Negative):\n","\n","Occurs when we fail to reject the null hypothesis when it is actually false.\n","Probability of Type II error is denoted by β (beta).\n","\n","Reducing β increases the risk of Type I error.\n","\n","2. Sample:\n","\n","A sample is a subset of a larger population.\n","\n","We collect sample data to make inferences about the entire population.\n","\n","Proper sampling techniques are crucial to ensure that the sample is\n"," representative of the population.\n","\n","Large Samples vs. Small Samples:\n","Large Samples:\n","\n","Typically, when the sample size (n) is 30 or more, we consider it a large sample.\n","\n","Advantages:\n","\n","Central Limit Theorem applies: Sample means are approximately normally distributed.\n","\n","More robust to outliers.\n","\n","Z-tests and t-tests can be used.\n","\n","\n","Disadvantages:\n","\n","Costly and time-consuming to collect large samples.\n","\n","May not be feasible in some cases.\n","\n","\n","Small Samples:\n","\n","When the sample size is less than 30, we consider it a small sample.\n","\n","Advantages:\n","\n","\n","Easier and quicker to collect.\n","\n","May be the only option in certain situations.\n","\n","Disadvantages:\n","\n","Less robust to outliers.\n","\n","T-distribution is used for inference.\n","\n","Limited generalizability."],"metadata":{"id":"APoET200hpNx"}},{"cell_type":"markdown","source":["35..\n"],"metadata":{"id":"s9hW_VrAh9bH"}},{"cell_type":"code","source":["import scipy.stats as stats\n","\n","# Given data\n","sample_size = 25\n","sample_std_dev = 9.0\n","hypothesized_std_dev = 10.5\n","\n","# Calculate the chi-square statistic\n","chi_square_statistic = ((sample_size - 1) * sample_std_dev**2) / hypothesized_std_dev**2\n","\n","# Degrees of freedom\n","df = sample_size - 1\n","\n","# Critical value (for a given significance level, e.g., alpha = 0.05)\n","critical_value = stats.chi2.ppf(0.95, df)\n","\n","# Compare chi-square statistic to critical value\n","if chi_square_statistic > critical_value:\n","    print(\"Reject the null hypothesis (population standard deviation is not 10.5).\")\n","else:\n","    print(\"Fail to reject the null hypothesis (population standard deviation is 10.5).\")\n","\n","# Alternatively, calculate the p-value\n","p_value = 1 - stats.chi2.cdf(chi_square_statistic, df)\n","print(f\"P-value: {p_value:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58DuvDzahLDp","executionInfo":{"status":"ok","timestamp":1720797697737,"user_tz":-330,"elapsed":802,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"c69f61ee-0edb-4dbd-d6e1-da165f661d68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fail to reject the null hypothesis (population standard deviation is 10.5).\n","P-value: 0.8205\n"]}]},{"cell_type":"markdown","source":["37..\n"],"metadata":{"id":"nZlYJ-nTijs5"}},{"cell_type":"code","source":["import numpy as np\n","import scipy.stats as stats\n","\n","# Given data\n","grades = ['A', 'B', 'C', 'D', 'E']\n","observed_freq = np.array([15, 17, 30, 22, 16])\n","total_students = 100\n","\n","# Calculate expected frequency assuming uniform distribution\n","expected_freq = total_students / len(grades)\n","\n","# Calculate chi-square statistic\n","chi_square_statistic = np.sum((observed_freq - expected_freq)**2 / expected_freq)\n","\n","# Degrees of freedom\n","df = len(grades) - 1\n","\n","# Critical value (for a given significance level, e.g., alpha = 0.05)\n","critical_value = stats.chi2.ppf(0.95, df)\n","\n","# Compare chi-square statistic to critical value\n","if chi_square_statistic > critical_value:\n","    print(\"Reject the null hypothesis (grades are not uniformly distributed).\")\n","else:\n","    print(\"Fail to reject the null hypothesis (grades are uniformly distributed).\")\n","\n","# Alternatively, calculate the p-value\n","p_value = 1 - stats.chi2.cdf(chi_square_statistic, df)\n","print(f\"P-value: {p_value:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6BG3xESiRCt","executionInfo":{"status":"ok","timestamp":1720797782103,"user_tz":-330,"elapsed":822,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"60902e6f-66ee-4654-a182-a4d0a5660863"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fail to reject the null hypothesis (grades are uniformly distributed).\n","P-value: 0.1032\n"]}]},{"cell_type":"markdown","source":["38..\n","\n","Formulate Hypotheses:\n","\n","Null Hypothesis (H₀): The distribution of grades is uniform.\n","\n","Alternative Hypothesis (H₁ or Ha): The distribution of grades is not uniform.\n","\n","Calculate the Chi-Square Statistic:\n","\n","We’ll use the chi-square test statistic formula: [ \\chi^2 = \\sum \\frac{{(O - E)^2}}{{E}} ] where:\n","\n","(O) is the observed frequency (given in the Total Frequency).\n","\n","(E) is the expected frequency under the assumption of uniform distribution (total number of students divided by the number of grades).\n","\n","Degrees of Freedom (df):\n","\n","For a chi-square test of goodness of fit, the degrees of freedom ((df)) is the number of categories minus 1.\n","\n","Critical Value and P-Value:\n","\n","We’ll compare the calculated chi-square statistic to the critical value from the chi-square distribution table.\n","\n","Alternatively, we can calculate the p-value associated with the chi-square statistic.\n","\n","Let’s perform the chi-square test using Python"],"metadata":{"id":"VJQvYgsWjGTw"}},{"cell_type":"code","source":["import numpy as np\n","import scipy.stats as stats\n","\n","# Given data\n","grades = ['A', 'B', 'C', 'D', 'E']\n","observed_freq = np.array([15, 17, 30, 22, 16])\n","total_students = 100\n","\n","# Calculate expected frequency assuming uniform distribution\n","expected_freq = total_students / len(grades)\n","\n","# Calculate chi-square statistic\n","chi_square_statistic = np.sum((observed_freq - expected_freq)**2 / expected_freq)\n","\n","# Degrees of freedom\n","df = len(grades) - 1\n","\n","# Critical value (for a given significance level, e.g., alpha = 0.05)\n","critical_value = stats.chi2.ppf(0.95, df)\n","\n","# Compare chi-square statistic to critical value\n","if chi_square_statistic > critical_value:\n","    print(\"Reject the null hypothesis (grades are not uniformly distributed).\")\n","else:\n","    print(\"Fail to reject the null hypothesis (grades are uniformly distributed).\")\n","\n","# Alternatively, calculate the p-value\n","p_value = 1 - stats.chi2.cdf(chi_square_statistic, df)\n","print(f\"P-value: {p_value:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8yaAZwHGilpT","executionInfo":{"status":"ok","timestamp":1720797950045,"user_tz":-330,"elapsed":935,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"abacb41f-0d25-419f-ae22-93cfe59472aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fail to reject the null hypothesis (grades are uniformly distributed).\n","P-value: 0.1032\n"]}]},{"cell_type":"markdown","source":["50.Machine Learning"],"metadata":{"id":"lAohD7I9_R5h"}},{"cell_type":"markdown","source":["1\n","\n","Series\n","A Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating-point numbers, Python objects, etc.). It is akin to a single column of a DataFrame.\n","\n","Key Characteristics of a Series:\n","\n","One-dimensional: Essentially a single column of data.\n","\n","Homogeneous: All elements in a Series are of the same data type.\n","\n","Labeled: Each element has an associated index label, making data access more intuitive.\n","\n","Fast Operations: Operations on a Series are generally faster than on a DataFrame due to its simplicity."],"metadata":{"id":"RS6x-2PE_c6C"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Creating a Series\n","data = [1, 2, 3, 4, 5]\n","index = ['a', 'b', 'c', 'd', 'e']\n","series = pd.Series(data, index=index)\n","print(series)\n","\n"],"metadata":{"id":"zZMGkIzAjOmi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721057078056,"user_tz":-330,"elapsed":11,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"b141301c-aaa8-438c-d46b-06d161cdc7f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a    1\n","b    2\n","c    3\n","d    4\n","e    5\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["DataFrame\n","\n","A DataFrame is a two-dimensional labeled data structure with columns that can be of different data types. It is akin to a table in a database or an Excel spreadsheet.\n","\n","Key Characteristics of a DataFrame:\n","\n","Two-dimensional: Essentially a table with rows and columns.\n","\n","Heterogeneous: Different columns can contain different data types.\n","\n","Labeled: Both rows and columns have labels (indices), allowing for more complex data manipulation and retrieval.\n","\n","Flexible: Can handle a large variety of data formats and sources, including CSV files, SQL databases, and Excel files"],"metadata":{"id":"OdC6pUvg_zMc"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Creating a DataFrame\n","data = {\n","    'Name': ['Alice', 'Bob', 'Charlie'],\n","    'Age': [25, 30, 35],\n","    'City': ['New York', 'Los Angeles', 'Chicago']\n","}\n","df = pd.DataFrame(data)\n","print(df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s183QdLY_pqA","executionInfo":{"status":"ok","timestamp":1721057121873,"user_tz":-330,"elapsed":1233,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"b2d19552-ec90-44d2-8f21-8e034e8e9ee6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      Name  Age         City\n","0    Alice   25     New York\n","1      Bob   30  Los Angeles\n","2  Charlie   35      Chicago\n"]}]},{"cell_type":"markdown","source":["3.\n","\n","\n","\n","loc\n","\n","Label-Based: loc is used for label-based indexing. You access rows and columns by their labels.\n","\n","Inclusive of End: When slicing with labels, the end label is included.\n","\n","Supports Boolean Indexing: You can pass boolean arrays to loc.\n","\n","Can Access Rows and Columns: You can specify both row and column labels to access specific elements.\n","\n","Examples:\n"],"metadata":{"id":"LWihdmWRAFHr"}},{"cell_type":"code","source":["df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=['a', 'b', 'c'])\n","print(df.loc['a'])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKaMrFkY_4qL","executionInfo":{"status":"ok","timestamp":1721057346443,"user_tz":-330,"elapsed":559,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"aee5d165-020e-48e9-c631-b2a19c089b82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A    1\n","B    4\n","Name: a, dtype: int64\n"]}]},{"cell_type":"code","source":["print(df.loc[['a', 'b']])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HstJOMDAAv4q","executionInfo":{"status":"ok","timestamp":1721057358690,"user_tz":-330,"elapsed":497,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"56f8cc6e-4d2d-44e7-f3cc-5f0f90677b48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   A  B\n","a  1  4\n","b  2  5\n"]}]},{"cell_type":"code","source":["print(df.loc['a':'b'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1t1wCv_Ay6X","executionInfo":{"status":"ok","timestamp":1721057369045,"user_tz":-330,"elapsed":6,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"974724b2-7fe2-4937-b392-4f1b9a969325"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   A  B\n","a  1  4\n","b  2  5\n"]}]},{"cell_type":"code","source":["print(df.loc['a', 'A'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sIQjPsrAA1gG","executionInfo":{"status":"ok","timestamp":1721057380958,"user_tz":-330,"elapsed":490,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"1da74b71-a0c6-4ca3-d223-90e311389635"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"markdown","source":["iloc\n","\n","Integer Position-Based: iloc is used for integer-location based indexing. You access rows and columns by their integer positions.\n","\n","Exclusive of End: When slicing with positions, the end position is excluded.\n","\n","Does Not Support Boolean Indexing: You cannot pass boolean arrays to iloc.\n","\n","Can Access Rows and Columns: You can specify both row and column positions to access specific elements."],"metadata":{"id":"cxA8tDAsA8io"}},{"cell_type":"code","source":["print(df.iloc[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbMWxPBVA4We","executionInfo":{"status":"ok","timestamp":1721057417427,"user_tz":-330,"elapsed":778,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"e0fdaf9e-c50d-4edd-a02e-4a7992b85950"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A    1\n","B    4\n","Name: a, dtype: int64\n"]}]},{"cell_type":"code","source":["print(df.iloc[[0, 1]])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6HqkDiFBBLt","executionInfo":{"status":"ok","timestamp":1721057436318,"user_tz":-330,"elapsed":9,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"fbe25854-9baf-438b-d707-c25987c94c41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   A  B\n","a  1  4\n","b  2  5\n"]}]},{"cell_type":"code","source":["print(df.iloc[0:2])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDwB7EXEBFxl","executionInfo":{"status":"ok","timestamp":1721057443025,"user_tz":-330,"elapsed":526,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"bef2db0e-ef34-4c1a-bd70-9e516e1e632a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   A  B\n","a  1  4\n","b  2  5\n"]}]},{"cell_type":"code","source":["print(df.iloc[0, 0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0IwKiZG4BHel","executionInfo":{"status":"ok","timestamp":1721057451545,"user_tz":-330,"elapsed":706,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"5d6375bf-c1ec-46bd-9040-280380c570e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"markdown","source":["4.Supervised Learning\n","\n","Definition: Supervised learning involves training a model on a labeled dataset, meaning that each training example is paired with an output label.\n","\n","Objective: The goal is to learn a mapping from inputs to outputs, so the model can predict the output for new, unseen data.\n","\n","Types:\n","\n","Classification: Predict categorical labels (e.g., spam detection, image recognition).\n","\n","Regression: Predict continuous values (e.g., house price prediction, temperature forecasting).\n","\n","Examples:\n","\n","Classification:\n","\n","Spam detection in email (spam or not spam).\n","\n","Image recognition (cat, dog, or other).\n","\n","Regression:\n","\n","Predicting housing prices based on features like square footage, number of bedrooms, etc.\n","\n","Forecasting stock prices.\n","\n","Algorithms:\n","\n","Linear Regression\n","\n","Logistic Regression\n","\n","Decision Trees\n","\n","\n","Random Forest\n","\n","Support Vector Machines (SVM)\n","\n","Neural Networks\n","\n","Example Workflow:\n","\n","Collect labeled data: (features, label) pairs.\n","\n","Split the data into training and test sets.\n","\n","Train the model on the training set.\n","\n","Validate the model on the test set.\n","\n","Use the trained model to make predictions on new data.\n","\n","Unsupervised Learning\n","\n","Definition: Unsupervised learning involves training a model on a dataset without labeled responses. The model tries to learn the underlying structure of the data.\n","\n","Objective: The goal is to find hidden patterns or intrinsic structures in the input data.\n","\n","Types:\n","\n","Clustering: Grouping data points into distinct clusters (e.g., customer segmentation).\n","\n","Association: Discovering rules that describe large portions of the data (e.g., market basket analysis).\n","\n","Dimensionality Reduction: Reducing the number of random variables under consideration (e.g., PCA).\n","\n","Examples:\n","\n","Clustering:\n","\n","Customer segmentation in marketing.\n","\n","Grouping similar documents in topic modeling.\n","\n","Association:\n","\n","Market basket analysis to find items frequently bought together.\n","\n","Dimensionality Reduction:\n","\n","Reducing the number of features in a dataset while retaining most of the information.\n","\n","Algorithms:\n","\n","K-Means Clustering\n","\n","\n","Hierarchical Clustering\n","\n","DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n","Apriori Algorithm (for association rules)\n","\n","Principal Component Analysis (PCA)\n","\n","t-Distributed Stochastic Neighbor Embedding (t-SNE)\n","\n","Example Workflow:\n","\n","Collect unlabeled data.\n","\n","Apply an unsupervised learning algorithm to the data.\n","\n","Analyze the results to identify patterns, groupings, or structures.\n","\n","Use the discovered patterns to inform decisions or for further analysis."],"metadata":{"id":"y2IzyVhdCo1s"}},{"cell_type":"markdown","source":["5.\n","\n","The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between two sources of error that affect the performance of predictive models:\n","\n","Bias: The error due to the model's assumptions.\n","\n","Variance: The error due to the model's sensitivity to small fluctuations in the training data.\n","\n","Bias\n","\n","Definition: Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model.\n","\n","High Bias: Indicates a model that is too simple and does not capture the underlying patterns in the data well (underfitting).\n","\n","Low Bias: Indicates a model that can capture the complex patterns in the data.\n","Variance\n","\n","Definition: Variance refers to the error introduced by the model's sensitivity to the specific training data it was trained on.\n","\n","High Variance: Indicates a model that fits the training data very closely but fails to generalize well to new, unseen data (overfitting).\n","\n","Low Variance: Indicates a model that is more stable and less sensitive to the training data.\n","\n","Bias-Variance Tradeoff\n","The bias-variance tradeoff describes the balance between these two sources of error:\n","\n","High Bias and Low Variance: The model is too simple, leading to underfitting. It does not perform well on the training data or new data because it misses relevant relations between features and the target outputs.\n","\n","Low Bias and High Variance: The model is too complex, leading to overfitting. It performs well on the training data but poorly on new data because it models the noise in the training data.\n","\n","Goal\n","\n","The goal is to find the right balance between bias and variance to minimize the total error.\n","\n","Visualization\n","\n","Underfitting: Both training error and validation error are high. The model has high bias and low variance.\n","\n","Overfitting: Training error is low, but validation error is high. The model has low bias and high variance.\n","\n","Optimal Model: Both training error and validation error are low, achieving a good balance between bias and variance.\n","\n","Strategies to Manage the Bias-Variance Tradeoff\n","\n","Model Complexity: Adjust the complexity of the model. Simple models (like linear regression) have high bias and low variance, while complex models (like deep neural networks) have low bias and high variance.\n","\n","Regularization: Techniques like L1 (Lasso) and L2 (Ridge) regularization can help reduce variance by adding a penalty to more complex models.\n","\n","Cross-Validation: Use cross-validation to estimate the model's performance on unseen data and help detect overfitting.\n","\n","Ensemble Methods: Techniques like bagging (e.g., Random Forest) can help reduce variance, while boosting (e.g., AdaBoost) can help reduce bias."],"metadata":{"id":"UUbfLrbBC7EA"}},{"cell_type":"markdown","source":["6.\n","\n","Precision, recall, and accuracy are key metrics used to evaluate the performance of a classification model in machine learning. Each metric provides different insights into the model's performance, especially in cases of imbalanced datasets.\n","\n","Precision\n","\n","Definition: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: Of all the instances the model predicted as positive, how many were actually positive?\n","Formula:\n","\n","Precision=\n","\n","  True Positives (TP)\n","_____________________\n","True Positives (TP)\n","+\n","False Positives (FP)\n","\n","Precision=\n","True Positives (TP)+False Positives (FP)\n","True Positives (TP)\n","​\n","\n","Interpretation: High precision indicates a low false positive rate, meaning that when the model predicts a positive class, it is usually correct.\n","\n","Recall (Sensitivity or True Positive Rate)\n","\n","Definition: Recall is the ratio of correctly predicted positive observations to all the actual positives. It answers the question: Of all the instances that are actually positive, how many did the model correctly identify?\n","Formula:\n","\n","Recall\n","=\n","True Positives (TP)\n","True Positives (TP)\n","+\n","False Negatives (FN)\n","Recall=\n","True Positives (TP)+False Negatives (FN)\n","True Positives (TP)\n","​\n","\n","Interpretation: High recall indicates a low false negative rate, meaning that the model is able to identify most of the actual positives.\n","Accuracy\n","\n","Definition: Accuracy is the ratio of correctly predicted observations (both positive and negative) to the total observations. It answers the question: How often is the model correct?\n","\n","Formula:\n","Accuracy\n","=\n","True Positives (TP)\n","+\n","True Negatives (TN)\n","Total Number of Observations\n","Accuracy=\n","Total Number of Observations\n","True Positives (TP)+True Negatives (TN)\n","​\n","\n","Interpretation: Accuracy is useful when the classes are balanced, but it can be misleading in cases of imbalanced datasets.\n","Differences Between Precision, Recall, and Accuracy\n","\n","Precision vs. Recall:\n","\n","Precision focuses on the quality of the positive predictions. It is important when the cost of a false positive is high.\n","Recall focuses on the quantity of the positive predictions. It is important when the cost of a false negative is high.\n","\n","Accuracy vs. Precision and Recall:\n","\n","Accuracy gives a single metric to evaluate the overall performance of the model, but it can be misleading in imbalanced datasets. For example, in a dataset where 95% of the samples are negative, a model that always predicts negative will have 95% accuracy, but it provides no useful information.\n","Precision and Recall provide more detailed insights into the performance of a model, especially when dealing with imbalanced classes. They help understand the trade-off between false positives and false negatives."],"metadata":{"id":"0YQ4hx1DDoVC"}},{"cell_type":"markdown","source":["7.\n","\n","Characteristics of Overfitting\n","\n","High Training Accuracy, Low Test Accuracy: The model performs exceptionally well on the training data but fails to generalize to the test data.\n","Complex Models: Models with too many parameters or overly complex architectures are more prone to overfitting.\n","\n","Small Datasets: Overfitting is more likely when the training dataset is small and not representative of the actual data distribution.\n","\n","How to Prevent Overfitting\n","\n","1. Simplify the Model\n","Reduce the Complexity: Use simpler models with fewer parameters to avoid capturing noise in the data.\n","\n","Regularization: Add a penalty to the loss function for large coefficients to constrain the model’s complexity.\n","\n","L1 Regularization (Lasso): Adds the absolute value of coefficients as a penalty term.\n","\n","L2 Regularization (Ridge): Adds the squared value of coefficients as a penalty term.\n","\n","Elastic Net: Combines L1 and L2 regularization.\n","\n","2. Use More Data\n","\n","Collect More Data: Increasing the size of the training dataset can help the model learn more generalizable patterns.\n","\n","Data Augmentation: Generate additional training data by augmenting existing data, especially useful in image and text data.\n","\n","3. Cross-Validation\n","\n","K-Fold Cross-Validation: Split the dataset into k subsets and train the model k times, each time using a different subset as the validation set and the remaining as the training set. This helps ensure that the model performs well on different subsets of the data.\n","\n","Stratified K-Fold: Ensures that each fold has the same proportion of each class, especially useful for imbalanced datasets.\n","\n","4. Prune the Model\n","\n","Decision Tree Pruning: Remove sections of the tree that provide little power in predicting target variables to avoid overly complex trees.\n","\n","Early Stopping: Stop the training process when the performance on the validation set starts to deteriorate, preventing the model from learning noise in the training data.\n","\n","5. Regularization Techniques in Neural Networks\n","\n","Dropout: Randomly drop units (along with their connections) from the neural network during training to prevent over-reliance on specific paths.\n","Weight Regularization: Add regularization terms to the loss function, similar to L1 and L2 regularization.\n","\n","6. Ensemble Methods\n","Bagging: Train multiple models (e.g., decision trees) on different subsets of the data and average their predictions. Random Forest is a common bagging technique.\n","\n","Boosting: Sequentially train models, each correcting the errors of its predecessor. Examples include AdaBoost and Gradient Boosting.\n","\n","Example: Regularization in Linear Regression\n","\n","Without Regularization:"],"metadata":{"id":"kclmHIdqHfVj"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","# Sample data (replace with your actual data)\n","X = [[1, 2], [3, 4], [5, 6], [7, 8]]  # Features\n","y = [3, 7, 11, 15]  # Labels\n","\n","# Now you can split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","y_train_pred = model.predict(X_train)\n","y_test_pred = model.predict(X_test)\n","\n","print(\"Training MSE:\", mean_squared_error(y_train, y_train_pred))\n","print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCuzWzcoH2q0","executionInfo":{"status":"ok","timestamp":1721059272050,"user_tz":-330,"elapsed":513,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"8024e18b-5ac6-4c48-eff5-c64f2e51204d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training MSE: 6.376625650536513e-30\n","Test MSE: 3.1554436208840472e-30\n"]}]},{"cell_type":"markdown","source":["With Regularization:"],"metadata":{"id":"PU_-5JccImoS"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","\n","model = Ridge(alpha=1.0)  # alpha is the regularization strength\n","model.fit(X_train, y_train)\n","\n","y_train_pred = model.predict(X_train)\n","y_test_pred = model.predict(X_test)\n","\n","print(\"Training MSE:\", mean_squared_error(y_train, y_train_pred))\n","print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pOgZFcNIFth","executionInfo":{"status":"ok","timestamp":1721059346922,"user_tz":-330,"elapsed":556,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"5e351eb2-1e01-464f-cf05-85817e11e441"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training MSE: 0.016937618147448024\n","Test MSE: 0.004839319470699501\n"]}]},{"cell_type":"markdown","source":["8.\n","\n","Why Cross-Validation?\n","\n","When building a machine learning model, we want to know how well it will perform on unseen data (i.e., data not used during training).\n","\n","A simple train-test split evaluates the model only once, which might lead to overfitting or underestimating its true performance.\n","\n","Cross-validation addresses this limitation by repeatedly splitting the data into training and validation subsets, allowing us to estimate the model’s generalization ability more robustly.\n","\n","k-Fold Cross-Validation:\n","\n","The most common form of cross-validation is k-fold cross-validation.\n","Here’s how it works:\n","\n","Divide the dataset into k equally sized folds (subsets).\n","\n","Train the model on k-1 folds and validate it on the remaining fold.\n","\n","Repeat this process k times, using a different fold as the validation set each time.\n","\n","Compute the average performance across all k iterations.\n","\n","The final model is trained on the entire dataset after cross-validation.\n","\n","Benefits of k-Fold Cross-Validation:\n","\n","Provides a more reliable estimate of model performance.\n","\n","Reduces the impact of data randomness or specific splits.\n","\n","Helps detect overfitting or underfitting.\n","\n","Choosing the Value of k:\n","\n","Common choices for k are 5 or 10, but it can vary based on the dataset size and characteristics.\n","\n","Larger k values reduce bias but increase computational cost.\n","\n","Smaller k values may lead to higher variance.\n","\n","\n","Variations on Cross-Validation:\n","\n","Stratified Cross-Validation: Ensures that each fold has a similar class distribution (important for imbalanced datasets).\n","\n","Repeated Cross-Validation: Repeats k-fold cross-validation multiple times to improve reliability."],"metadata":{"id":"OuDSPUJLcAIb"}},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# Sample data\n","X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n","y = np.array([0, 1, 0, 1, 0])\n","\n","# KFold setup\n","kf = KFold(n_splits=5)\n","\n","# Model\n","model = LogisticRegression()\n","\n","# Cross-validation\n","accuracies = []\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    model.fit(X_train, y_train)\n","    predictions = model.predict(X_test)\n","\n","    accuracy = accuracy_score(y_test, predictions)\n","    accuracies.append(accuracy)\n","\n","print(\"Cross-Validation Accuracies:\", accuracies)\n","print(\"Mean Accuracy:\", np.mean(accuracies))\n"],"metadata":{"id":"MqXWwwtcIYSJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721316201143,"user_tz":-330,"elapsed":2037,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"de6ef931-be16-4b2f-88b6-74d805b53419"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-Validation Accuracies: [0.0, 0.0, 1.0, 0.0, 0.0]\n","Mean Accuracy: 0.2\n"]}]},{"cell_type":"markdown","source":["9.\n","\n","Classification\n","Definition:\n","\n","Classification is the task of predicting a categorical label for a given input.\n","The target variable in classification is discrete.\n","\n","Output:\n","\n","The output is a class label. Examples include binary classification (e.g., spam vs. not spam) and multi-class classification (e.g., classifying types of animals).\n","\n","Examples:\n","\n","Email spam detection: Classify emails as \"spam\" or \"not spam\".\n","Handwriting recognition: Classify handwritten digits (0-9).\n","Disease diagnosis: Classify whether a patient has a disease (e.g., cancer) or not.\n","\n","Algorithms:\n","\n","Common algorithms include Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), and Neural Networks.\n","Evaluation Metrics:\n","\n","Accuracy, Precision, Recall, F1 Score, ROC-AUC, Confusion Matrix.\n","Regression\n","\n","Definition:\n","\n","Regression is the task of predicting a continuous numerical value for a given input.\n","\n","The target variable in regression is continuous.\n","\n","Output:\n","\n","The output is a continuous value. Examples include predicting house prices, temperatures, or stock prices.\n","\n","Examples:\n","\n","House price prediction: Predict the price of a house based on its features (e.g., size, location).\n","\n","Temperature forecasting: Predict the temperature for a given day.\n","Sales prediction: Predict the number of units that will be sold in a month.\n","\n","Algorithms:\n","\n","Common algorithms include Linear Regression, Polynomial Regression, Decision Trees, Random Forests, Support Vector Regression (SVR), and Neural Networks.\n","Evaluation Metrics:\n","\n","Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared."],"metadata":{"id":"G4gpRGPYck8M"}},{"cell_type":"markdown","source":["Practical Example\n","Classification:\n","\n","\n","Problem: Predict if an email is spam or not.\n","Data: A dataset with features like word frequency, email length, etc.\n","Output: Spam (1) or Not Spam (0).\n","\n","Regression:\n","\n","Problem: Predict the price of a house.\n","Data: A dataset with features like square footage, number of bedrooms, location, etc.\n","\n","Output: A continuous value representing the price of the house."],"metadata":{"id":"sL_wMgF3cvPJ"}},{"cell_type":"markdown","source":["10..\n","\n","Key Concepts in Ensemble Learning\n","\n","Diversity:\n","\n","Ensemble methods work best when the individual models are diverse, meaning they make different errors on the same data. Diversity can be achieved through different algorithms, different training data subsets, or different training techniques.\n","\n","Base Models (Weak Learners):\n","\n","The individual models in an ensemble are often referred to as base models or weak learners. These models can be of the same type (homogeneous) or different types (heterogeneous).\n","\n","Combination Strategy:\n","\n","The outputs of the base models are combined to produce a final prediction. Common combination strategies include voting (for classification), averaging (for regression), and more sophisticated methods like stacking and boosting.\n","Types of Ensemble Methods\n","\n","Bagging (Bootstrap Aggregating):\n","\n","Concept: Multiple versions of a model are trained on different bootstrap samples (randomly drawn subsets with replacement) of the training data, and their predictions are aggregated.\n","\n","Example: Random Forests.\n","\n","Advantage: Reduces variance and helps in avoiding overfitting.\n","Boosting:\n","\n","Concept: Models are trained sequentially, with each new model focusing on correcting the errors of the previous ones. Models are combined with weighted voting or averaging, where models that perform better get higher weights.\n","Example: AdaBoost, Gradient Boosting Machines (GBM), XGBoost.\n","\n","Advantage: Reduces bias and improves the model’s accuracy.\n","\n","Stacking (Stacked Generalization):\n","\n","Concept: Multiple base models (level-0) are trained, and their predictions are used as input features for a meta-model (level-1), which makes the final prediction.\n","\n","Example: Using logistic regression as a meta-model on top of several decision trees and SVMs.\n","\n","Advantage: Can leverage the strengths of different learning algorithms.\n","Voting:\n","\n","Concept: Multiple models are trained, and their predictions are combined using majority voting (for classification) or averaging (for regression).\n","\n","Types: Hard voting (majority vote) and soft voting (weighted probabilities).\n","Advantage: Simple and effective for combining models.\n","\n","Example: Random Forest (A Bagging Method)\n","\n","Random Forest is a popular ensemble method that builds multiple decision trees and merges them to get a more accurate and stable prediction.\n","\n","Training: Each tree is trained on a different bootstrap sample of the data.\n","\n","Prediction (Classification): Each tree votes for a class, and the class with the majority vote is chosen.\n","\n","\n","Prediction (Regression): Predictions from each tree are averaged to produce the final result.\n","\n","Here’s an illustration of how to use Random Forest in Python with scikit-learn:"],"metadata":{"id":"Bo-rGMG-cyyI"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Sample data\n","X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n","y = [0, 1, 0, 1, 0]\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Initialize and train the Random Forest model\n","model = RandomForestClassifier(n_estimators=10)\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","predictions = model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, predictions)\n","print(f\"Accuracy: {accuracy}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JX1GgnjcMJD","executionInfo":{"status":"ok","timestamp":1721316562541,"user_tz":-330,"elapsed":634,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"09a114a2-b7cc-4ddf-e408-2feb9f7b297e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.0\n"]}]},{"cell_type":"markdown","source":["11..\n","\n","Key Concepts of Gradient Descent\n","Loss Function:\n","\n","A mathematical function that measures the difference between the model's predictions and the actual target values. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy Loss for classification.\n","Gradient:\n","\n","The gradient is a vector of partial derivatives of the loss function with respect to each parameter. It indicates the direction and rate of the steepest increase in the loss function.\n","Learning Rate:\n","\n","A hyperparameter that controls the size of the steps taken to reach a minimum. A smaller learning rate means smaller steps, and a larger learning rate means larger steps.\n","How Gradient Descent Works\n","Initialization:\n","\n","Initialize the model parameters (weights) randomly or with some heuristic.\n","Compute the Loss:\n","\n","Calculate the loss function based on the current parameters.\n","Compute the Gradient:\n","\n","Calculate the gradient of the loss function with respect to each parameter. The gradient indicates how to change the parameters to decrease the loss.\n","Update the Parameters:\n","\n","Adjust the parameters in the opposite direction of the gradient. The size of the adjustment is controlled by the learning rate.\n","𝜃\n","=\n","𝜃\n","−\n","𝜂\n","⋅\n","∇\n","𝜃\n","𝐽\n","(\n","𝜃\n",")\n","θ=θ−η⋅∇\n","θ\n","​\n"," J(θ)\n","𝜃\n","θ represents the model parameters.\n","𝜂\n","η is the learning rate.\n","∇\n","𝜃\n","𝐽\n","(\n","𝜃\n",")\n","∇\n","θ\n","​\n"," J(θ) is the gradient of the loss function with respect to the parameters.\n","Iterate:\n","\n","Repeat steps 2-4 until the loss function converges to a minimum or a predefined number of iterations is reached.\n","Types of Gradient Descent\n","Batch Gradient Descent:\n","\n","Uses the entire dataset to compute the gradient and update the parameters.\n","Pros: Accurate gradient estimation.\n","Cons: Can be slow and computationally expensive for large datasets.\n","Stochastic Gradient Descent (SGD):\n","\n","Uses one data point at a time to compute the gradient and update the parameters.\n","Pros: Faster and can handle large datasets.\n","Cons: More noisy updates, which can lead to oscillations in the loss function.\n","Mini-Batch Gradient Descent:\n","\n","Uses a small random subset (mini-batch) of the data to compute the gradient and update the parameters.\n","Pros: Balance between batch and stochastic gradient descent, efficient, and good convergence properties.\n","\n","Cons: Requires choosing a proper mini-batch size."],"metadata":{"id":"fMGNPxB6d3d5"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Generate sample data\n","np.random.seed(42)\n","X = 2 * np.random.rand(100, 1)\n","y = 4 + 3 * X + np.random.randn(100, 1)\n","\n","# Initialize parameters\n","theta = np.random.randn(2, 1)  # Random initialization\n","learning_rate = 0.1\n","iterations = 1000\n","m = len(X)\n","\n","# Add bias term (X0 = 1) to each instance\n","X_b = np.c_[np.ones((m, 1)), X]\n","\n","# Gradient descent\n","for iteration in range(iterations):\n","    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n","    theta = theta - learning_rate * gradients\n","\n","print(\"Estimated parameters:\", theta)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ln-DNORNdk1d","executionInfo":{"status":"ok","timestamp":1721316677167,"user_tz":-330,"elapsed":506,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"4b3de2fd-a58c-42f5-febb-009d582d3e8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Estimated parameters: [[4.21509616]\n"," [2.77011339]]\n"]}]},{"cell_type":"markdown","source":["12..\n","\n","Gradient Calculation:\n","\n","In BGD, the gradient is computed using the entire dataset. This means that for each iteration, you use all the training examples to calculate the gradient of the loss function with respect to the model parameters.\n","Parameter Update:\n","\n","The model parameters are updated after computing the gradient from the entire dataset.\n","\n","Convergence:\n","\n","BGD tends to converge smoothly to the minimum of the loss function since it uses the entire dataset, resulting in stable updates.\n","\n","Efficiency:\n","\n","BGD can be very slow and computationally expensive, especially for large datasets, because each iteration requires a pass through the entire dataset.\n","\n","Use Cases:\n","\n","Suitable for small to medium-sized datasets where computational resources are sufficient to handle the entire dataset in memory.\n","Stochastic Gradient Descent (SGD)\n","\n","Gradient Calculation:\n","\n","In SGD, the gradient is computed using only one training example at a time. This means that for each iteration, a single example is randomly picked from the dataset to calculate the gradient.\n","\n","Parameter Update:\n","\n","The model parameters are updated immediately after computing the gradient from a single training example.\n","\n","Convergence:\n","\n","SGD can converge faster than BGD because it updates parameters more frequently. However, the path to convergence is more erratic due to the noise introduced by the single-example updates. This can sometimes help in escaping local minima.\n","\n","Efficiency:\n","\n","SGD is much more efficient for large datasets since it updates parameters after processing each training example, making it suitable for online learning scenarios.\n","\n","Use Cases:\n","\n","Suitable for large datasets or streaming data where it’s impractical to load the entire dataset into memory."],"metadata":{"id":"ekRC-M9ae5nX"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Generate sample data\n","np.random.seed(42)\n","X = 2 * np.random.rand(100, 1)\n","y = 4 + 3 * X + np.random.randn(100, 1)\n","\n","# Add bias term (X0 = 1) to each instance\n","X_b = np.c_[np.ones((len(X), 1)), X]\n","\n","# Batch Gradient Descent\n","theta_bgd = np.random.randn(2, 1)  # Random initialization\n","learning_rate = 0.1\n","iterations = 1000\n","\n","for iteration in range(iterations):\n","    gradients = 2 / len(X_b) * X_b.T.dot(X_b.dot(theta_bgd) - y)\n","    theta_bgd = theta_bgd - learning_rate * gradients\n","\n","print(\"Parameters estimated by BGD:\", theta_bgd)\n","\n","# Stochastic Gradient Descent\n","theta_sgd = np.random.randn(2, 1)  # Random initialization\n","learning_rate = 0.1\n","epochs = 50\n","\n","for epoch in range(epochs):\n","    for i in range(len(X_b)):\n","        random_index = np.random.randint(len(X_b))\n","        xi = X_b[random_index:random_index + 1]\n","        yi = y[random_index:random_index + 1]\n","        gradients = 2 * xi.T.dot(xi.dot(theta_sgd) - yi)\n","        theta_sgd = theta_sgd - learning_rate * gradients\n","\n","print(\"Parameters estimated by SGD:\", theta_sgd)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jb-kVmnNeBD5","executionInfo":{"status":"ok","timestamp":1721316942066,"user_tz":-330,"elapsed":821,"user":{"displayName":"Samarendra Rout","userId":"10746499176876453042"}},"outputId":"4c2a0a65-5bd4-4215-8f2f-c3f9dcb4ab65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameters estimated by BGD: [[4.21509616]\n"," [2.77011339]]\n","Parameters estimated by SGD: [[4.00597696]\n"," [2.27516959]]\n"]}]},{"cell_type":"markdown","source":["13..\n","\n","Data Sparsity:\n","\n","In high-dimensional spaces, data points become sparse. Imagine a vast hyperspace where data points are scattered thinly.\n","\n","Sparse data makes it challenging to discern meaningful patterns or relationships due to the vast amount of data required to adequately sample the space.\n","\n","Increased Computational Complexity:\n","\n","As the number of dimensions grows, computational complexity escalates.\n","Algorithms take longer to process, and resource requirements (memory, CPU) increase significantly.\n","\n","Risk of Overfitting:\n","\n","High-dimensional spaces allow models to fit the training data too closely (overfitting).\n","\n","Overfit models perform well on training data but generalize poorly to unseen data.\n","\n","Spurious Correlations:\n","\n","In high dimensions, random correlations can emerge due to the sheer number of features.\n","\n","These spurious correlations can mislead models.\n","\n","How to Tackle It?\n","\n","Dimensionality Reduction Techniques:\n","\n","Feature Selection: Identify and select relevant features while discarding irrelevant or redundant ones. This simplifies the model and improves efficiency.\n","\n","Feature Extraction: Transform high-dimensional data into a lower-dimensional space by creating new features that capture essential information. Techniques like Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) are commonly used.\n","\n","Data Preprocessing:\n","\n","Normalization: Scale features to a similar range to prevent dominance by certain features (especially in distance-based algorithms).\n","\n","Handling Missing Values: Address missing data appropriately (imputation or deletion).\n","\n","Careful Model Design:\n","\n","Choose models that handle high-dimensional data well (e.g., tree-based models, neural networks).\n","\n","Regularization techniques (e.g., L1 regularization) can help prevent overfitting.\n","\n","Python Implementation Example:"],"metadata":{"id":"6MV5_lqwfDwb"}},{"cell_type":"code","source":["# Assume you have a high-dimensional dataset\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Feature selection - Adjust k to be less than or equal to the number of features\n","selector = SelectKBest(score_func=f_classif, k=2) # k must be <= number of features\n","selected_features = selector.fit_transform(X_train, y_train)\n","\n","# Feature extraction (PCA)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X_train)\n","pca = PCA(n_components=2) # Number of components should be less than or equal to number of features\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# Train your model using selected features or PCA-transformed data\n","# Evaluate its performance"],"metadata":{"id":"05eMpCEuf7bD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["14.."],"metadata":{"id":"jLOmF5BmgFkj"}},{"cell_type":"markdown","source":["L1 Regularization (Lasso Regression)\n","Penalty Term: The penalty added to the loss function is proportional to the sum of the absolute values of the model parameters (weights).\n","Mathematical Formulation: The loss function with L1 regularization can be written as:\n","Loss\n","=\n","Original Loss\n","+\n","𝜆\n","∑\n","𝑖\n","∣\n","𝑤\n","𝑖\n","∣\n","Loss=Original Loss+λ\n","i\n","∑\n","​\n"," ∣w\n","i\n","​\n"," ∣\n","\n","where\n","𝜆\n","λ is the regularization parameter, and\n","𝑤\n","𝑖\n","w\n","i\n","​\n","  are the model weights.\n","Effect on Weights: L1 regularization can lead to sparse models, meaning it can drive some weights to exactly zero. This makes it useful for feature selection as it effectively removes less important features.\n","Optimization: The absolute value function is not differentiable at zero, but optimization techniques like subgradient methods can handle this.\n","L2 Regularization (Ridge Regression)\n","Penalty Term: The penalty added to the loss function is proportional to the sum of the squared values of the model parameters.\n","Mathematical Formulation: The loss function with L2 regularization can be written as:\n","Loss\n","=\n","Original Loss\n","+\n","𝜆\n","∑\n","𝑖\n","𝑤\n","𝑖\n","2\n","Loss=Original Loss+λ\n","i\n","∑\n","​\n"," w\n","i\n","2\n","​\n","\n","\n","where\n","𝜆\n","λ is the regularization parameter, and\n","𝑤\n","𝑖\n","w\n","i\n","​\n","  are the model weights.\n","Effect on Weights: L2 regularization tends to shrink the weights uniformly, but not to exactly zero. It helps in reducing the model complexity by penalizing large weights, but all features are retained.\n","Optimization: The squared function is differentiable, making it easier to optimize using gradient descent and other methods.\n","Key Differences\n","Sparsity: L1 regularization can create sparse models by driving some weights to zero, effectively performing feature selection. L2 regularization does not produce sparse models but shrinks all weights.\n","Penalty: L1 uses the absolute values of the weights, while L2 uses the squared values of the weights.\n","Applications: L1 is often used when we want feature selection or believe that only a few features are important. L2 is used when we believe that all features contribute to the output but we want to avoid overfitting by reducing the overall magnitude of the weights.\n","In practice, both methods can be combined in a technique called Elastic Net, which includes both L1 and L2 penalties:\n","\n","Loss\n","=\n","Original Loss\n","+\n","𝜆\n","1\n","∑\n","𝑖\n","∣\n","𝑤\n","𝑖\n","∣\n","+\n","𝜆\n","2\n","∑\n","𝑖\n","𝑤\n","𝑖\n","2\n","Loss=Original Loss+λ\n","1\n","​\n","  \n","i\n","∑\n","​\n"," ∣w\n","i\n","​\n"," ∣+λ\n","2\n","​\n","  \n","i\n","∑\n","​\n"," w\n","i\n","2\n","​\n"],"metadata":{"id":"0WOEnJ_M6p4H"}},{"cell_type":"markdown","source":["15..\n","\n","Components of a Confusion Matrix\n","For a binary classification problem, the confusion matrix is a 2x2 table with four key components:\n","\n","True Positives (TP): The number of instances correctly predicted as the positive class.\n","\n","True Negatives (TN): The number of instances correctly predicted as the negative class.\n","\n","False Positives (FP): The number of instances incorrectly predicted as the positive class (Type I error).\n","\n","False Negatives (FN): The number of instances incorrectly predicted as the negative class (Type II error)."],"metadata":{"id":"tEtR8Wis6zhm"}},{"cell_type":"markdown","source":["Metrics Derived from the Confusion Matrix\n","From the confusion matrix, several important metrics can be derived, such as:\n","\n","Accuracy:\n","(\n","𝑇\n","𝑃\n","+\n","𝑇\n","𝑁\n",")\n","/\n","(\n","𝑇\n","𝑃\n","+\n","𝑇\n","𝑁\n","+\n","𝐹\n","𝑃\n","+\n","𝐹\n","𝑁\n",")\n","(TP+TN)/(TP+TN+FP+FN)\n","\n","\n","Precision:\n","𝑇\n","𝑃\n","/\n","(\n","𝑇\n","𝑃\n","+\n","𝐹\n","𝑃\n",")\n","TP/(TP+FP)\n","\n","\n","Recall (Sensitivity):\n","𝑇\n","𝑃\n","/\n","(\n","𝑇\n","𝑃\n","+\n","𝐹\n","𝑁\n",")\n","TP/(TP+FN)\n","\n","\n","\n","F1 Score:\n","2\n","×\n","(\n","Precision\n","×\n","Recall\n",")\n","/\n","(\n","Precision\n","+\n","Recall\n",")\n","2×(Precision×Recall)/(Precision+Recall)"],"metadata":{"id":"TA9dT_aQ8OXE"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","# Example true labels and predicted labels\n","y_true = np.array([0, 1, 0, 1, 0, 1, 1, 0])\n","y_pred = np.array([0, 1, 0, 0, 0, 1, 1, 1])\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n","# Get more detailed classification report\n","report = classification_report(y_true, y_pred)\n","print(\"\\nClassification Report:\")\n","print(report)\n"],"metadata":{"id":"fMbFPKqx8aTp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["16..\n","\n","The AUC-ROC curve, or Area Under the Receiver Operating Characteristic curve, is a graphical representation of the performance of a binary classification model (which can also be used for multi-class classification) at various classification thresholds.\n","\n","Here are the key points:\n","\n","True Positive Rate (TPR):\n","\n","Also known as sensitivity, recall, or probability of detection.\n","It represents the proportion of true positive predictions (correctly predicted positive instances) out of all actual positive instances.\n","\n","False Positive Rate (FPR):\n","\n","Also known as the probability of false alarm.\n","It equals (1 - \\text{specificity}), where specificity represents the proportion of true negative predictions (correctly predicted negative instances) out of all actual negative instances.\n","\n","ROC Curve:\n","\n","The ROC curve is a plot of TPR (sensitivity) against FPR at different threshold settings.\n","\n","It shows how well the model distinguishes between positive and negative classes as the decision threshold varies.\n","\n","AUC (Area Under the Curve):\n","\n","The AUC of an ROC curve is a single scalar value that summarizes the model’s performance across all classification thresholds.\n","\n","The AUC value ranges from 0 to 1.\n","\n","A model with perfect predictive accuracy would have an AUC of 1, perfectly distinguishing between the classes for all possible thresholds."],"metadata":{"id":"I9-zsuNX8naK"}},{"cell_type":"markdown","source":["17..\n","\n","The KNN algorithm is a non-parametric, supervised learning method used for both classification and regression tasks. It’s a simple yet powerful approach that relies on proximity to make predictions.\n","'\n","Here are the key points:\n","\n","Intuition:\n","\n","Imagine you have a dataset with labeled examples (features and corresponding class labels).\n","\n","KNN works by finding the k nearest neighbors (data points) to a new, unlabeled query point.\n","\n","The class label or value of the query point is determined based on the majority class (for classification) or the average value (for regression) among its k nearest neighbors.\n","\n","How It Works:\n","\n","Given a query point, KNN:\n","\n","Measures the distance (usually Euclidean distance) between the query point and all other data points in the dataset.\n","\n","Selects the k nearest neighbors (based on distance).\n","\n","Assigns the query point the class label or value based on the majority vote or average of the k neighbors.\n","\n","Parameters:\n","\n","k: The number of neighbors to consider (a hyperparameter).\n","Choosing an appropriate k value is crucial; a small k may lead to noise, while a large k may oversmooth the decision boundary.\n","\n","Use Cases:\n","\n","KNN is useful when:\n","\n","Data is not linearly separable.\n","\n","Decision boundaries are complex.\n","\n","You want a simple yet effective model.\n","\n","Pros and Cons:\n","\n","Pros:\n","\n","Intuitive and easy to understand.\n","No explicit training phase (lazy learning).\n","Works well with noisy data.\n","\n","Cons:\n","Computationally expensive (especially for large datasets).\n","Sensitive to irrelevant features.\n","Requires careful preprocessing (scaling, handling missing values)."],"metadata":{"id":"TkJZimS39TPn"}},{"cell_type":"markdown","source":["18..\n","An SVM (Support Vector Machine) is a powerful machine learning algorithm used for both classification and regression tasks. It’s particularly effective for binary classification problems but can be extended to multi-class classification as well.\n","\n","Here are the key points:\n","\n","Objective:\n","\n","The main goal of an SVM is to find an optimal hyperplane (or decision boundary) in an N-dimensional feature space that best separates data points belonging to different classes.\n","\n","The hyperplane should maximize the margin (distance) between the closest points of different classes.\n","\n","How It Works:\n","Given labeled training data, SVM:\n","\n","Maps the data points into a higher-dimensional space (using a kernel function).\n","Finds the hyperplane that maximizes the margin while correctly classifying the training data.\n","\n","The data points closest to the hyperplane (support vectors) play a crucial role in defining the decision boundary.\n","\n","Margin and Support Vectors:\n","\n","The margin is the distance between the hyperplane and the nearest data points of each class.\n","\n","Support vectors are the data points lying on the margin or within it.\n","\n","SVM aims to maximize this margin while minimizing classification errors.\n","Kernel Trick:\n","\n","SVMs can handle non-linear data by using kernel functions (e.g., polynomial, radial basis function).\n","\n","These kernels implicitly map data into a higher-dimensional space, allowing SVMs to find non-linear decision boundaries.\n","\n","C vs. γ (Regularization Parameters):\n","\n","C: Controls the trade-off between achieving a wider margin and minimizing classification errors. Larger C values lead to narrower margins.\n","γ: Influences the shape of the decision boundary (for non-linear kernels). Smaller γ values result in smoother boundaries.\n","\n","Pros and Cons:\n","\n","Pros:\n","Effective in high-dimensional spaces.\n","\n","Robust against overfitting (due to margin maximization).\n","\n","Works well with both linear and non-linear data.\n","\n","Cons:\n","\n","Computationally expensive for large datasets.\n","Sensitive to kernel choice and hyperparameter tuning."],"metadata":{"id":"i1YPbzRJ9taq"}},{"cell_type":"markdown","source":["19..\n","\n","The Kernel Trick is a powerful technique used in SVMs to handle non-linear data. It allows SVMs to implicitly map input data into a higher-dimensional space where a linear separator (hyperplane) can be used to divide the classes.\n","\n"," Here’s how it works:\n","\n","Motivation:\n","\n","SVMs are excellent at finding optimal hyperplanes for linearly separable data.\n","However, many real-world problems involve non-linear data distributions.\n","The goal is to find a decision boundary that separates classes effectively, even when the data is not linearly separable.\n","\n","Feature Mapping:\n","To deal with non-linear data, one approach is to map the input data into a higher-dimensional space where it becomes linearly separable.\n","\n","This mapping involves transforming the data into a new feature space where the separation between data points is clearer.\n","\n","For example, consider data points that are not linearly separable in two dimensions. By mapping them into a three-dimensional space, we might find that they can be separated by a plane in this higher-dimensional space.\n","\n","Kernel Functions:\n","\n","Kernel functions allow SVMs to implicitly perform this feature mapping without explicitly calculating the coordinates in the higher space.\n","\n","Instead of directly computing the dot products in the higher-dimensional space, SVMs use kernel functions to compute the similarity between data points.\n","\n","These kernel functions replace the dot products and enable SVMs to find non-linear decision boundaries.\n","\n","Types of Kernel Functions:\n","\n","Linear Kernel: Assumes data is already linearly separable.\n","\n","Polynomial Kernel: Maps inputs into a polynomial feature space, capturing interactions between features.\n","\n","Radial Basis Function (RBF) Kernel: Useful for capturing complex regions by considering distances between points.\n","\n","Sigmoid Kernel: Mimics neural networks using a sigmoid function.\n","\n","Computational Efficiency:\n","\n","The beauty of the kernel trick lies in its efficiency.\n","\n","It avoids the explicit calculation of coordinates in the higher-dimensional space, saving computational resources and time."],"metadata":{"id":"LaUnM3nS-H31"}},{"cell_type":"markdown","source":["20..\n","\n","Linear Kernel:\n","Use Case: When the data is linearly separable or when the number of features is large compared to the number of samples.\n","\n","Advantages:\n","Effective for high-dimensional data (e.g., text classification, document classification).\n","\n","Simple and computationally efficient.\n","\n","Kernel Function:\n","\n","The linear kernel computes the dot product between data points directly in the original feature space.\n","\n","It can be expressed as: [ K(x, x’) = x \\cdot x’ ]\n","\n","Polynomial Kernel:\n","\n","Use Case: Suitable for non-linear problems.\n","\n","Advantages:\n","\n","Captures interactions between features.\n","\n","Handles curved decision boundaries.\n","\n","\n","Kernel Function:\n","\n","The polynomial kernel maps data into a higher-dimensional space using polynomial functions.\n","\n","It can be expressed as: [ K(x, x’) = (x \\cdot x’ + c)^d ]\n","\n","Parameters:\n","\n","\n","(d): Degree of the polynomial (usually 2 or 3).\n","\n","(c): An optional constant term.\n","\n","Radial Basis Function (RBF) Kernel:\n","\n","Use Case: Default choice for SVMs; effective for complex relationships.\n","\n","Advantages:\n","\n","Handles non-linear data effectively.\n","\n","Captures intricate patterns.\n","\n","Kernel Function:\n","\n","The RBF kernel computes the similarity between data points using a Gaussian function.\n","\n","It can be expressed as: [ K(x, x’) = e^{-\\gamma |x - x’|^2} ]\n","Parameter:\n","\n","(\\gamma): Controls the shape of the decision boundary (higher values make it more flexible).\n","\n","Sigmoid Kernel:\n","\n","Use Case: Rarely used; resembles neural networks.\n","\n","Advantages:\n","\n","Can capture complex relationships.\n","\n","\n","Kernel Function:\n","\n","The sigmoid kernel maps data into a sigmoid-shaped space.\n","It can be expressed as: [ K(x, x’) = \\tanh(\\alpha x \\cdot x’ + c) ]\n","\n","Parameters:\n","\n","(\\alpha): Scaling factor.\n","(c): An optional constant term."],"metadata":{"id":"_-mcgUeq-4V6"}},{"cell_type":"markdown","source":["21.\n","\n","What Is a Hyperplane?\n","\n","In SVM, a hyperplane is a decision boundary that separates data points belonging to different classes.\n","\n","For binary classification, the hyperplane divides the feature space into two regions—one for each class.\n","\n","Mathematically, a hyperplane in an N-dimensional space is represented by the equation: [ w \\cdot x + b = 0 ]\n","\n","Here:\n","\n","(w) is the normal vector to the hyperplane.\n","\n","\n","\n","(x) is a data point (feature vector).\n","\n","(b) is a scalar parameter known as the bias term.\n","\n","Determining the Hyperplane:\n","\n","The goal is to find the optimal hyperplane that maximizes the margin (distance) between the closest points of different classes.\n","\n","The margin is the region between two parallel hyperplanes (support vectors) that contains the data points.\n","\n","The optimal hyperplane is the one that maximizes this margin while correctly classifying the training data.\n","\n","Support Vectors:\n","\n","The data points lying on the margin or within it are called support vectors.\n","These support vectors play a crucial role in defining the decision boundary.\n","\n","Choosing the Optimal Hyperplane:\n","\n","SVM aims to find the hyperplane that maximizes the margin while minimizing classification errors.\n","\n","The optimal hyperplane is the one with the largest margin.\n","It is determined by solving the SVM optimization problem."],"metadata":{"id":"T81Yajm51JVy"}},{"cell_type":"markdown","source":["22..\n","\n","Pros of Using Support Vector Machines (SVMs):\n","\n","High Accuracy in High-Dimensional Spaces:\n","\n","SVMs excel at handling high-dimensional data, making them suitable for complex datasets.\n","\n","They avoid the curse of dimensionality better than many other classifiers.\n","\n","Applications: Bioinformatics, text classification, and image recognition.\n","\n","Robustness Against Overfitting in High-Dimensional Space:\n","\n","SVMs are inherently robust against overfitting, especially when the number of dimensions exceeds the number of samples.\n","\n","The regularization parameter balances training accuracy and model complexity for better generalization.\n","\n","Useful in finance, healthcare, and critical applications.\n","\n","Works for Non-Linear Data:\n","\n","Through the kernel trick, SVMs handle non-linearly separable data effectively.\n","Captures intricate patterns and complex relationships.\n","\n","Cons of Using Support Vector Machines (SVMs):\n","\n","Training Time for Large Datasets:\n","\n","SVMs can be computationally expensive, especially for large datasets.\n","\n","Training time increases significantly as the dataset size grows.\n","\n","Consider alternative models for big data scenarios.\n","\n","Less Effective on Noisier Datasets with Overlapping Classes:\n","\n","SVMs assume that data is well-separated.\n","\n","In noisy datasets with overlapping classes, SVMs may struggle.\n","\n","Proper preprocessing and feature engineering are crucial.\n","\n","Complex Model Interpretability:\n","\n","SVMs find complex decision boundaries, which can be hard to interpret.\n","\n","Understanding the final model and individual feature impact may be challenging."],"metadata":{"id":"c5JiBN3N1Rev"}},{"cell_type":"markdown","source":["23..\n","\n","Hard Margin SVM:\n","\n","Objective: In a hard margin SVM, the goal is to identify a hyperplane that completely separates data points belonging to different classes, ensuring a clear demarcation with the utmost margin width possible.\n","\n","Margin: The margin is the distance between the hyperplane and the nearest data point (also known as the support vectors) from each class.\n","\n","Scenario:\n","\n","Hard margin SVM is suitable when the data is linearly separable.\n","\n","It aims for perfect separation without allowing any misclassification.\n","\n","Soft Margin SVM:\n","\n","Objective: In a soft margin SVM, we allow for some misclassification (softening the constraints) to handle non-linearly separable data.\n","\n","Margin: The margin is still important, but we tolerate a few data points falling within the margin or even on the wrong side of the hyperplane.\n","\n","Scenario:\n","\n","Soft margin SVM is appropriate when the data is partially overlapping or when outliers are present.\n","\n","It balances the trade-off between maximizing the margin and allowing some errors."],"metadata":{"id":"K3_FnJP111H2"}},{"cell_type":"markdown","source":["24..\n","\n","Import Necessary Libraries: First, we’ll import the libraries required for building our decision tree. The essential ones include:"],"metadata":{"id":"3AUmGrQs2Rw8"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"1k_YPQ1Y9Stv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load and Prepare Your Data:\n","\n","Load your dataset using the read_csv() function from Pandas.\n","Display the top few rows of your dataset using the head() function to understand its structure.\n","\n","Separate the independent features (attributes) and the dependent variable (target) using slicing.\n","\n","Split the Data:\n","\n","Split your dataset into training and testing subsets. This ensures that we can evaluate our model’s performance.\n"],"metadata":{"id":"Uj3200Rb29la"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"],"metadata":{"id":"m56R2IbF3ECN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the Decision Tree Model:\n","Initialize a decision tree classifier. You can choose either the Gini impurity or entropy as the splitting criterion. For instance:"],"metadata":{"id":"KEir0dA33H1Y"}},{"cell_type":"code","source":["clf = DecisionTreeClassifier(criterion='gini', min_samples_leaf=30, random_state=0)\n"],"metadata":{"id":"Ayzu2MQ33IzO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fit the model to your training data:"],"metadata":{"id":"YnMhChDV3OFj"}},{"cell_type":"code","source":["clf.fit(X_train, y_train)\n"],"metadata":{"id":"5XDtGrB73PyL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize the Decision Tree:\n","You can plot the decision tree to understand its structure. This visualization helps you see how the tree makes decisions."],"metadata":{"id":"czlJB8zt3Scd"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","tree.plot_tree(clf, filled=True, feature_names=X.columns)\n","plt.show()\n"],"metadata":{"id":"NEeHMSw53Tmo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make Predictions:\n","\n","Once your model is trained, you can use it to make predictions on new data.\n"],"metadata":{"id":"gjtvFCDR3XiV"}},{"cell_type":"code","source":["y_pred = clf.predict(X_test)\n"],"metadata":{"id":"4EtkXPKF3Z8E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluate Model Performance:\n","\n","Calculate the accuracy, precision, recall, F1-score, or any other relevant metrics to assess how well your decision tree performs."],"metadata":{"id":"pyF26a5F3cZi"}},{"cell_type":"markdown","source":["25...\n","\n","\n","\n","Selecting the Best Attribute:\n","\n","Decision trees start with the entire dataset at the root node.\n","\n","The algorithm selects the best attribute (feature) to split the data. This selection is crucial and is based on metrics like information gain, Gini impurity, or entropy.\n","\n","The goal is to find the attribute that provides the most useful separation of data into different classes.\n","\n","Splitting the Data:\n","\n","Once the best attribute is chosen, the dataset is split into subsets based on the attribute’s values.\n","\n","Each subset corresponds to a branch from the root node.\n","\n","The process continues recursively for each subset, creating internal nodes and branches.\n","\n","Leaf Nodes and Predictions:\n","\n","When to stop splitting? We define stopping criteria, such as:\n","\n","All instances in a subset belong to the same class.\n","\n","A maximum depth is reached.\n","\n","At this point, we create leaf nodes representing the final decisions or predictions.\n","\n","For classification tasks, the majority class in the leaf node becomes the predicted class.\n","\n","For regression tasks, the average (or another suitable measure) of the target variable in the leaf node becomes the prediction.\n","\n","Attribute Selection and Splitting Criteria:\n","\n","\n","Gini Impurity:\n","\n","Measures how often a randomly chosen element would be incorrectly identified."],"metadata":{"id":"sTR414te3fgF"}},{"cell_type":"markdown","source":["Entropy:\n","\n","Measures the amount of uncertainty or impurity in the dataset."],"metadata":{"id":"6HBSsRde398h"}},{"cell_type":"markdown","source":["Information Gain:\n","\n","Measures the reduction in entropy or Gini impurity after splitting."],"metadata":{"id":"DT4-XfMK4Bfl"}},{"cell_type":"markdown","source":["Visualizing Decision Trees:\n","\n","You can plot decision trees using libraries like scikit-learn or graphviz.\n","Visualizing the tree helps you understand how it makes decisions.\n","\n","Assumptions and Caveats:\n","\n","We assume attributes are categorical for information gain and continuous for Gini index.\n","\n","Decision trees can overfit if not pruned properly.\n","\n","Example in Python:"],"metadata":{"id":"xr1rzes04E-3"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","\n","# Load your dataset (e.g., from a CSV file)\n","data = pd.read_csv(\"your_dataset.csv\")\n","\n","# Separate features (X) and target (y)\n","X = data.drop(columns=[\"target_column\"])\n","y = data[\"target_column\"]\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and fit the decision tree model\n","clf = DecisionTreeClassifier(max_depth=5)\n","clf.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = clf.predict(X_test)\n"],"metadata":{"id":"AjFsqlBF4LrM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["26..\n","\n","What Is Information Gain?\n","\n","Information gain quantifies how much “information” a feature provides about the class labels.\n","\n","Imagine you’re playing detective with data: Each feature is a clue, and information gain tells you how important that clue is for solving the case (classifying instances).\n","\n","Specifically, it measures the reduction in entropy (a measure of uncertainty) of the target variable (class labels) when we know a particular feature.\n","\n","Entropy and Uncertainty:\n","\n","Entropy (Η): Think of it as the level of chaos or randomness in a dataset. High entropy means more uncertainty.\n","\n","When we split data based on a feature, we want to reduce this uncertainty—make things clearer.\n","\n","So, information gain tells us how much clearer things become after considering a specific feature.\n","\n","How It Works:\n","\n","Suppose we have a node in our decision tree. We want to split it using one of the available features.\n","\n","We calculate the entropy of the current node (before splitting).\n","\n","Then, we calculate the weighted average entropy of the child nodes (after splitting) based on the chosen feature.\n","\n","The difference between the initial entropy and the average child entropy gives us the information gain.\n","\n","Decision Tree Splitting:\n","\n","When constructing a decision tree:\n","\n","We evaluate information gain for each feature.\n","\n","The feature with the highest information gain becomes the splitting criterion for that node.\n","\n","It’s like choosing the most informative clue to proceed with our investigation.\n","\n","Mathematically:\n","\n","The information gain for an attribute (a) is defined in terms of Shannon entropy: [ \\text{Information Gain} = \\text{Entropy}{\\text{parent}} - \\sum{v} \\left(\\frac{|D_v|}{|D|} \\ast \\text{Entropy}(D_v)\\right) ]\n","(D_v) represents the subset of data where attribute (a) takes value (v).\n","\n","We calculate the entropy for each subset and weigh it by the proportion of instances in that subset.\n","\n","Why Information Gain Matters:\n","\n","It guides the decision tree toward the most informative features.\n","\n","Features that lead to better separation of classes are preferred.\n","\n","By choosing wisely, we build a tree that efficiently classifies instances."],"metadata":{"id":"ifm4uYfo4Qef"}},{"cell_type":"markdown","source":["27..\n","\n","Gini impurity is a measure of impurity or disorder within a dataset. Specifically, it quantifies the likelihood that a randomly selected data point would be misclassified based on the distribution of classes in that dataset. Think of it as the “chaos factor” in our decision-making process.\n","\n","Here’s the essence of Gini impurity:\n","\n","The Gini Index Formula:\n","\n","The Gini impurity for a given node (or subset of data) is calculated as follows:\n","\n"," [ \\text{Gini Index} = 1 - \\sum_{i=1}^n (p_i)^2 ]\n","\n","Here, (p_i) represents the probability of an instance belonging to class (i).\n","The Gini Index ranges from 0 (perfect purity, where all instances belong to the same class) to 0.5 (maximum impurity, where instances are evenly distributed across classes).\n","\n","Role in Decision Trees:\n","\n","Decision trees aim to create pure subsets (nodes) by minimizing impurity.\n","When deciding how to split a node, we evaluate potential splits based on their resulting Gini impurity.\n","\n","The split that reduces impurity the most (i.e., maximizes information gain) is chosen.\n","\n","By selecting features that lead to purer nodes, decision trees become effective classifiers.\n","\n","Comparison with Entropy:\n","\n","Gini impurity and entropy (another splitting criterion) are closely related.\n","Both measure disorder, but they have different mathematical formulations.\n","\n","Gini impurity tends to favor larger partitions with dominant classes, while entropy considers more balanced splits.\n","\n","In practice, they often lead to similar results, but the choice depends on the problem and personal preference."],"metadata":{"id":"wTPlndgO4pU1"}},{"cell_type":"markdown","source":["28..\n","\n","\n","Advantages of Decision Trees:\n","\n","Relatively Easy to Interpret:\n","\n","Trained decision trees are quite intuitive to understand. Their entire structure can be visualized as a simple flowchart.\n","\n","Unlike some other complex algorithms, decision trees don’t hide their secrets—they lay them out branch by branch.\n","\n","Robust to Outliers:\n","\n","A well-regularized decision tree can handle outliers gracefully.\n","\n","Predictions are based on aggregated functions (e.g., mean or mode) over subsets of training data, making them less sensitive to extreme values.\n","\n","Can Deal with Missing Values:\n","\n","The CART (Classification and Regression Trees) algorithm naturally handles missing values.\n","\n","No additional preprocessing is needed to address missing data—a relief compared to many other algorithms.\n","\n","Non-Linear:\n","\n","Decision trees are inherently non-linear models.\n","\n","They work well for complex problems where linear assumptions fall short.\n","\n","Non-Parametric:\n","Decision trees don’t assume specific underlying data distributions.\n","\n","You’re free from worrying about whether your data fits a particular statistical model.\n","\n","Combining Features for Predictions:\n","\n","Decision rules (those if-else conditions) can combine features via AND relationships.\n","\n","It’s like teamwork—features collaborate to make predictions.\n","\n","Disadvantages of Decision Trees:\n","\n","Prone to Overfitting:\n","\n","Decision trees can get too cozy with the training data, capturing noise and becoming overly complex.\n","\n","Regularization techniques (like pruning) are needed to prevent this.\n","\n","Unstable to Changes in the Data:\n","Small changes in the dataset can lead to different splits and tree structures.\n","Stability isn’t their strong suit.\n","\n","Unstable to Noise:\n","\n","Noise in the data can mislead decision trees.\n","\n","They might split based on irrelevant features.\n","\n","Non-Continuous:\n","\n","Decision trees create step-like boundaries, which might not be suitable for continuous data.\n","\n","Unbalanced Classes:\n","\n","If your classes are imbalanced, decision trees tend to favor the majority class.\n","You might need to balance the class distribution or use other techniques.\n","\n","Greedy Algorithm:\n","\n","Decision trees make locally optimal decisions at each node (greedy approach).\n","This can lead to suboptimal global structures.\n","\n","Computationally Expensive on Large Datasets:\n","\n","Constructing decision trees can be resource-intensive for large datasets.\n","\n","They’re like marathon runners—great for sprints, but not ideal for marathons.\n","\n","Complex Calculations on Large Datasets:\n","\n","Calculating information gain or Gini impurity for every feature can be\n","demanding.\n","\n","Decision trees might need a coffee break when faced with massive data."],"metadata":{"id":"6OjiL3-Z5PcC"}},{"cell_type":"markdown","source":["29..\n","\n","\n","The Ensemble Cast:\n","\n","Decision Trees: These are like solo artists—each tree makes predictions independently. They can be quirky, overfit, and sometimes a tad too expressive.\n","Random Forests: Picture a band—a whole ensemble of decision trees jamming together. They’re like a musical collaboration, and they bring harmony to the chaos.\n","\n","Reducing Overfitting:\n","\n","Decision Trees: These fellas tend to memorize the training data. They’re like that friend who insists on recounting every detail of last night’s dream.\n","\n","Random Forests: They’re the chill version. They take random samples (with replacement) from the data and build multiple trees. Then they average out the results. It’s like crowd wisdom—no one tree dominates the conversation.\n","\n","Stability and Robustness:\n","\n","Decision Trees: A gust of wind, and they change their minds. They’re as stable as a Jenga tower during an earthquake.\n","\n","Random Forests: They’re the rock-solid buddies. By combining many trees, they smooth out the rough edges. No sudden mood swings here!\n","\n","Bias and Variance Dance:\n","\n","Decision Trees: Sometimes they’re too biased (like that stubborn uncle who insists pineapple belongs on pizza). Other times, they’re all over the place (hello, variance!).\n","\n","Random Forests: They strike a balance. Their ensemble approach reduces bias and tames variance. It’s like Goldilocks—just right.\n","\n","Metrics and Applause:\n","\n","Decision Trees: Solo performances can be hit or miss. Metrics like accuracy might wobble.\n","\n","Random Forests: They’re the headliners. Accuracy, AUC score, RMSE—all get a standing ovation. 🎤👏\n","\n","Real-World Reliability:\n","\n","Decision Trees: Great for small gigs, but risky for big concerts. They might forget lyrics or go off-key.\n","\n","Random Forests: They’re the seasoned pros. Reliable, robust, and ready for the stadium tour."],"metadata":{"id":"ctOEa35456dF"}},{"cell_type":"markdown","source":["30..\n","\n","What Is a Random Forest?\n","\n","Ensemble Magic:\n","\n","Imagine a forest where each tree has its own personality. Random Forest is an ensemble of decision trees, and together, they form a powerful choir.\n","\n","These trees collaborate, vote, and create a symphony of predictions.\n","\n","The Steps of the Random Forest Waltz:\n","\n","Random Sampling (Bootstrapping):\n","\n","The music begins! Random Forest selects random samples (with replacement) from the original dataset.\n","\n","Each sample becomes a mini-dataset for a decision tree.\n","\n","Creating Decision Trees:\n","\n","For each mini-dataset, a decision tree grows. These trees are like soloists, each learning from a different perspective.\n","\n","But wait, there’s more randomness:\n","\n","Randomly select a subset of features (columns) for each tree. Variety is the spice of life!\n","\n","Split nodes based on these features, creating branches and leaves.\n","\n","Predictions by the Choir:\n","\n","When a new data point arrives, each tree sings its prediction.\n","For classification:\n","\n","Majority voting: The choir counts the votes for each class.\n","\n","The class with the most votes becomes the final prediction.\n","\n","For regression:\n","\n","Averages: Each tree hums its numeric prediction.\n","\n","The ensemble takes the average—a harmonic blend.\n","\n","Why Randomness?\n","\n","Diversity Matters:\n","\n","Randomness introduces variability among individual trees.\n","\n","It prevents overfitting (when a tree memorizes the training data) and keeps our choir balanced.\n","\n","Advantages of Our Forest:\n","\n","Robustness:\n","\n","Outliers? No worries! The ensemble smooths them out.\n","\n","Stability? Our choir won’t change its tune with minor data shifts.\n","\n","Non-Linear Magic:\n","\n","Random Forest handles complex relationships. It’s like jazz—improvising, yet harmonious.\n","\n","Feature Love:\n","\n","All features get a chance to shine. No prima donnas here!\n","\n","Caveats:\n","\n","Computation Cost:\n","\n","Our forest isn’t lightweight. Constructing many trees takes time.\n","But hey, quality over speed, right?\n","\n","Hyperparameters:\n","\n","We tweak settings like the number of trees, feature subset size, and tree depth.\n","It’s like tuning instruments for a perfect melody."],"metadata":{"id":"SXSkAGzh6d9k"}},{"cell_type":"markdown","source":["31..\n","\n","Bootstrapping and Random Forests\n","\n","What Is Bootstrapping?\n","\n","Bootstrapping is a statistical resampling technique that involves randomly sampling data with replacement from a dataset.\n","\n","Imagine you’re hosting a party, and you want to estimate the average height of your guests. Instead of measuring everyone, you randomly select a few guests, measure their heights, and then put them back into the party (with replacement). Repeat this process multiple times.\n","\n","Bootstrapping allows us to generate new samples from existing data without collecting additional data. It’s like creating virtual parties within the same crowd!\n","\n","Random Forests and Bootstrapping:\n","\n","In the magical world of random forests, bootstrapping plays a crucial role.\n","Here’s how it works:\n","\n","Step 1: Bootstrapped Samples\n","\n","For each decision tree in the random forest, we create a bootstrapped sample.\n","We randomly select data points (instances) from the original training set, allowing duplicates (replacement).\n","\n","These bootstrapped samples become the training data for individual trees.\n","\n","Step 2: Tree Construction\n","\n","Each decision tree learns from its bootstrapped sample.\n","\n","But wait, there’s more randomness:\n","\n","We also randomly select a subset of features (columns) for each tree. Variety is the spice of the forest!\n","\n","The tree splits nodes based on these features, creating branches and leaves.\n","Step 3: Ensemble Predictions\n","\n","When a new data point arrives, each tree in the forest makes its prediction.\n","For classification:\n","\n","Majority voting: The forest counts the votes for each class.\n","\n","The class with the most votes becomes the final prediction.\n","\n","For regression:\n","\n","Averages: Each tree contributes its numeric prediction.\n","\n","\n","The ensemble takes the average—a harmonious blend.\n","\n","Why Bootstrapping Matters:\n","\n","Diversity and Stability:\n","\n","Bootstrapping introduces variability among individual trees.\n","\n","It prevents overfitting (when a tree memorizes the training data) and keeps our forest balanced.\n","\n","Robustness:\n","\n","Outliers? No worries! The ensemble smooths them out.\n","\n","Stability? Our forest won’t change its tune with minor data shifts.\n","\n","In a Nutshell:\n","\n","Random Forests are the maestros of ensemble learning—combining wisdom, randomness, and harmony.\n","\n","So next time you’re in a data forest, remember: It’s all about teamwork! 🌲🎩"],"metadata":{"id":"Ger49Mr966ZY"}},{"cell_type":"markdown","source":["32..\n","\n","The Melody of Feature Importance:\n","\n","Random Forests and Their Ensemble Symphony:\n","\n","Random Forests, those harmonious ensembles, consist of multiple decision trees. Each tree learns from a different perspective, like soloists in a choir.\n","But how do we measure the contribution of each feature to the grand prediction made by this forest?\n","\n","The Impurity Dance:\n","\n","Random Forests calculate feature importance by evaluating the decrease in impurity (think Gini impurity or entropy) each time a feature is used to split the data.\n","\n","Here’s the rhythm:\n","\n","When a feature is chosen for a split in a decision tree, it reduces the impurity within that node.\n","\n","The more a feature reduces impurity, the more important it’s considered.\n","\n","Why It Matters: The Encore Performance:\n","\n","Understanding feature importance offers several advantages:\n","\n","Enhanced Model Performance: By identifying the most influential features, we can prioritize them during model training, leading to more accurate predictions.\n","Faster Training Times: Focusing on relevant features streamlines the training process, saving valuable time and computational resources.\n","\n","Reduced Overfitting: Overfitting occurs when a model memorizes the training data instead of learning general patterns. By emphasizing important features, we prevent overreliance on specific data points.\n","\n","Feature Importance Techniques: The Variations in Harmony:\n","\n","Built-in Feature Importance:\n","\n","Utilizes the model’s internal calculations (e.g., Gini importance or mean decrease in accuracy).\n","\n","Measures how much impurity decreases within a decision tree node when a specific feature is used for splitting.\n","\n","Permutation Feature Importance:\n","\n","Assesses the significance of each feature independently.\n","\n","Evaluates the impact of individual feature permutations on predictions.\n","SHAP (SHapley Additive exPlanations) Values:\n","\n","Delve deeper into the contribution of each feature to individual predictions.\n","Offers a comprehensive understanding of feature importance across various data points."],"metadata":{"id":"FNKXhA_27MXY"}},{"cell_type":"markdown","source":["33..\n","\n","Key Hyperparameters of Random Forest:\n","\n","n_estimators (Number of Trees):\n","\n","This parameter defines how many decision trees form our forest.\n","\n","More trees generally lead to better performance, but there’s a trade-off with computational cost.\n","\n","Too few trees may result in underfitting, while too many can lead to overfitting.\n","\n","max_depth (Maximum Depth of Trees):\n","\n","Controls how deep each individual tree can grow.\n","\n","A deeper tree can capture intricate patterns but risks overfitting.\n","Shallower trees generalize better but might miss complex relationships.\n","\n","min_samples_split (Minimum Samples per Split):\n","\n","Specifies the minimum number of samples required to split a node.\n","Higher values prevent excessive splitting, promoting robustness.\n","Too high, and the tree might miss important patterns.\n","\n","min_samples_leaf (Minimum Samples per Leaf):\n","\n","Sets the minimum number of samples required in a leaf node.\n","Larger values prevent tiny leaves (overfitting), but too large can underfit.\n","\n","max_features (Number of Features to Consider):\n","\n","Determines how many features are randomly considered at each split.\n","\n","Smaller values increase diversity among trees, reducing overfitting.\n","\n","Larger values allow more exploration but might lead to correlated trees.\n","\n","max_samples (Bootstrap Sample Size):\n","\n","Controls the size of the bootstrap sample (randomly drawn with replacement).\n","\n","Smaller values reduce overfitting but might sacrifice performance.\n","\n","Effects on the Model:\n","\n","Overfitting vs. Underfitting:\n","\n","Adjusting hyperparameters helps find the sweet spot between overfitting (memorizing data) and underfitting (too simplistic).\n","\n","Regularization hyperparameters (e.g., max_depth, min_samples_split, min_samples_leaf) combat overfitting.\n","\n","Increasing n_estimators and exploring more features can improve generalization.\n","\n","Trade-offs:\n","\n","Tinkering with hyperparameters involves trade-offs:\n","\n","More trees (higher n_estimators) improve stability but increase computation time.\n","\n","Deeper trees (higher max_depth) capture complex patterns but risk overfitting.\n","Smaller leaves (higher min_samples_leaf) prevent overfitting but might miss details.\n","\n","Feature Importance:\n","\n","Random Forests naturally provide feature importance scores.\n","\n","By analyzing these, we discover which features contribute most to predictions."],"metadata":{"id":"pTqHR-h27sKy"}},{"cell_type":"markdown","source":["34..\n","\n","\n","\n","Logistic Regression: The Binary Serenade\n","Logistic regression is like a musical duet between data and prediction. It’s specifically designed for situations where our response variable (the outcome we’re trying to predict) is binary—meaning it can take only two possible values. Think of it as predicting whether a song will be a hit or a miss based on certain features.\n","\n","\n","Here’s the sheet music for logistic regression:\n","\n","The Binary Response Variable:\n","\n","Logistic regression assumes that our response variable (the dependent variable) has only two outcomes.\n","\n","Examples include:\n","\n","Yes or No\n","\n","Male or Female\n","\n","Pass or Fail\n","\n","Malignant or Benign (in medical diagnoses)\n","\n","Independence of Observations:\n","\n","Each data point (observation) should be independent of the others.\n","No repeated measurements of the same individual or any hidden connections.\n","Imagine each note in our melody being played without interference from neighboring notes.\n","\n","No Multicollinearity Among Explanatory Variables:\n","\n","Multicollinearity occurs when explanatory variables (features) are highly correlated with each other.\n","\n","We want our features to provide unique and independent information.\n","\n","For example, if we’re predicting player performance in basketball, we wouldn’t want both “height” and “shoe size” as features—they’re likely correlated.\n","\n","No Extreme Outliers or Influential Observations:\n","\n","Outliers can disrupt our harmonious tune.\n","\n","We check for extreme data points using metrics like Cook’s distance.\n","\n","If outliers exist, we decide whether to remove them, replace them, or keep them with a note about their impact.\n","\n","No Linear Relationship Assumption:\n","\n","Unlike linear regression, logistic regression doesn’t require a linear relationship between features and the log-odds of the response.\n","\n","It’s more about probabilities and thresholds.\n","\n","No Normality Assumption:\n","\n","Logistic regression doesn’t assume that the error terms (residuals) follow a normal distribution.\n","\n","We’re not playing the symphony of residuals here."],"metadata":{"id":"oDphOv4X8K20"}},{"cell_type":"markdown","source":["35..\n","\n","Logistic Regression: The Binary Ballad\n","\n","\n","When our data yearns for a binary answer—yes or no, win or lose, pass or fail—logistic regression steps onto the stage. It’s the classifier of choice for such melodious dilemmas. 🎶\n","\n","Here’s how it orchestrates its magic:\n","\n","\n","Binary Outcomes Modeling:\n","Imagine you’re balancing on a seesaw of probability. Lean slightly to one side, and it predicts one outcome; lean the other way, and it predicts the opposite.\n","Logistic regression estimates the probability that a given input point belongs to a particular category. It’s all about the odds of ‘yes’ vs. ‘no.’\n","\n","Odds Ratio and Log-odds:\n","\n","Unlike linear regression (which predicts continuous outputs), logistic regression predicts the log-odds of the dependent variable.\n","\n","Think of log-odds as the backstage whispers—the hidden probabilities behind the curtain.\n","\n","Sigmoid Function: The Magical Squeeze:\n","\n","Logistic regression employs a sigmoid (or logistic) function. It gently squeezes the output of a linear equation between 0 and 1.\n","\n","This squeezing transforms raw scores into interpretable probabilities. It’s like turning up the spotlight on our predictions.\n","\n","Maximizing Likelihood: The Fitting Process:\n","\n","Logistic regression aims to make the observed outcomes as probable as possible given the model’s parameters.\n","\n","It dances through the data, adjusting its steps to match the rhythm of reality.\n","Threshold Determination: The Final Note:\n","\n","By setting a threshold (often 0.5), the model decides which category to assign to a new observation.\n","\n","It’s like a conductor signaling the orchestra—the decisive moment.\n","\n","Common Uses for Logistic Regression:\n","\n","Email Filtering: Spam or not spam? Logistic regression knows.\n","\n","Medical Diagnosis: Assessing disease likelihood based on patient\n","characteristics.\n","\n","Credit Scoring: Predicting loan default probabilities.\n","\n","Customer Churn Prediction: Will they stay or will they go?"],"metadata":{"id":"zHFgt_wf8hHE"}},{"cell_type":"markdown","source":["36..\n","\n","The Sigmoid Function (aka Logistic Function)\n","\n","The sigmoid function is like a gentle curve—a bridge between linear and non-linear worlds. It’s a mathematical gem that maps any real-valued input to a value within the range of 0 to 1. Here’s the sheet music for the sigmoid melody:\n","\n","Definition:\n","\n","The sigmoid function is denoted as: [ \\text{sigmoid}(x) = \\frac{1}{1 + e^{-x}} ]\n","Here, (x) represents the output of our logistic regression model.\n","\n","Shape:\n","\n","Picture an “S” curve—it starts low, gradually rises, and eventually levels off near 1.\n","\n","As (x) moves from negative infinity to positive infinity, the sigmoid smoothly transitions from 0 to 1.\n","\n","Interpretation:\n","\n","The sigmoid function transforms raw scores (linear combinations of features) into probabilities.\n","\n","For example, if our model predicts a value of (x = 2), the sigmoid gives us (\\text{sigmoid}(2) \\approx 0.88). This means an 88% probability of belonging to the positive class.\n","\n","Sigmoid in Logistic Regression:\n","\n","Probability Bridge:\n","\n","Logistic regression aims to estimate the probability that an observation belongs to a specific class (e.g., spam or not spam).\n","\n","The sigmoid function bridges the gap between linear predictions and these probabilities.\n","\n","Log-odds Transformation:\n","\n","Logistic regression models the log-odds of the probability: [ \\text{log-odds} = \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots ]\n","Here, (p) is the probability, and (\\beta_i) are the coefficients.\n","\n","Decision Threshold:\n","\n","By setting a threshold (often 0.5), we decide which class to assign.\n","If (\\text{sigmoid}(x) > 0.5), we predict the positive class; otherwise, the negative class.\n","\n","Why Sigmoid Matters:\n","\n","It gracefully transforms linear predictions into interpretable probabilities.\n","It’s the secret sauce that makes logistic regression work for binary classification."],"metadata":{"id":"OOn-wAi184cC"}},{"cell_type":"markdown","source":["37..\n","\n","The Cost Function in Logistic Regression\n","In our musical journey, the cost function plays a crucial role—it’s like the conductor’s baton, guiding our model toward harmony. Let’s explore:\n","\n","What Is a Cost Function?\n","\n","A cost function (also known as a loss function or objective function) quantifies the difference between the predicted values and the actual (ground truth) values.\n","\n","It’s the price our algorithm pays if it makes predictions that deviate from reality.\n","\n","Why Do We Need It?\n","\n","Imagine predicting whether a patient has diabetes based on sugar levels. Linear regression fails here—it assumes a linear relationship, which isn’t suitable for binary classification.\n","\n","Logistic regression steps in, using the sigmoid function to model probabilities. But how do we measure its performance? Enter the cost function!\n","\n","The Hypothesis and Cost:\n","\n","Our hypothesis function (h_\\theta(x)) predicts the probability of an instance belonging to a particular class.\n","\n","The cost function assesses the model’s performance by comparing these predictions to the actual labels (0 or 1).\n","\n","Log Loss (Cross-Entropy):\n","\n","The most commonly used cost function for logistic regression is the log loss (or cross-entropy) function.\n","\n","It’s like the sheet music—the notes that guide our learning process.\n","The log loss for a single observation is: [ \\text{Cost} = -y \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x)) ]\n","\n","Here, (y) is the actual label (0 or 1), and (h_\\theta(x)) is the predicted probability.\n","\n","Minimizing the Cost:\n","\n","Our goal is to minimize the overall cost across all observations.\n","\n","Gradient descent adjusts the model’s parameters (coefficients) to find the optimal balance.\n","\n","Why It Matters:\n","\n","Minimizing the cost function aligns our model with reality.\n","\n","It ensures our predictions harmonize with the ground truth."],"metadata":{"id":"qMAr8Z7L9Ymm"}},{"cell_type":"markdown","source":["38..\n","\n","\n","Multiclass Logistic Regression\n","\n","Logistic regression, by default, is designed for binary classification—predicting outcomes with two classes (e.g., spam or not spam). But what if we want to predict more than two classes (e.g., cat, dog, or bird)? That’s where multiclass logistic regression steps in.\n","\n","\n","Here are the magical extensions:\n","\n","One-vs-Rest (OvR) Approach:\n","\n","Imagine a musical ensemble where each musician plays their own tune.\n","\n","In OvR, we create a separate binary logistic regression model for each class.\n","\n","For each model:\n","\n","The positive class is the target class we’re interested in.\n","\n","The negative class includes all other classes.\n","\n","When making predictions, we choose the class with the highest probability.\n","\n","Multinomial Logistic Regression (MLR):\n","\n","This is the true multiclass extension of logistic regression.\n","\n","Instead of transforming the problem into multiple binary problems, MLR directly predicts probabilities for each class.\n","\n","It uses the softmax function to convert raw scores into class probabilities.\n","The sum of probabilities across all classes equals 1.\n","\n","Why It Matters:\n","\n","Multiclass logistic regression elegantly handles diverse prediction tasks.\n","Whether it’s identifying flowers, recognizing handwritten digits, or classifying music genres, MLR sings the right tune."],"metadata":{"id":"wmqIYJT69hL-"}},{"cell_type":"markdown","source":["39..\n","\n","L1 Regularization (Lasso):\n","\n","Penalty Term:\n","\n","L1 regularization adds the absolute value of the coefficient magnitudes as a penalty term to the loss function.\n","\n","It’s like gently nudging the coefficients toward zero.\n","\n","Feature Selection:\n","\n","L1 regularization is a great feature selector.\n","\n","It tends to shrink less important feature coefficients to exactly zero.\n","In other words, it performs automatic feature selection by eliminating irrelevant features.\n","\n","\n","Sparsity:\n","\n","The resulting model often has sparse coefficients (many are exactly zero).\n","It’s like decluttering—keeping only the essential features.\n","\n","L2 Regularization (Ridge):\n","\n","Penalty Term:\n","\n","L2 regularization adds the squared magnitude of the coefficient as the penalty term to the loss function.\n","\n","It’s like gently taming the coefficients.\n","\n","Balanced Shrinkage:\n","\n","L2 regularization shrinks all coefficients evenly.\n","\n","It doesn’t force any coefficient to exactly zero (unless the regularization strength is extremely high).\n","\n","Collinearity Handling:\n","\n","L2 regularization works well when features are correlated (collinear).\n","It balances their impact without completely excluding any.\n","\n","When to Use Which?\n","\n","L1 (Lasso):\n","\n","For feature selection.\n","\n","When you suspect some features are irrelevant.\n","\n","Creates sparse models.\n","\n","\n","L2 (Ridge):\n","\n","For overall model improvement.\n","\n","When all features are potentially relevant.\n","\n","Balances coefficients without excluding any."],"metadata":{"id":"p1GR0EI29_tH"}},{"cell_type":"markdown","source":["40..\n","\n","\n","What Is XGBoost?\n","\n","The Ensemble Maestro:\n","\n","XGBoost belongs to the family of boosting algorithms—ensemble learning techniques that combine the predictions of multiple weak learners (usually decision trees) to create a strong predictive model.\n","\n","It’s like assembling a dream team of musicians, each playing their part to perfection.\n","\n","Extreme Gradient Boosting: The X Factor:\n","\n","XGBoost takes gradient boosting to the next level:\n","\n","Regularization Elements: It includes regularization terms in the objective function, preventing overfitting and improving generalization.\n","\n","Learning Rate (Shrinkage): A new parameter (symbolized as “eta”) controls each tree’s contribution to the total prediction. Lower learning rates make the model more resilient.\n","\n","Level-wise Tree Construction: XGBoost constructs trees level by level, assessing whether adding a new node (split) improves the overall objective function. If not, it trims the split.\n","\n","Advanced Optimization: It uses more sophisticated optimization algorithms, enhancing both accuracy and efficiency.\n","\n","Why Is It “eXtreme”?\n","\n","Preventing Overfitting:\n","\n","XGBoost’s learning rate (shrinkage) makes it more conservative, avoiding overfitting.\n","\n","It’s like a seasoned musician—playing just the right notes.\n","\n","Easier-to-Understand Trees:\n","XGBoost’s level-wise growth and trimming create trees that are easier to interpret.\n","\n","It’s like sheet music—clear and concise.\n","\n","Robustness and Power:\n","\n","The regularization terms, along with other techniques, prevent overfitting and improve generalization.\n","\n","XGBoost handles large datasets across various tasks—regression, classification, and more."],"metadata":{"id":"-snN2wnD-KON"}},{"cell_type":"markdown","source":["41..\n","\n","Boosting: The Ensemble Symphony\n","\n","Boosting is like assembling a dream team of musicians, each playing their part to perfection. In ensemble learning, it combines the predictions of multiple weak learners (usually decision trees) to create a strong predictive model.\n","\n","Here’s how it orchestrates its magic:\n","\n","Sequential Learning:\n","\n","Imagine a musical ensemble where each musician plays their own tune.\n","\n","Boosting trains a series of weak models sequentially:\n","\n","Each model corrects errors made by the previous one.\n","\n","It’s like fine-tuning the orchestra note by note.\n","\n","Weighted Voting:\n","\n","Each weak model gets a vote in the final prediction.\n","\n","Models that perform well get more weight, while underperforming ones get less.\n","It’s like harmonizing the voices—the stronger singers lead the chorus.\n","\n","Adaptive Learning:\n","\n","Boosting focuses on samples that are misclassified or have high residuals.\n","It’s like giving extra practice to struggling musicians.\n","\n","Popular Boosting Algorithms:\n","\n","AdaBoost, Gradient Boosting, and XGBoost are some famous members of the boosting ensemble."],"metadata":{"id":"NlOgjfbW-sSX"}},{"cell_type":"markdown","source":["42.\n","\n","Handling Missing Values:\n","\n","XGBoost supports missing values by default.\n","\n","During training, it learns branch directions for missing values within its decision trees.\n","\n","The gblinear booster treats missing values as zeros.\n","\n","Tree Algorithms and Missing Values:\n","\n","In tree algorithms (like XGBoost), missing values are gracefully handled during training.\n","\n","The model figures out the best way to split nodes even when some data points have missing features."],"metadata":{"id":"GQG2CQ9i-zdq"}},{"cell_type":"markdown","source":["43..\n","\n","eta (Learning Rate):\n","\n","The learning rate controls how quickly the model learns from the data.\n","\n","Smaller values (e.g., 0.01 to 0.3) require more boosting rounds but can lead to better generalization.\n","\n","Larger values make computation faster but don’t necessarily improve the best optimum.\n","\n","max_depth (Maximum Tree Depth):\n","\n","Increasing max_depth makes the model more complex and can improve performance.\n","However, too high values risk overfitting.\n","\n","Typical range: 3-10 for shallow trees, up to 15-25 for deep trees.\n","\n","min_child_weight (Minimum Samples per Leaf):\n","\n","It sets the minimum number of samples required in a leaf node for further splitting.\n","\n","\n","Higher values prevent overfitting.\n","\n","Typical range: 1-10, with higher values for sparse datasets.\n","\n","subsample and colsample_bytree (Subsampling Fractions):\n","\n","These control row and column sampling during each iteration.\n","\n","Typical range: 0.5-1.0.\n","\n","Lower values are more conservative and prevent overfitting.\n","\n","gamma (Minimum Loss Reduction to Split a Node):\n","\n","Higher values make the algorithm more conservative.\n","\n","Typical range: 0-10.\n","\n","Grid Search and Cross-Validation:\n","\n","The best way to select parameters is through grid search with cross-validation.\n","Explore a wide range of values for important parameters (like eta, max_depth, and min_child_weight).\n","\n","Start with shallow trees before exploring deeper ones.\n","\n","Also, tune regularization parameters (e.g., lambda, alpha) to control model complexity and prevent overfitting."],"metadata":{"id":"K7jQtIQ1_FkL"}},{"cell_type":"markdown","source":["44..\n","\n","Gradient Boosting and XGBoost: The Ensemble Symphony\n","Gradient boosting is like assembling a dream team of musicians, each playing their part to perfection. It’s an ensemble learning technique that combines the predictions of multiple weak learners (usually decision trees) to create a strong predictive model. Now, let’s add the “X” factor—XGBoost—to this musical ensemble:\n","\n","Sequential Learning:\n","\n","Imagine a musical ensemble where each musician plays their own tune.\n","\n","Gradient boosting trains a series of weak models sequentially:\n","\n","Each model corrects errors made by the previous one.\n","\n","It’s like fine-tuning the orchestra note by note.\n","\n","Weighted Voting:\n","\n","Each weak model gets a vote in the final prediction.\n","\n","Models that perform well get more weight, while underperforming ones get less.\n","It’s like harmonizing the voices—the stronger singers lead the chorus.\n","\n","Adaptive Learning:\n","\n","Gradient boosting focuses on samples that are misclassified or have high residuals.\n","\n","It’s like giving extra practice to struggling musicians.\n","\n","XGBoost’s Extra Touches:\n","\n","Regularization Elements:\n","\n","XGBoost includes regularization terms in the objective function.\n","\n","It improves generalization and prevents overfitting.\n","\n","Learning Rate (Shrinkage):\n","\n","The learning rate (symbolized as “eta”) controls each tree’s contribution to the total prediction.\n","\n","Lower learning rates make the model more resilient.\n","\n","Level-wise Tree Construction:\n","\n","XGBoost constructs trees level by level, assessing whether adding a new node (split) improves the overall objective function.\n","\n","The split is trimmed if not.\n","\n","This level growth along with trimming makes the trees easier to understand and create.\n","\n","Advanced Optimization:\n","\n","XGBoost uses more sophisticated optimization algorithms, enhancing both accuracy and efficiency."],"metadata":{"id":"JfxaXuBJ_aa4"}},{"cell_type":"markdown","source":["45..\n","\n","Advantages of XGBoost:\n","\n","High Performance and Accuracy:\n","\n","XGBoost consistently delivers excellent predictive performance.\n","\n","It’s particularly effective for structured data with a mix of numeric and categorical features.\n","\n","Efficient Handling of Missing Values and Outliers:\n","\n","XGBoost can gracefully handle missing values during training.\n","\n","It’s robust to outliers, making it suitable for real-world datasets.\n","\n","Built-in Regularization:\n","\n","XGBoost includes regularization terms (L1 and L2) to prevent overfitting.\n","This helps generalize well to unseen data.\n","\n","Scalability to Large Datasets:\n","\n","XGBoost scales well to large datasets with thousands to millions of instances.\n","It efficiently handles feature engineering and model training.\n","\n","Feature Importance Scores:\n","\n","XGBoost provides feature importance scores, helping interpret model decisions.\n","You can understand which features contribute most to predictions.\n","\n","Disadvantages of XGBoost:\n","\n","Parameter Tuning Complexity:\n","\n","To achieve optimal performance, XGBoost requires careful parameter tuning.\n","Finding the right combination of hyperparameters can be time-consuming.\n","\n","Risk of Overfitting:\n","\n","Without proper regularization, XGBoost can overfit the training data.\n","Regularization parameters need to be tuned carefully.\n","\n","Computationally Expensive Training:\n","\n","Training XGBoost models can be computationally expensive, especially with large datasets.\n","\n","However, the trade-off is improved performance.\n","\n","Complex Model Interpretation:\n","\n","Interpreting XGBoost models can be challenging due to their complexity.\n","Feature importance scores help, but understanding the entire model can be daunting.\n","\n","When to Use XGBoost:\n","\n","XGBoost is well-suited for:\n","\n","Tabular data with a mix of numeric and categorical features.\n","\n","Problems where model performance is a priority.\n","\n","Datasets of moderate to large size."],"metadata":{"id":"LOnsvRiH_x50"}}]}